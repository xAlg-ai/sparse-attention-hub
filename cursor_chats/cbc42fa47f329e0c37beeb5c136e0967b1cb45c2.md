# Understanding the sparse_attention_hub project
_Exported on 7/23/2025 at 15:22:44 PDT from Cursor (1.2.4)_

---

**User**

Understand the sparse_attention_hub project

---

**Cursor**

I'll help you understand the sparse_attention_hub project by exploring its structure and key components. Let me start by examining the project layout and then dive into the main modules.


Now let me explore the key modules to understand the architecture better:


Now let me examine some key files to understand the core functionality:


Now let me examine the current stress test script to understand how the project is being used:


Let me also check the project configuration:


Now let me check some test files to understand the testing structure:


Based on my exploration of the sparse_attention_hub project, I can now provide you with a comprehensive understanding of this framework.

## Sparse Attention Hub Project Overview

The **Sparse Attention Hub** is a comprehensive framework for implementing and evaluating sparse attention mechanisms in deep learning models. It's designed to provide efficient attention algorithms, benchmarking tools, and seamless integration with popular frameworks like HuggingFace Transformers.

### ðŸ—ï¸ **Core Architecture**

The project is organized into several key modules:

#### 1. **Sparse Attention Module** (`sparse_attention_hub/sparse_attention/`)
- **Base Classes**: `SparseAttention`, `EfficientAttention`, `ResearchAttention`
- **Two Main Categories**:
  - **Efficient Attention**: Production-ready implementations like `DoubleSparsity` and `HashAttention`
  - **Research Attention**: Experimental masking strategies for research purposes

#### 2. **Adapter System** (`sparse_attention_hub/adapters/`)
- **ModelAdapterHF**: HuggingFace integration adapter
- **Request/RequestResponse**: Structured request handling
- **Abstract interfaces** for different model hosting libraries

#### 3. **Benchmarking Framework** (`benchmark/`)
- **BenchmarkExecutor**: Parallel GPU execution with resumability
- **Multiple benchmark datasets**: LongBench, Loogle, InfBench, ZeroScrolls, AIME2024/2025
- **Comprehensive evaluation matrix**: Model Ã— Sparse Config Ã— Benchmark Ã— Subset

#### 4. **Metrics & Visualization** (`sparse_attention_hub/metric_logging/`, `sparse_attention_hub/plotting/`)
- **MicroMetricLogger**: Detailed performance tracking
- **PlotGenerator**: Attention pattern visualizations

### ï¿½ï¿½ **Key Features**

#### **Sparse Attention Algorithms**
The framework implements various sparse attention mechanisms:

**Fixed Pattern Maskers:**
- `LocalMasker`: Local window attention
- `SinkMasker`: Sink token attention (for recent tokens)
- `OracleTopK`: Oracle-based top-k selection
- `CausalMasker`: Causal attention patterns

**Sampling Maskers:**
- `RandomSamplingMasker`: Random sampling strategies
- `AdaptiveSamplingMaskerConfig`: Adaptive sampling with epsilon-delta parameters
- `MagicPig`: LSH-based sampling

**Efficient Implementations:**
- `DoubleSparsity`: Double sparsity attention
- `HashAttention`: Hash-based attention

#### **Benchmarking Capabilities**
The framework supports extensive benchmarking across:
- **9 different benchmark datasets** including LongBench, Loogle, InfBench, ZeroScrolls, AIME2024/2025
- **Parallel GPU execution** with dynamic allocation
- **Resumable experiments** with progress tracking
- **Comprehensive metrics** collection and analysis

#### **Integration Features**
- **HuggingFace Transformers** integration
- **Flash Attention 2** support
- **Context manager** for sparse/dense mode switching
- **Structured request/response** handling

### ðŸ“Š **Current Usage Example**

Looking at the stress test script, the project is currently being used for:

```python
# Creating adaptive sampling configurations
SPARSE_CONFIGS = []
for (epsilon, delta) in [(0.01, 0.01), (0.05, 0.05), ...]:
    for (sink_size, window_size) in [(0.001, 0.001), ...]:
        for heavy_size in [0.005, 0.01, ...]:
            for base_rate_sampling in [0.01, 0.05, 0.1]:
                config = ResearchAttentionConfig(masker_configs=[
                    SinkMaskerConfig(sink_size=sink_size),
                    LocalMaskerConfig(window_size=window_size),
                    OracleTopKConfig(heavy_size=heavy_size),
                    AdaptiveSamplingMaskerConfig(
                        base_rate_sampling=base_rate_sampling, 
                        epsilon=epsilon, 
                        delta=delta
                    )
                ])
```

This creates **672 different sparse attention configurations** for systematic evaluation.

### ðŸŽ¯ **Project Goals**

1. **Research Platform**: Enable systematic evaluation of sparse attention mechanisms
2. **Production Ready**: Provide efficient implementations for real-world use
3. **Extensible**: Modular design for easy addition of new algorithms
4. **Comprehensive**: Full benchmarking and evaluation pipeline
5. **User Friendly**: Simple interfaces for model integration

### ðŸ”¬ **Research Focus**

The project appears to be focused on:
- **Adaptive sampling strategies** with epsilon-delta guarantees
- **Multi-masker combinations** (sink + local + oracle + adaptive)
- **Long-context understanding** across various benchmarks
- **Systematic parameter exploration** for optimal configurations

This is a sophisticated research framework that bridges the gap between theoretical sparse attention algorithms and practical implementation, with a strong emphasis on systematic evaluation and benchmarking.

```python
# Creating adaptive sampling configurations
SPARSE_CONFIGS = []
for (epsilon, delta) in [(0.01, 0.01), (0.05, 0.05), ...]:
    for (sink_size, window_size) in [(0.001, 0.001), ...]:
        for heavy_size in [0.005, 0.01, ...]:
            for base_rate_sampling in [0.01, 0.05, 0.1]:
                config = ResearchAttentionConfig(masker_configs=[
                    SinkMaskerConfig(sink_size=sink_size),
                    LocalMaskerConfig(window_size=window_size),
                    OracleTopKConfig(heavy_size=heavy_size),
                    AdaptiveSamplingMaskerConfig(
                        base_rate_sampling=base_rate_sampling, 
                        epsilon=epsilon, 
                        delta=delta
                    )
                ])
```

---

**User**

The stress_test_adaptive.matrix has a dump of micro_metrics and config for each run in config.json. Understand the structure of this dump folder

---

**Cursor**

I'll explore the structure of the stress test results to understand how the micro metrics and configurations are dumped. Let me start by examining the results directory.


Let me look at a specific configuration directory to understand the structure:


Perfect! Now let me examine the key files to understand the structure:


Now let me look at a few more examples to understand the pattern better:


Let me also check the micro metrics logger implementation to understand how these metrics are generated:


Now let me check how the benchmark executor creates these result directories:


Perfect! Now I have a comprehensive understanding of the stress test results structure. Let me provide you with a detailed explanation.

## Stress Test Results Structure Analysis

The `stress_test_adaptive.matrix` directory contains a comprehensive dump of micro metrics and configurations for each experimental run. Here's the detailed structure:

### ðŸ“ **Directory Hierarchy**

```
stress_test_adaptive.matrix/
â””â”€â”€ meta-llama_Llama-3.1-8B-Instruct/
    â””â”€â”€ adaptive_sampling.sink_{sink_size}_window_{window_size}_heavy_{heavy_size}_base_{base_rate}_epsilon_{epsilon}_delta_{delta}/
        â””â”€â”€ longbench_passage_retrieval_en/
            â”œâ”€â”€ config.json
            â”œâ”€â”€ micro_metrics.jsonl
            â”œâ”€â”€ metrics.json
            â””â”€â”€ raw_results.csv
```

### ðŸ”§ **Configuration Structure (`config.json`)**

Each run stores the complete experimental configuration:

```json
{
  "model_kwargs": {
    "torch_dtype": "torch.bfloat16",
    "attn_implementation": "flash_attention_2"
  },
  "tokenizer_kwargs": {
    "padding_side": "left"
  },
  "sparse_attention_config": {
    "masker_configs": [
      {"sink_size": 0.001},           // SinkMaskerConfig
      {"window_size": 0.001},         // LocalMaskerConfig  
      {"heavy_size": 0.005},          // OracleTopKConfig
      {
        "base_rate_sampling": 0.01,   // AdaptiveSamplingMaskerConfig
        "epsilon": 0.01,
        "delta": 0.01,
        "init_offset": 0.001,
        "local_offset": 0.001
      }
    ]
  },
  "generation_kwargs": {
    "max_new_tokens": 20,
    "do_sample": false,
    "temperature": 1.0,
    "top_p": 1.0,
    "pad_token_id": null
  },
  "request_kwargs": {
    "max_context_length": 32000,
    "max_requests": 1
  },
  "benchmark_name": "longbench",
  "subsets_to_run": ["passage_retrieval_en"],
  "huggingface_dataset_id": "Xnhyacinth/LongBench"
}
```

### ðŸ“Š **Micro Metrics Structure (`micro_metrics.jsonl`)**

This file contains detailed per-layer metrics in JSONL format (one JSON object per line):

```json
{
  "timestamp": "2025-07-23T13:38:50.531863",
  "metric": "research_attention_density",
  "value": 0.5858072409723574,
  "metadata": {"layer_idx": 0},
  "location": "sparse_attention_hub.sparse_attention.research_attention.base.ResearchAttention.custom_attention"
}
```

**Key Metrics Tracked:**
- **`research_attention_density`**: The sparsity ratio (0.0 = fully sparse, 1.0 = dense)
- **`research_attention_output_error`**: Error between sparse and dense attention outputs

**Metadata Structure:**
- **`layer_idx`**: Transformer layer index (0-31 for Llama-3.1-8B)
- **`timestamp`**: Precise timing of each measurement
- **`location`**: Auto-inferred calling location for debugging

### ðŸ“ˆ **Aggregate Metrics (`metrics.json`)**

High-level benchmark results:

```json
{
  "task_scores": {
    "passage_retrieval_en": 100.0
  },
  "standard_overall_score": 100.0,
  "overall_score": 100.0,
  "summary": {
    "total_tasks": 1,
    "standard_tasks": 1,
    "extended_tasks": 0,
    "total_samples": 1
  }
}
```

### ðŸ“‹ **Raw Results (`raw_results.csv`)**

Detailed CSV with input-output pairs:
- **Input**: Context, questions, task metadata
- **Output**: Model predictions, ground truth, evaluation metrics

### ðŸ”¬ **Experimental Matrix**

The directory structure reveals the systematic parameter exploration:

**Configuration Parameters:**
- **Sink Size**: `[0.001, 0.005, 0.01, 0.02]` (4 values)
- **Window Size**: `[0.001, 0.005, 0.01, 0.02]` (4 values)  
- **Heavy Size**: `[0.005, 0.01, 0.02, 0.05]` (4 values)
- **Base Rate Sampling**: `[0.01, 0.05, 0.1]` (3 values)
- **Epsilon/Delta**: `[(0.01,0.01), (0.05,0.05), (0.1,0.1), (0.25,0.25), (0.3,0.3), (0.4,0.4), (0.5,0.5)]` (7 values)

**Total Configurations**: 4 Ã— 4 Ã— 4 Ã— 3 Ã— 7 = **1,344 configurations**

### ðŸŽ¯ **Key Insights from the Structure**

1. **Granular Monitoring**: Each layer's attention behavior is tracked separately
2. **Temporal Resolution**: Precise timestamps enable performance analysis
3. **Reproducibility**: Complete configuration capture ensures experiment replication
4. **Multi-level Analysis**: From micro (per-layer) to macro (benchmark scores)
5. **Systematic Exploration**: Exhaustive parameter space coverage for adaptive sampling

### ðŸ” **Data Analysis Potential**

This structure enables:
- **Layer-wise sparsity analysis**: How sparsity varies across transformer layers
- **Parameter sensitivity**: Impact of epsilon/delta on attention density
- **Performance correlation**: Relationship between sparsity and accuracy
- **Temporal patterns**: Attention behavior evolution during inference
- **Configuration optimization**: Finding optimal parameter combinations

The comprehensive logging system provides unprecedented visibility into sparse attention mechanisms, enabling detailed research into their behavior and optimization.

```plaintext
stress_test_adaptive.matrix/
â””â”€â”€ meta-llama_Llama-3.1-8B-Instruct/
    â””â”€â”€ adaptive_sampling.sink_{sink_size}_window_{window_size}_heavy_{heavy_size}_base_{base_rate}_epsilon_{epsilon}_delta_{delta}/
        â””â”€â”€ longbench_passage_retrieval_en/
            â”œâ”€â”€ config.json
            â”œâ”€â”€ micro_metrics.jsonl
            â”œâ”€â”€ metrics.json
            â””â”€â”€ raw_results.csv
```

```json
{
  "model_kwargs": {
    "torch_dtype": "torch.bfloat16",
    "attn_implementation": "flash_attention_2"
  },
  "tokenizer_kwargs": {
    "padding_side": "left"
  },
  "sparse_attention_config": {
    "masker_configs": [
      {"sink_size": 0.001},           // SinkMaskerConfig
      {"window_size": 0.001},         // LocalMaskerConfig  
      {"heavy_size": 0.005},          // OracleTopKConfig
      {
        "base_rate_sampling": 0.01,   // AdaptiveSamplingMaskerConfig
        "epsilon": 0.01,
        "delta": 0.01,
        "init_offset": 0.001,
        "local_offset": 0.001
      }
    ]
  },
  "generation_kwargs": {
    "max_new_tokens": 20,
    "do_sample": false,
    "temperature": 1.0,
    "top_p": 1.0,
    "pad_token_id": null
  },
  "request_kwargs": {
    "max_context_length": 32000,
    "max_requests": 1
  },
  "benchmark_name": "longbench",
  "subsets_to_run": ["passage_retrieval_en"],
  "huggingface_dataset_id": "Xnhyacinth/LongBench"
}
```

```json
{
  "timestamp": "2025-07-23T13:38:50.531863",
  "metric": "research_attention_density",
  "value": 0.5858072409723574,
  "metadata": {"layer_idx": 0},
  "location": "sparse_attention_hub.sparse_attention.research_attention.base.ResearchAttention.custom_attention"
}
```

```json
{
  "task_scores": {
    "passage_retrieval_en": 100.0
  },
  "standard_overall_score": 100.0,
  "overall_score": 100.0,
  "summary": {
    "total_tasks": 1,
    "standard_tasks": 1,
    "extended_tasks": 0,
    "total_samples": 1
  }
}
```

---

**User**

Create a python script  in benchmark/scripts folder named analyse_stress_tests.py which  dumps error,density in a tsv file called vector.tsv and metadata.tsv file containing layer_idx and different settings from config file regarding the sparse attention config . 




---

**Cursor**

I'll create a Python script to analyze the stress test results and generate the requested TSV files. Let me start by examining the structure and then create the analysis script.


Let me check what conda environments are available and then proceed with creating the script:


---

**User**

use sat environment

---

**Cursor**

Perfect! Now let me create the analysis script. First, let me examine a few more examples of the data structure to understand the patterns better:


Now I'll create the analysis script:


```python
#!/usr/bin/env python3
"""
Analysis script for stress test results.

This script processes the stress_test_adaptive.matrix directory and generates:
1. vector.tsv - Contains error and density values for each layer and configuration
2. metadata.tsv - Contains layer indices and sparse attention configuration settings

Usage:
    python benchmark/scripts/analyse_stress_tests.py
"""

import json
import os
import re
from pathlib import Path
from typing import Dict, List, Tuple, Any
import argparse


def parse_config_name(config_name: str) -> Dict[str, float]:
    """Parse configuration name to extract parameters.
    
    Args:
        config_name: Configuration name like "adaptive_sampling.sink_0.001_window_0.001_heavy_0.005_base_0.01_epsilon_0.01_delta_0.01"
        
    Returns:
        Dictionary with parsed parameters
    """
    # Extract parameters using regex
    pattern = r"adaptive_sampling\.sink_([\d.]+)_window_([\d.]+)_heavy_([\d.]+)_base_([\d.]+)_epsilon_([\d.]+)_delta_([\d.]+)"
    match = re.match(pattern, config_name)
    
    if not match:
        return {}
    
    return {
        "sink_size": float(match.group(1)),
        "window_size": float(match.group(2)),
        "heavy_size": float(match.group(3)),
        "base_rate_sampling": float(match.group(4)),
        "epsilon": float(match.group(5)),
        "delta": float(match.group(6))
    }


def load_config_file(config_path: Path) -> Dict[str, Any]:
    """Load configuration from JSON file.
    
    Args:
        config_path: Path to config.json file
        
    Returns:
        Configuration dictionary
    """
    with open(config_path, 'r') as f:
        return json.load(f)


def load_micro_metrics(metrics_path: Path) -> List[Dict[str, Any]]:
    """Load micro metrics from JSONL file.
    
    Args:
        metrics_path: Path to micro_metrics.jsonl file
        
    Returns:
        List of metric entries
    """
    metrics = []
    with open(metrics_path, 'r') as f:
        for line in f:
            if line.strip():
                metrics.append(json.loads(line))
    return metrics


def process_experiment_directory(exp_dir: Path) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """Process a single experiment directory.
    
    Args:
        exp_dir: Path to experiment directory
        
    Returns:
        Tuple of (metrics_data, config_data)
    """
    # Find the benchmark subdirectory (e.g., longbench_passage_retrieval_en)
    benchmark_dirs = [d for d in exp_dir.iterdir() if d.is_dir()]
    if not benchmark_dirs:
        return [], {}
    
    benchmark_dir = benchmark_dirs[0]  # Take the first benchmark directory
    
    # Load configuration
    config_path = benchmark_dir / "config.json"
    if not config_path.exists():
        return [], {}
    
    config = load_config_file(config_path)
    
    # Load micro metrics
    metrics_path = benchmark_dir / "micro_metrics.jsonl"
    if not metrics_path.exists():
        return [], {}
    
    metrics = load_micro_metrics(metrics_path)
    
    return metrics, config


def extract_sparse_config_params(config: Dict[str, Any]) -> Dict[str, Any]:
    """Extract sparse attention configuration parameters.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Dictionary with sparse attention parameters
    """
    sparse_config = config.get("sparse_attention_config", {})
    masker_configs = sparse_config.get("masker_configs", [])
    
    params = {}
    
    # Extract parameters from masker configs
    for masker_config in masker_configs:
        if "sink_size" in masker_config:
            params["sink_size"] = masker_config["sink_size"]
        elif "window_size" in masker_config:
            params["window_size"] = masker_config["window_size"]
        elif "heavy_size" in masker_config:
            params["heavy_size"] = masker_config["heavy_size"]
        elif "base_rate_sampling" in masker_config:
            params["base_rate_sampling"] = masker_config["base_rate_sampling"]
            params["epsilon"] = masker_config.get("epsilon")
            params["delta"] = masker_config.get("delta")
            params["init_offset"] = masker_config.get("init_offset")
            params["local_offset"] = masker_config.get("local_offset")
    
    return params


def organize_metrics_by_layer(metrics: List[Dict[str, Any]]) -> Dict[int, Dict[str, float]]:
    """Organize metrics by layer index.
    
    Args:
        metrics: List of metric entries
        
    Returns:
        Dictionary mapping layer_idx to metrics
    """
    layer_metrics = {}
    
    for metric in metrics:
        layer_idx = metric.get("metadata", {}).get("layer_idx")
        if layer_idx is None:
            continue
            
        if layer_idx not in layer_metrics:
            layer_metrics[layer_idx] = {}
        
        metric_name = metric.get("metric")
        value = metric.get("value")
        
        if metric_name == "research_attention_density":
            layer_metrics[layer_idx]["density"] = value
        elif metric_name == "research_attention_output_error":
            layer_metrics[layer_idx]["error"] = value
    
    return layer_metrics


def analyze_stress_tests(results_dir: str, output_dir: str) -> None:
    """Analyze stress test results and generate TSV files.
    
    Args:
        results_dir: Path to stress_test_adaptive.matrix directory
        output_dir: Output directory for TSV files
    """
    results_path = Path(results_dir)
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # Find model directories
    model_dirs = [d for d in results_path.iterdir() if d.is_dir()]
    
    all_vector_data = []
    all_metadata = []
    
    for model_dir in model_dirs:
        model_name = model_dir.name
        
        # Find configuration directories
        config_dirs = [d for d in model_dir.iterdir() if d.is_dir()]
        
        for config_dir in config_dirs:
            config_name = config_dir.name
            
            # Parse configuration name
            parsed_params = parse_config_name(config_name)
            
            # Process experiment directory
            metrics, config = process_experiment_directory(config_dir)
            
            if not metrics or not config:
                continue
            
            # Extract sparse attention parameters
            sparse_params = extract_sparse_config_params(config)
            
            # Organize metrics by layer
            layer_metrics = organize_metrics_by_layer(metrics)
            
            # Generate vector data
            for layer_idx, layer_data in layer_metrics.items():
                if "density" in layer_data and "error" in layer_data:
                    vector_entry = {
                        "model": model_name,
                        "config": config_name,
                        "layer_idx": layer_idx,
                        "density": layer_data["density"],
                        "error": layer_data["error"]
                    }
                    all_vector_data.append(vector_entry)
            
            # Generate metadata entry
            metadata_entry = {
                "model": model_name,
                "config": config_name,
                "layer_idx": "all",  # This will be expanded for each layer
                **parsed_params,
                **sparse_params
            }
            
            # Add metadata for each layer
            for layer_idx in layer_metrics.keys():
                layer_metadata = metadata_entry.copy()
                layer_metadata["layer_idx"] = layer_idx
                all_metadata.append(layer_metadata)
    
    # Write vector.tsv
    vector_path = output_path / "vector.tsv"
    with open(vector_path, 'w') as f:
        # Write header
        f.write("model\tconfig\tlayer_idx\tdensity\terror\n")
        
        # Write data
        for entry in all_vector_data:
            f.write(f"{entry['model']}\t{entry['config']}\t{entry['layer_idx']}\t{entry['density']}\t{entry['error']}\n")
    
    # Write metadata.tsv
    metadata_path = output_path / "metadata.tsv"
    with open(metadata_path, 'w') as f:
        if all_metadata:
            # Write header
            headers = list(all_metadata[0].keys())
            f.write("\t".join(headers) + "\n")
            
            # Write data
            for entry in all_metadata:
                values = [str(entry.get(header, "")) for header in headers]
                f.write("\t".join(values) + "\n")
    
    print(f"Analysis complete!")
    print(f"Vector data written to: {vector_path}")
    print(f"Metadata written to: {metadata_path}")
    print(f"Total vector entries: {len(all_vector_data)}")
    print(f"Total metadata entries: {len(all_metadata)}")


def main():
    """Main function."""
    parser = argparse.ArgumentParser(description="Analyze stress test results")
    parser.add_argument(
        "--results-dir", 
        default="./stress_test_adaptive.matrix",
        help="Path to stress test results directory"
    )
    parser.add_argument(
        "--output-dir", 
        default="./analysis_output",
        help="Output directory for TSV files"
    )
    
    args = parser.parse_args()
    
    # Check if results directory exists
    if not os.path.exists(args.results_dir):
        print(f"Error: Results directory '{args.results_dir}' does not exist")
        return
    
    analyze_stress_tests(args.results_dir, args.output_dir)


if __name__ == "__main__":
    main()
```

Now let's test the script to make sure it works correctly:


Great! Now let's run the script to generate the TSV files:


Excellent! The script ran successfully and generated 10,752 entries. Let's examine the output files to verify they contain the expected data:


Perfect! Let's also check the file sizes and get a quick summary:


Excellent! The script has been successfully created and executed. Let me provide you with a summary of what was accomplished:

## âœ… **Analysis Script Successfully Created and Executed**

### ðŸ“ **Script Location**
- **File**: `benchmark/scripts/analyse_stress_tests.py`
- **Executable**: Yes (with proper shebang and permissions)

### ï¿½ï¿½ **Generated Output Files**

#### **1. `vector.tsv`** (1.6MB, 10,753 lines)
**Structure**: `model | config | layer_idx | density | error`

**Sample Data**:
```
model	config	layer_idx	density	error
meta-llama_Llama-3.1-8B-Instruct	adaptive_sampling.sink_0.02_window_0.02_heavy_0.005_base_0.1_epsilon_0.01_delta_0.01	0	0.6010530951743823	0.040283203125
meta-llama_Llama-3.1-8B-Instruct	adaptive_sampling.sink_0.02_window_0.02_heavy_0.005_base_0.1_epsilon_0.01_delta_0.01	1	0.19736196592632235	0.035888671875
```

#### **2. `metadata.tsv`** (1.7MB, 10,753 lines)
**Structure**: `model | config | layer_idx | sink_size | window_size | heavy_size | base_rate_sampling | epsilon | delta | init_offset | local_offset`

**Sample Data**:
```
model	config	layer_idx	sink_size	window_size	heavy_size	base_rate_sampling	epsilon	delta	init_offset	local_offset
meta-llama_Llama-3.1-8B-Instruct	adaptive_sampling.sink_0.02_window_0.02_heavy_0.005_base_0.1_epsilon_0.01_delta_0.01	0	0.02	0.02	0.005	0.1	0.01	0.01	0.02	0.02
```

### ï¿½ï¿½ **Data Coverage**
- **Total Entries**: 10,752 data points (10,753 lines including headers)
- **Layers**: 32 layers per configuration (0-31 for Llama-3.1-8B)
- **Configurations**: ~336 different sparse attention configurations
- **Models**: 1 model (meta-llama_Llama-3.1-8B-Instruct)

### ï¿½ï¿½ï¸ **Script Features**

#### **Key Functions**:
1. **`parse_config_name()`**: Extracts parameters from configuration directory names
2. **`load_config_file()`**: Loads JSON configuration files
3. **`load_micro_metrics()`**: Processes JSONL micro metrics files
4. **`extract_sparse_config_params()`**: Extracts sparse attention parameters
5. **`organize_metrics_by_layer()`**: Groups metrics by layer index
6. **`analyze_stress_tests()`**: Main analysis function

#### **Command Line Interface**:
```bash
python benchmark/scripts/analyse_stress_tests.py --help
python benchmark/scripts/analyse_stress_tests.py --results-dir ./stress_test_adaptive.matrix --output-dir ./analysis_output
```

### ðŸ“ˆ **Data Insights**

The generated files reveal interesting patterns:

1. **Density Variation**: 
   - Higher epsilon/delta values (0.3-0.5) â†’ Lower density (~0.11-0.18)
   - Lower epsilon/delta values (0.01-0.05) â†’ Higher density (~0.3-0.6)

2. **Error Patterns**:
   - Generally lower errors with higher epsilon/delta values
   - Layer-specific error variations across the transformer

3. **Parameter Relationships**:
   - Sink size, window size, and heavy size combinations
   - Adaptive sampling parameters (epsilon, delta, base_rate_sampling)

### ï¿½ï¿½ **Usage**

The TSV files can now be used for:
- **Statistical analysis** of sparse attention performance
- **Visualization** of density vs error relationships
- **Parameter optimization** studies
- **Layer-wise analysis** of attention behavior
- **Machine learning** on sparse attention configurations

The script successfully processed the entire stress test matrix and extracted the requested error/density data along with comprehensive metadata for further analysis!

```plaintext
model	config	layer_idx	density	error
meta-llama_Llama-3.1-8B-Instruct	adaptive_sampling.sink_0.02_window_0.02_heavy_0.005_base_0.1_epsilon_0.01_delta_0.01	0	0.6010530951743823	0.040283203125
meta-llama_Llama-3.1-8B-Instruct	adaptive_sampling.sink_0.02_window_0.02_heavy_0.005_base_0.1_epsilon_0.01_delta_0.01	1	0.19736196592632235	0.035888671875
```

```plaintext
model	config	layer_idx	sink_size	window_size	heavy_size	base_rate_sampling	epsilon	delta	init_offset	local_offset
meta-llama_Llama-3.1-8B-Instruct	adaptive_sampling.sink_0.02_window_0.02_heavy_0.005_base_0.1_epsilon_0.01_delta_0.01	0	0.02	0.02	0.005	0.1	0.01	0.01	0.02	0.02
```

```shellscript
python benchmark/scripts/analyse_stress_tests.py --help
python benchmark/scripts/analyse_stress_tests.py --results-dir ./stress_test_adaptive.matrix --output-dir ./analysis_output
```

---

**User**

In the script make the following changes
1., in vector.tsv:
    - before dumping average errors and density over layer_idx 
    - only dump error and density 
2. in metadata.tsv, keep corresponding lines to vector.tsv

---

**Cursor**

I'll modify the script to average the error and density values over layer indices and update both files accordingly.


```python
#!/usr/bin/env python3
"""
Analysis script for stress test results.

This script processes the stress_test_adaptive.matrix directory and generates:
1. vector.tsv - Contains average error and density values for each configuration
2. metadata.tsv - Contains sparse attention configuration settings

Usage:
    python benchmark/scripts/analyse_stress_tests.py
"""

import json
import os
import re
from pathlib import Path
from typing import Dict, List, Tuple, Any
import argparse


def parse_config_name(config_name: str) -> Dict[str, float]:
    """Parse configuration name to extract parameters.
    
    Args:
        config_name: Configuration name like "adaptive_sampling.sink_0.001_window_0.001_heavy_0.005_base_0.01_epsilon_0.01_delta_0.01"
        
    Returns:
        Dictionary with parsed parameters
    """
    # Extract parameters using regex
    pattern = r"adaptive_sampling\.sink_([\d.]+)_window_([\d.]+)_heavy_([\d.]+)_base_([\d.]+)_epsilon_([\d.]+)_delta_([\d.]+)"
    match = re.match(pattern, config_name)
    
    if not match:
        return {}
    
    return {
        "sink_size": float(match.group(1)),
        "window_size": float(match.group(2)),
        "heavy_size": float(match.group(3)),
        "base_rate_sampling": float(match.group(4)),
        "epsilon": float(match.group(5)),
        "delta": float(match.group(6))
    }


def load_config_file(config_path: Path) -> Dict[str, Any]:
    """Load configuration from JSON file.
    
    Args:
        config_path: Path to config.json file
        
    Returns:
        Configuration dictionary
    """
    with open(config_path, 'r') as f:
        return json.load(f)


def load_micro_metrics(metrics_path: Path) -> List[Dict[str, Any]]:
    """Load micro metrics from JSONL file.
    
    Args:
        metrics_path: Path to micro_metrics.jsonl file
        
    Returns:
        List of metric entries
    """
    metrics = []
    with open(metrics_path, 'r') as f:
        for line in f:
            if line.strip():
                metrics.append(json.loads(line))
    return metrics


def process_experiment_directory(exp_dir: Path) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
    """Process a single experiment directory.
    
    Args:
        exp_dir: Path to experiment directory
        
    Returns:
        Tuple of (metrics_data, config_data)
    """
    # Find the benchmark subdirectory (e.g., longbench_passage_retrieval_en)
    benchmark_dirs = [d for d in exp_dir.iterdir() if d.is_dir()]
    if not benchmark_dirs:
        return [], {}
    
    benchmark_dir = benchmark_dirs[0]  # Take the first benchmark directory
    
    # Load configuration
    config_path = benchmark_dir / "config.json"
    if not config_path.exists():
        return [], {}
    
    config = load_config_file(config_path)
    
    # Load micro metrics
    metrics_path = benchmark_dir / "micro_metrics.jsonl"
    if not metrics_path.exists():
        return [], {}
    
    metrics = load_micro_metrics(metrics_path)
    
    return metrics, config


def extract_sparse_config_params(config: Dict[str, Any]) -> Dict[str, Any]:
    """Extract sparse attention configuration parameters.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Dictionary with sparse attention parameters
    """
    sparse_config = config.get("sparse_attention_config", {})
    masker_configs = sparse_config.get("masker_configs", [])
    
    params = {}
    
    # Extract parameters from masker configs
    for masker_config in masker_configs:
        if "sink_size" in masker_config:
            params["sink_size"] = masker_config["sink_size"]
        elif "window_size" in masker_config:
            params["window_size"] = masker_config["window_size"]
        elif "heavy_size" in masker_config:
            params["heavy_size"] = masker_config["heavy_size"]
        elif "base_rate_sampling" in masker_config:
            params["base_rate_sampling"] = masker_config["base_rate_sampling"]
            params["epsilon"] = masker_config.get("epsilon")
            params["delta"] = masker_config.get("delta")
            params["init_offset"] = masker_config.get("init_offset")
            params["local_offset"] = masker_config.get("local_offset")
    
    return params


def organize_metrics_by_layer(metrics: List[Dict[str, Any]]) -> Dict[int, Dict[str, float]]:
    """Organize metrics by layer index.
    
    Args:
        metrics: List of metric entries
        
    Returns:
        Dictionary mapping layer_idx to metrics
    """
    layer_metrics = {}
    
    for metric in metrics:
        layer_idx = metric.get("metadata", {}).get("layer_idx")
        if layer_idx is None:
            continue
            
        if layer_idx not in layer_metrics:
            layer_metrics[layer_idx] = {}
        
        metric_name = metric.get("metric")
        value = metric.get("value")
        
        if metric_name == "research_attention_density":
            layer_metrics[layer_idx]["density"] = value
        elif metric_name == "research_attention_output_error":
            layer_metrics[layer_idx]["error"] = value
    
    return layer_metrics


def calculate_average_metrics(layer_metrics: Dict[int, Dict[str, float]]) -> Tuple[float, float]:
    """Calculate average density and error across all layers.
    
    Args:
        layer_metrics: Dictionary mapping layer_idx to metrics
        
    Returns:
        Tuple of (average_density, average_error)
    """
    densities = []
    errors = []
    
    for layer_data in layer_metrics.values():
        if "density" in layer_data:
            densities.append(layer_data["density"])
        if "error" in layer_data:
            errors.append(layer_data["error"])
    
    avg_density = sum(densities) / len(densities) if densities else 0.0
    avg_error = sum(errors) / len(errors) if errors else 0.0
    
    return avg_density, avg_error


def analyze_stress_tests(results_dir: str, output_dir: str) -> None:
    """Analyze stress test results and generate TSV files.
    
    Args:
        results_dir: Path to stress_test_adaptive.matrix directory
        output_dir: Output directory for TSV files
    """
    results_path = Path(results_dir)
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # Find model directories
    model_dirs = [d for d in results_path.iterdir() if d.is_dir()]
    
    all_vector_data = []
    all_metadata = []
    
    for model_dir in model_dirs:
        model_name = model_dir.name
        
        # Find configuration directories
        config_dirs = [d for d in model_dir.iterdir() if d.is_dir()]
        
        for config_dir in config_dirs:
            config_name = config_dir.name
            
            # Parse configuration name
            parsed_params = parse_config_name(config_name)
            
            # Process experiment directory
            metrics, config = process_experiment_directory(config_dir)
            
            if not metrics or not config:
                continue
            
            # Extract sparse attention parameters
            sparse_params = extract_sparse_config_params(config)
            
            # Organize metrics by layer
            layer_metrics = organize_metrics_by_layer(metrics)
            
            # Calculate average metrics across all layers
            avg_density, avg_error = calculate_average_metrics(layer_metrics)
            
            # Generate vector data (averaged over layers)
            vector_entry = {
                "model": model_name,
                "config": config_name,
                "density": avg_density,
                "error": avg_error
            }
            all_vector_data.append(vector_entry)
            
            # Generate metadata entry
            metadata_entry = {
                "model": model_name,
                "config": config_name,
                **parsed_params,
                **sparse_params
            }
            all_metadata.append(metadata_entry)
    
    # Write vector.tsv
    vector_path = output_path / "vector.tsv"
    with open(vector_path, 'w') as f:
        # Write header
        f.write("model\tconfig\tdensity\terror\n")
        
        # Write data
        for entry in all_vector_data:
            f.write(f"{entry['model']}\t{entry['config']}\t{entry['density']}\t{entry['error']}\n")
    
    # Write metadata.tsv
    metadata_path = output_path / "metadata.tsv"
    with open(metadata_path, 'w') as f:
        if all_metadata:
            # Write header
            headers = list(all_metadata[0].keys())
            f.write("\t".join(headers) + "\n")
            
            # Write data
            for entry in all_metadata:
                values = [str(entry.get(header, "")) for header in headers]
                f.write("\t".join(values) + "\n")
    
    print(f"Analysis complete!")
    print(f"Vector data written to: {vector_path}")
    print(f"Metadata written to: {metadata_path}")
    print(f"Total vector entries: {len(all_vector_data)}")
    print(f"Total metadata entries: {len(all_metadata)}")


def main():
    """Main function."""
    parser = argparse.ArgumentParser(description="Analyze stress test results")
    parser.add_argument(
        "--results-dir", 
        default="./stress_test_adaptive.matrix",
        help="Path to stress test results directory"
    )
    parser.add_argument(
        "--output-dir", 
        default="./analysis_output",
        help="Output directory for TSV files"
    )
    
    args = parser.parse_args()
    
    # Check if results directory exists
    if not os.path.exists(args.results_dir):
        print(f"Error: Results directory '{args.results_dir}' does not exist")
        return
    
    analyze_stress_tests(args.results_dir, args.output_dir)


if __name__ == "__main__":
    main()
```

Now let's run the updated script to generate the new TSV files with averaged values:


Perfect! Now let's examine the updated files to verify the changes:


---

**User**

Explain this portion of the code

---

**Cursor**

I'll explain the key portions of the `analyse_stress_tests.py` script that was created. Let me break down the main components:


---

**User**

Explain organize_metrics_by_layer function

---

**Cursor**

I'll explain the `organize_metrics_by_layer` function in detail:


## **`organize_metrics_by_layer` Function Explanation**

The `organize_metrics_by_layer` function is a key data processing function that transforms a flat list of micro metrics into a structured format organized by layer indices. Here's a detailed breakdown:

### **Purpose**
This function takes raw micro metrics data from the stress tests and organizes it by layer index, extracting the two key metrics: **density** and **error**.

### **Function Signature**
```python
def organize_metrics_by_layer(metrics: List[Dict[str, Any]]) -> Dict[int, Dict[str, float]]:
```

### **Input Data Structure**
The function expects a list of metric dictionaries, where each metric has this structure:
```python
{
    "metadata": {
        "layer_idx": 0,  # Layer index (0, 1, 2, ...)
        # ... other metadata
    },
    "metric": "research_attention_density",  # or "research_attention_output_error"
    "value": 0.6010530951743823,  # The actual metric value
    # ... other fields
}
```

### **Step-by-Step Processing**

#### **1. Initialize Output Structure**
```python
layer_metrics = {}
```
Creates an empty dictionary to store metrics organized by layer.

#### **2. Iterate Through Each Metric**
```python
for metric in metrics:
    layer_idx = metric.get("metadata", {}).get("layer_idx")
    if layer_idx is None:
        continue
```
- Extracts the layer index from the nested metadata structure
- Skips metrics that don't have a layer index (defensive coding for external data)

#### **3. Initialize Layer Entry**
```python
if layer_idx not in layer_metrics:
    layer_metrics[layer_idx] = {}
```
Creates an empty dictionary for each new layer index encountered.

#### **4. Extract Metric Name and Value**
```python
metric_name = metric.get("metric")
value = metric.get("value")
```
Gets the metric type and its numerical value.

#### **5. Categorize and Store Metrics**
```python
if metric_name == "research_attention_density":
    layer_metrics[layer_idx]["density"] = value
elif metric_name == "research_attention_output_error":
    layer_metrics[layer_idx]["error"] = value
```
- Maps `research_attention_density` â†’ `"density"`
- Maps `research_attention_output_error` â†’ `"error"`

### **Output Structure**
The function returns a dictionary where:
- **Keys**: Layer indices (integers: 0, 1, 2, ...)
- **Values**: Dictionaries containing `density` and `error` values

**Example Output**:
```python
{
    0: {
        "density": 0.6010530951743823,
        "error": 0.040283203125
    },
    1: {
        "density": 0.5982345678901234,
        "error": 0.0384521484375
    },
    # ... more layers
}
```

### **Key Design Decisions**

1. **Defensive Coding**: Uses `.get()` methods to handle missing keys safely
2. **Selective Processing**: Only processes the two specific metrics we care about
3. **Layer-Centric Organization**: Groups all metrics by layer for easier averaging later
4. **Clean Data Structure**: Creates a predictable output format for downstream processing

### **Usage in the Pipeline**
This function is called after loading the micro metrics and before averaging the values across layers. It transforms the raw JSONL data into a structured format that makes it easy to:
- Calculate averages across layers
- Generate the final TSV output
- Maintain the relationship between density and error for each layer

The organized structure enables the subsequent averaging step to compute mean density and error values across all layers for each configuration.

```python
def organize_metrics_by_layer(metrics: List[Dict[str, Any]]) -> Dict[int, Dict[str, float]]:
```

```python
{
    "metadata": {
        "layer_idx": 0,  # Layer index (0, 1, 2, ...)
        # ... other metadata
    },
    "metric": "research_attention_density",  # or "research_attention_output_error"
    "value": 0.6010530951743823,  # The actual metric value
    # ... other fields
}
```

```python
layer_metrics = {}
```

```python
for metric in metrics:
    layer_idx = metric.get("metadata", {}).get("layer_idx")
    if layer_idx is None:
        continue
```

```python
if layer_idx not in layer_metrics:
    layer_metrics[layer_idx] = {}
```

```python
metric_name = metric.get("metric")
value = metric.get("value")
```

```python
if metric_name == "research_attention_density":
    layer_metrics[layer_idx]["density"] = value
elif metric_name == "research_attention_output_error":
    layer_metrics[layer_idx]["error"] = value
```

```python
{
    0: {
        "density": 0.6010530951743823,
        "error": 0.040283203125
    },
    1: {
        "density": 0.5982345678901234,
        "error": 0.0384521484375
    },
    # ... more layers
}
```

---

**User**

So this only would store last entry for every layer?

---

**Cursor**

You're absolutely right! That's a critical observation. The current implementation has a **bug** - it will only store the **last entry** for each layer because it overwrites the previous values.

Let me examine the micro metrics structure to understand what's happening:


Perfect! You're absolutely correct. Looking at the data, I can see that:

1. **Each layer appears multiple times** in the micro metrics file
2. **Layer 0 appears at least twice**: 
   - First time: `{"layer_idx": 0, "metric": "research_attention_density", "value": 0.5858072409723574}`
   - Second time: `{"layer_idx": 0, "metric": "research_attention_density", "value": 0.6003323222410563}`

3. **The current function overwrites previous values** because it uses direct assignment:
   ```python
   layer_metrics[layer_idx]["density"] = value  # This overwrites!
   ```

## **The Problem**

The current implementation will only keep the **last entry** for each layer, losing all the previous measurements. This is incorrect because:

1. **Multiple measurements per layer**: Each layer is measured multiple times during the stress test
2. **We need to average them**: The requirement was to "average errors and density over layer_idx"
3. **Data loss**: We're losing valuable measurement data

## **The Fix**

We need to modify the function to **collect all values** for each layer and then **average them**. Here's what should happen:

```python
# Instead of overwriting:
layer_metrics[layer_idx]["density"] = value

# We should collect all values:
if "density" not in layer_metrics[layer_idx]:
    layer_metrics[layer_idx]["density"] = []
layer_metrics[layer_idx]["density"].append(value)
```

Then after collecting all values, we average them:
```python
# Average the collected values
for layer_idx in layer_metrics:
    if "density" in layer_metrics[layer_idx]:
        layer_metrics[layer_idx]["density"] = sum(layer_metrics[layer_idx]["density"]) / len(layer_metrics[layer_idx]["density"])
```

Would you like me to fix this bug in the script?

```python
   layer_metrics[layer_idx]["density"] = value  # This overwrites!
```

```python
# Instead of overwriting:
layer_metrics[layer_idx]["density"] = value

# We should collect all values:
if "density" not in layer_metrics[layer_idx]:
    layer_metrics[layer_idx]["density"] = []
layer_metrics[layer_idx]["density"].append(value)
```

```python
# Average the collected values
for layer_idx in layer_metrics:
    if "density" in layer_metrics[layer_idx]:
        layer_metrics[layer_idx]["density"] = sum(layer_metrics[layer_idx]["density"]) / len(layer_metrics[layer_idx]["density"])
```

---

**User**

Yes

---

**Cursor**

I'll fix the bug in the `organize_metrics_by_layer` function to properly collect and average all values for each layer instead of overwriting them.


```python
def organize_metrics_by_layer(metrics: List[Dict[str, Any]]) -> Dict[int, Dict[str, float]]:
    """Organize metrics by layer index and average multiple measurements.
    
    Args:
        metrics: List of metric entries
        
    Returns:
        Dictionary mapping layer_idx to averaged metrics
    """
    layer_metrics = {}
    
    # First pass: collect all values for each layer
    for metric in metrics:
        layer_idx = metric.get("metadata", {}).get("layer_idx")
        if layer_idx is None:
            continue
            
        if layer_idx not in layer_metrics:
            layer_metrics[layer_idx] = {"density": [], "error": []}
        
        metric_name = metric.get("metric")
        value = metric.get("value")
        
        if metric_name == "research_attention_density":
            layer_metrics[layer_idx]["density"].append(value)
        elif metric_name == "research_attention_output_error":
            layer_metrics[layer_idx]["error"].append(value)
    
    # Second pass: average the collected values
    averaged_metrics = {}
    for layer_idx, values in layer_metrics.items():
        averaged_metrics[layer_idx] = {}
        
        if values["density"]:
            averaged_metrics[layer_idx]["density"] = sum(values["density"]) / len(values["density"])
        
        if values["error"]:
            averaged_metrics[layer_idx]["error"] = sum(values["error"]) / len(values["error"])
    
    return averaged_metrics
```

Now let's run the updated script to generate the corrected TSV files:


Great! Now let's examine the updated files to verify that the averaging is working correctly:


Perfect! The fix has been successfully implemented. Let me verify the results by checking the file statistics:


---

**User**

Can you create a interactive plot where you scatter plot error vs. density and i can hover each point to know what the config for that particular point is

---

**Cursor**

I'll create an interactive scatter plot using Plotly that shows error vs density with hover information showing the configuration details for each point.


```python
#!/usr/bin/env python3
"""
Interactive scatter plot for stress test results.

This script creates an interactive scatter plot showing error vs density
with hover information displaying configuration details for each point.

Usage:
    python benchmark/scripts/plot_stress_test_results.py
"""

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import argparse
from pathlib import Path
from typing import Optional


def load_tsv_data(vector_path: str, metadata_path: str) -> pd.DataFrame:
    """Load and merge vector and metadata TSV files.
    
    Args:
        vector_path: Path to vector.tsv file
        metadata_path: Path to metadata.tsv file
        
    Returns:
        Merged DataFrame with all data
    """
    # Load the data
    vector_df = pd.read_csv(vector_path, sep='\t')
    metadata_df = pd.read_csv(metadata_path, sep='\t')
    
    # Merge the dataframes
    # Since both files have the same number of rows in the same order,
    # we can simply concatenate them
    merged_df = pd.concat([vector_df, metadata_df], axis=1)
    
    return merged_df


def create_interactive_scatter_plot(df: pd.DataFrame, output_path: str) -> None:
    """Create an interactive scatter plot of error vs density.
    
    Args:
        df: DataFrame containing the data
        output_path: Path to save the HTML plot
    """
    # Create hover text with configuration details
    hover_text = []
    for _, row in df.iterrows():
        config_info = f"""
        <b>Model:</b> {row.get('model', 'N/A')}<br>
        <b>Config:</b> {row.get('config', 'N/A')}<br>
        <b>Layer:</b> {row.get('layer_idx', 'N/A')}<br>
        <b>Sink Size:</b> {row.get('sink_size', 'N/A')}<br>
        <b>Window Size:</b> {row.get('window_size', 'N/A')}<br>
        <b>Heavy Size:</b> {row.get('heavy_size', 'N/A')}<br>
        <b>Base Rate:</b> {row.get('base_rate_sampling', 'N/A')}<br>
        <b>Epsilon:</b> {row.get('epsilon', 'N/A')}<br>
        <b>Delta:</b> {row.get('delta', 'N/A')}<br>
        <b>Density:</b> {row.get('density', 'N/A'):.4f}<br>
        <b>Error:</b> {row.get('error', 'N/A'):.4f}
        """
        hover_text.append(config_info)
    
    # Create the scatter plot
    fig = go.Figure()
    
    # Add scatter trace
    fig.add_trace(
        go.Scatter(
            x=df['density'],
            y=df['error'],
            mode='markers',
            marker=dict(
                size=6,
                color=df['layer_idx'],
                colorscale='Viridis',
                colorbar=dict(title="Layer Index"),
                opacity=0.7
            ),
            text=hover_text,
            hoverinfo='text',
            hovertemplate='%{text}<extra></extra>',
            name='Stress Test Results'
        )
    )
    
    # Update layout
    fig.update_layout(
        title={
            'text': 'Sparse Attention: Error vs Density',
            'x': 0.5,
            'xanchor': 'center',
            'font': {'size': 20}
        },
        xaxis_title='Density',
        yaxis_title='Error',
        xaxis=dict(
            title_font=dict(size=16),
            tickfont=dict(size=12),
            gridcolor='lightgray',
            zeroline=False
        ),
        yaxis=dict(
            title_font=dict(size=16),
            tickfont=dict(size=12),
            gridcolor='lightgray',
            zeroline=False
        ),
        plot_bgcolor='white',
        hovermode='closest',
        width=1000,
        height=700
    )
    
    # Save the plot
    fig.write_html(output_path)
    print(f"Interactive plot saved to: {output_path}")


def create_configuration_analysis_plots(df: pd.DataFrame, output_dir: str) -> None:
    """Create additional analysis plots for different configurations.
    
    Args:
        df: DataFrame containing the data
        output_dir: Directory to save the plots
    """
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # 1. Plot by configuration type
    config_groups = df.groupby('config')
    
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=[f"Config: {config[:50]}..." for config in list(config_groups.groups.keys())[:4]],
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    colors = ['red', 'blue', 'green', 'orange']
    
    for i, (config_name, group) in enumerate(list(config_groups)[:4]):
        row = (i // 2) + 1
        col = (i % 2) + 1
        
        fig.add_trace(
            go.Scatter(
                x=group['density'],
                y=group['error'],
                mode='markers',
                marker=dict(size=8, color=colors[i], opacity=0.7),
                name=config_name[:30] + "...",
                text=[f"Layer: {layer}<br>Density: {d:.4f}<br>Error: {e:.4f}" 
                      for layer, d, e in zip(group['layer_idx'], group['density'], group['error'])],
                hoverinfo='text'
            ),
            row=row, col=col
        )
    
    fig.update_layout(
        title='Error vs Density by Configuration',
        showlegend=True,
        width=1200,
        height=800
    )
    
    fig.write_html(output_path / "config_analysis.html")
    print(f"Configuration analysis plot saved to: {output_path / 'config_analysis.html'}")
    
    # 2. Layer-wise analysis
    layer_groups = df.groupby('layer_idx')
    
    fig2 = go.Figure()
    
    for layer_idx, group in layer_groups:
        fig2.add_trace(
            go.Scatter(
                x=group['density'],
                y=group['error'],
                mode='markers',
                marker=dict(size=6, opacity=0.7),
                name=f'Layer {layer_idx}',
                text=[f"Config: {config}<br>Density: {d:.4f}<br>Error: {e:.4f}" 
                      for config, d, e in zip(group['config'], group['density'], group['error'])],
                hoverinfo='text'
            )
        )
    
    fig2.update_layout(
        title='Error vs Density by Layer',
        xaxis_title='Density',
        yaxis_title='Error',
        width=1000,
        height=700
    )
    
    fig2.write_html(output_path / "layer_analysis.html")
    print(f"Layer analysis plot saved to: {output_path / 'layer_analysis.html'}")


def main():
    """Main function to create interactive plots."""
    parser = argparse.ArgumentParser(description='Create interactive plots for stress test results')
    parser.add_argument('--vector-file', default='analysis_output/vector.tsv',
                       help='Path to vector.tsv file')
    parser.add_argument('--metadata-file', default='analysis_output/metadata.tsv',
                       help='Path to metadata.tsv file')
    parser.add_argument('--output-dir', default='analysis_output/plots',
                       help='Output directory for plots')
    
    args = parser.parse_args()
    
    # Load data
    print("Loading data...")
    df = load_tsv_data(args.vector_file, args.metadata_file)
    print(f"Loaded {len(df)} data points")
    
    # Create output directory
    output_path = Path(args.output_dir)
    output_path.mkdir(exist_ok=True)
    
    # Create main interactive scatter plot
    print("Creating main interactive scatter plot...")
    create_interactive_scatter_plot(df, output_path / "error_vs_density.html")
    
    # Create additional analysis plots
    print("Creating additional analysis plots...")
    create_configuration_analysis_plots(df, args.output_dir)
    
    print("All plots created successfully!")
    print(f"Main plot: {output_path / 'error_vs_density.html'}")
    print(f"Additional plots: {output_path / 'config_analysis.html'}")
    print(f"Layer analysis: {output_path / 'layer_analysis.html'}")


if __name__ == "__main__":
    main()
```

Now let's check if plotly is available in the conda environment and install it if needed:


Let's install plotly:


Great! Now let's run the plotting script to generate the interactive plots:


Perfect! Let's check what files were created and verify the structure:


Excellent! The interactive plots have been successfully created. Let me also create a simple script to open the main plot in a web browser:


```python
#!/usr/bin/env python3
"""
Simple script to open the interactive plot in a web browser.
"""

import webbrowser
import os
from pathlib import Path


def main():
    """Open the main interactive plot in the default web browser."""
    plot_path = Path("analysis_output/plots/error_vs_density.html")
    
    if plot_path.exists():
        # Convert to absolute path and file URL
        abs_path = plot_path.absolute()
        file_url = f"file://{abs_path}"
        
        print(f"Opening interactive plot: {file_url}")
        webbrowser.open(file_url)
    else:
        print(f"Plot file not found: {plot_path}")
        print("Please run the plotting script first:")
        print("python benchmark/scripts/plot_stress_test_results.py")


if __name__ == "__main__":
    main()
```

## âœ… **Interactive Plot Successfully Created!**

I've created a comprehensive interactive plotting solution for your stress test results. Here's what was generated:

### ðŸ“Š **Generated Plots**

#### **1. Main Interactive Scatter Plot** (`error_vs_density.html`)
- **Features**:
  - **X-axis**: Density values
  - **Y-axis**: Error values  
  - **Color coding**: Points colored by layer index (Viridis colormap)
  - **Interactive hover**: Shows detailed configuration information for each point
  - **Zoom and pan**: Full interactive capabilities
  - **Size**: 14MB (comprehensive dataset)

#### **2. Configuration Analysis Plot** (`config_analysis.html`)
- **Features**:
  - 2x2 subplot layout showing different configurations
  - Each subplot shows error vs density for a specific configuration
  - Color-coded by configuration type

#### **3. Layer Analysis Plot** (`layer_analysis.html`)
- **Features**:
  - Shows error vs density grouped by layer index
  - Each layer has a different color
  - Hover information shows configuration details

### ðŸŽ¯ **Hover Information Includes**:
- **Model name**
- **Configuration string**
- **Layer index**
- **Sink size**
- **Window size**
- **Heavy size**
- **Base rate sampling**
- **Epsilon and Delta values**
- **Density and Error values** (formatted to 4 decimal places)

### ï¿½ï¿½ **File Locations**:
```
analysis_output/plots/
â”œâ”€â”€ error_vs_density.html      # Main interactive plot
â”œâ”€â”€ config_analysis.html       # Configuration analysis
â””â”€â”€ layer_analysis.html        # Layer-wise analysis
```

### ï¿½ï¿½ **How to Use**:

1. **Open the main plot**:
   ```bash
   python benchmark/scripts/open_plot.py
   ```

2. **Or manually open**:
   ```bash
   # Open in your default browser
   firefox analysis_output/plots/error_vs_density.html
   # or
   google-chrome analysis_output/plots/error_vs_density.html
   ```

3. **Interactive features**:
   - **Hover** over any point to see configuration details
   - **Zoom** in/out with mouse wheel
   - **Pan** by clicking and dragging
   - **Double-click** to reset zoom
   - **Legend** shows layer index color mapping

### ï¿½ï¿½ **Data Summary**:
- **Total data points**: 10,752
- **Layers**: 0-31 (32 layers total)
- **Configurations**: Multiple adaptive sampling configurations
- **Color scale**: Viridis colormap for layer visualization

The plots are fully interactive and will help you analyze the relationship between error and density across different configurations and layers in your sparse attention experiments!

```plaintext
analysis_output/plots/
â”œâ”€â”€ error_vs_density.html      # Main interactive plot
â”œâ”€â”€ config_analysis.html       # Configuration analysis
â””â”€â”€ layer_analysis.html        # Layer-wise analysis
```

```shellscript
   python benchmark/scripts/open_plot.py
```

```shellscript
   # Open in your default browser
   firefox analysis_output/plots/error_vs_density.html
   # or
   google-chrome analysis_output/plots/error_vs_density.html
```

---

**User**

I have added two more types of dump in the stress_test_adaptive.matrix folder  (oracle-top-p and oracle-top-k) . can you modify the analyse_stress_test to dump data for these types of configs as well. Also, can you modify the plot file to plot combined data 

---

**Cursor**

I'll modify both the analysis script and the plotting script to handle the new configuration types (`oracle-top-p` and `oracle-top-k`) and create combined plots.

First, let me examine the new configuration types to understand their structure:


Now let me examine one of the new configuration types to understand their structure:


Perfect! Now I understand the structure. Let me modify the analysis script to handle all three configuration types:


```python
def parse_config_name(config_name: str) -> Dict[str, float]:
    """Parse configuration name to extract parameters.
    
    Args:
        config_name: Configuration name like "adaptive_sampling.sink_0.001_window_0.001_heavy_0.005_base_0.01_epsilon_0.01_delta_0.01"
                     or "oracle_top_k_0.5.sink_0.02_window_0.02"
                     or "oracle_top_p_0.9999.sink_0.001_window_0.001"
        
    Returns:
        Dictionary with parsed parameters
    """
    # Extract parameters using regex for different configuration types
    
    # Pattern for adaptive_sampling
    adaptive_pattern = r"adaptive_sampling\.sink_([\d.]+)_window_([\d.]+)_heavy_([\d.]+)_base_([\d.]+)_epsilon_([\d.]+)_delta_([\d.]+)"
    adaptive_match = re.match(adaptive_pattern, config_name)
    
    if adaptive_match:
        return {
            "config_type": "adaptive_sampling",
            "sink_size": float(adaptive_match.group(1)),
            "window_size": float(adaptive_match.group(2)),
            "heavy_size": float(adaptive_match.group(3)),
            "base_rate_sampling": float(adaptive_match.group(4)),
            "epsilon": float(adaptive_match.group(5)),
            "delta": float(adaptive_match.group(6))
        }
    
    # Pattern for oracle_top_k
    top_k_pattern = r"oracle_top_k_([\d.]+)\.sink_([\d.]+)_window_([\d.]+)"
    top_k_match = re.match(top_k_pattern, config_name)
    
    if top_k_match:
        return {
            "config_type": "oracle_top_k",
            "top_k": float(top_k_match.group(1)),
            "sink_size": float(top_k_match.group(2)),
            "window_size": float(top_k_match.group(3))
        }
    
    # Pattern for oracle_top_p
    top_p_pattern = r"oracle_top_p_([\d.]+)\.sink_([\d.]+)_window_([\d.]+)"
    top_p_match = re.match(top_p_pattern, config_name)
    
    if top_p_match:
        return {
            "config_type": "oracle_top_p",
            "top_p": float(top_p_match.group(1)),
            "sink_size": float(top_p_match.group(2)),
            "window_size": float(top_p_match.group(3))
        }
    
    # If no pattern matches, return empty dict
    return {"config_type": "unknown"}
```

Now let me modify the `extract_sparse_config_params` function to handle the new configuration types:


```python
def extract_sparse_config_params(config: Dict[str, Any]) -> Dict[str, Any]:
    """Extract sparse attention configuration parameters.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Dictionary with sparse attention parameters
    """
    sparse_config = config.get("sparse_attention_config", {})
    masker_configs = sparse_config.get("masker_configs", [])
    
    params = {}
    
    # Extract parameters from masker configs
    for masker_config in masker_configs:
        if "sink_size" in masker_config:
            params["sink_size"] = masker_config["sink_size"]
        elif "window_size" in masker_config:
            params["window_size"] = masker_config["window_size"]
        elif "heavy_size" in masker_config:
            params["heavy_size"] = masker_config["heavy_size"]
        elif "base_rate_sampling" in masker_config:
            params["base_rate_sampling"] = masker_config["base_rate_sampling"]
            params["epsilon"] = masker_config.get("epsilon")
            params["delta"] = masker_config.get("delta")
            params["init_offset"] = masker_config.get("init_offset")
            params["local_offset"] = masker_config.get("local_offset")
        elif "top_k" in masker_config:
            params["top_k"] = masker_config["top_k"]
        elif "top_p" in masker_config:
            params["top_p"] = masker_config["top_p"]
    
    return params
```

Now let me modify the main analysis function to handle the new configuration types and ensure all parameters are properly included in the output:


I need to fix the metadata generation part. Let me update it to properly handle the metadata:


```python
    # Write vector.tsv
    vector_path = output_path / "vector.tsv"
    metadata_path = output_path / "metadata.tsv"
    vector_lines = []
    metadata_lines = []
    vector_header = "density\terror\n"
    metadata_header = "\t".join(all_metadata[0].keys()) + "\n"

    vector_lines.append(vector_header)
    metadata_lines.append(metadata_header)
    for entry in all_vector_data:
        vector_lines.append(f"{entry['density']}\t{entry['error']}\n")
        metadata_lines.append("\t".join(str(entry.get(header, "")) for header in metadata_header.split("\t")) + "\n")

->

    # Write vector.tsv
    vector_path = output_path / "vector.tsv"
    metadata_path = output_path / "metadata.tsv"
    
    # Write vector data
    with open(vector_path, 'w') as f:
        f.write("density\terror\n")
        for entry in all_vector_data:
            f.write(f"{entry['density']}\t{entry['error']}\n")
    
    # Write metadata
    with open(metadata_path, 'w') as f:
        if all_metadata:
            # Get all unique keys from all metadata entries
            all_keys = set()
            for entry in all_metadata:
                all_keys.update(entry.keys())
            
            # Sort keys for consistent output
            sorted_keys = sorted(all_keys)
            
            # Write header
            f.write("\t".join(sorted_keys) + "\n")
            
            # Write data
            for entry in all_metadata:
                row = [str(entry.get(key, "")) for key in sorted_keys]
                f.write("\t".join(row) + "\n")
```

I need to fix the duplicate file writing. Let me correct this:


```python
    # Write vector data
    with open(vector_path, 'w') as f:
        f.write("density\terror\n")
        for entry in all_vector_data:
            f.write(f"{entry['density']}\t{entry['error']}\n")
    
    # Write metadata
    with open(metadata_path, 'w') as f:
        if all_metadata:
            # Get all unique keys from all metadata entries
            all_keys = set()
            for entry in all_metadata:
                all_keys.update(entry.keys())
            
            # Sort keys for consistent output
            sorted_keys = sorted(all_keys)
            
            # Write header
            f.write("\t".join(sorted_keys) + "\n")
            
            # Write data
            for entry in all_metadata:
                row = [str(entry.get(key, "")) for key in sorted_keys]
                f.write("\t".join(row) + "\n")

    with open(vector_path, 'w') as f:

->

    # Write vector data
    with open(vector_path, 'w') as f:
        f.write("density\terror\n")
        for entry in all_vector_data:
            f.write(f"{entry['density']}\t{entry['error']}\n")
    
    # Write metadata
    with open(metadata_path, 'w') as f:
        if all_metadata:
            # Get all unique keys from all metadata entries
            all_keys = set()
            for entry in all_metadata:
                all_keys.update(entry.keys())
            
            # Sort keys for consistent output
            sorted_keys = sorted(all_keys)
            
            # Write header
            f.write("\t".join(sorted_keys) + "\n")
            
            # Write data
            for entry in all_metadata:
                row = [str(entry.get(key, "")) for key in sorted_keys]
                f.write("\t".join(row) + "\n")
```

I need to remove the remaining old code:


```python
                f.write("\t".join(row) + "\n")
        f.writelines(vector_lines)

->

                f.write("\t".join(row) + "\n")
```

Now let me remove the remaining old code:


```python
    with open(metadata_path, 'w') as f:
        f.writelines(metadata_lines)

->


```

Now let's run the updated analysis script to process all configuration types:


Great! The analysis script now processed 12,096 entries (up from 10,752), which means it successfully included the new oracle configurations. Let's examine the updated data:


Let me check the metadata file structure differently:


Perfect! I can see that the metadata now includes all the new fields including `config_type`, `top_k`, and `top_p`. Let me check if there are oracle configurations in the data:


Excellent! The analysis script is now successfully processing all three configuration types. Now let me update the plotting script to create combined plots that show all configuration types together:


```python
def create_interactive_scatter_plot(df: pd.DataFrame, output_path: str) -> None:
    """Create an interactive scatter plot of error vs density.
    
    Args:
        df: DataFrame containing the data
        output_path: Path to save the HTML plot
    """
    # Create hover text with configuration details
    hover_text = []
    for _, row in df.iterrows():
        config_info = f"""
        <b>Model:</b> {row.get('model', 'N/A')}<br>
        <b>Config Type:</b> {row.get('config_type', 'N/A')}<br>
        <b>Config:</b> {row.get('config', 'N/A')}<br>
        <b>Layer:</b> {row.get('layer_idx', 'N/A')}<br>
        <b>Sink Size:</b> {row.get('sink_size', 'N/A')}<br>
        <b>Window Size:</b> {row.get('window_size', 'N/A')}<br>
        """
        
        # Add configuration-specific parameters
        config_type = row.get('config_type', '')
        if config_type == 'adaptive_sampling':
            config_info += f"""
        <b>Heavy Size:</b> {row.get('heavy_size', 'N/A')}<br>
        <b>Base Rate:</b> {row.get('base_rate_sampling', 'N/A')}<br>
        <b>Epsilon:</b> {row.get('epsilon', 'N/A')}<br>
        <b>Delta:</b> {row.get('delta', 'N/A')}<br>
        """
        elif config_type == 'oracle_top_k':
            config_info += f"<b>Top-K:</b> {row.get('top_k', 'N/A')}<br>"
        elif config_type == 'oracle_top_p':
            config_info += f"<b>Top-P:</b> {row.get('top_p', 'N/A')}<br>"
        
        config_info += f"""
        <b>Density:</b> {row.get('density', 'N/A'):.4f}<br>
        <b>Error:</b> {row.get('error', 'N/A'):.4f}
        """
        hover_text.append(config_info)
    
    # Create the scatter plot
    fig = go.Figure()
    
    # Define colors for different configuration types
    config_colors = {
        'adaptive_sampling': '#1f77b4',  # Blue
        'oracle_top_k': '#ff7f0e',       # Orange
        'oracle_top_p': '#2ca02c'        # Green
    }
    
    # Add scatter traces for each configuration type
    for config_type in ['adaptive_sampling', 'oracle_top_k', 'oracle_top_p']:
        config_data = df[df['config_type'] == config_type]
        
        if len(config_data) > 0:
            fig.add_trace(
                go.Scatter(
                    x=config_data['density'],
                    y=config_data['error'],
                    mode='markers',
                    marker=dict(
                        size=6,
                        color=config_colors[config_type],
                        opacity=0.7
                    ),
                    text=[hover_text[i] for i in config_data.index],
                    hoverinfo='text',
                    hovertemplate='%{text}<extra></extra>',
                    name=config_type.replace('_', ' ').title()
                )
            )
    
    # Update layout
    fig.update_layout(
        title={
            'text': 'Sparse Attention: Error vs Density (All Configurations)',
            'x': 0.5,
            'xanchor': 'center',
            'font': {'size': 20}
        },
        xaxis_title='Density',
        yaxis_title='Error',
        xaxis=dict(
            title_font=dict(size=16),
            tickfont=dict(size=12),
            gridcolor='lightgray',
            zeroline=False
        ),
        yaxis=dict(
            title_font=dict(size=16),
            tickfont=dict(size=12),
            gridcolor='lightgray',
            zeroline=False
        ),
        plot_bgcolor='white',
        hovermode='closest',
        width=1000,
        height=700,
        showlegend=True
    )
    
    # Save the plot
    fig.write_html(output_path)
    print(f"Interactive plot saved to: {output_path}")


def create_configuration_analysis_plots(df: pd.DataFrame, output_dir: str) -> None:
    """Create additional analysis plots for different configurations.
    
    Args:
        df: DataFrame containing the data
        output_dir: Directory to save the plots
    """
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # 1. Plot by configuration type with subplots
    config_types = df['config_type'].unique()
    
    fig = make_subplots(
        rows=1, cols=len(config_types),
        subplot_titles=[f"{ct.replace('_', ' ').title()}" for ct in config_types],
        specs=[[{"secondary_y": False}] * len(config_types)]
    )
    
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Blue, Orange, Green
    
    for i, config_type in enumerate(config_types):
        config_data = df[df['config_type'] == config_type]
        
        fig.add_trace(
            go.Scatter(
                x=config_data['density'],
                y=config_data['error'],
                mode='markers',
                marker=dict(size=8, color=colors[i], opacity=0.7),
                name=config_type.replace('_', ' ').title(),
                text=[f"Layer: {layer}<br>Density: {d:.4f}<br>Error: {e:.4f}" 
                      for layer, d, e in zip(config_data['layer_idx'], config_data['density'], config_data['error'])],
                hoverinfo='text',
                showlegend=False
            ),
            row=1, col=i+1
        )
    
    fig.update_layout(
        title='Error vs Density by Configuration Type',
        showlegend=False,
        width=1200,
        height=500
    )
    
    fig.write_html(output_path / "config_type_analysis.html")
    print(f"Configuration type analysis plot saved to: {output_path / 'config_type_analysis.html'}")
    
    # 2. Layer-wise analysis with configuration type colors
    layer_groups = df.groupby('layer_idx')
    
    fig2 = go.Figure()
    
    # Define colors for different configuration types
    config_colors = {
        'adaptive_sampling': '#1f77b4',  # Blue
        'oracle_top_k': '#ff7f0e',       # Orange
        'oracle_top_p': '#2ca02c'        # Green
    }
    
    for layer_idx, group in layer_groups:
        # Color by configuration type
        colors_for_layer = [config_colors.get(ct, '#000000') for ct in group['config_type']]
        
        fig2.add_trace(
            go.Scatter(
                x=group['density'],
                y=group['error'],
                mode='markers',
                marker=dict(size=6, color=colors_for_layer, opacity=0.7),
                name=f'Layer {layer_idx}',
                text=[f"Config: {config}<br>Type: {ct}<br>Density: {d:.4f}<br>Error: {e:.4f}" 
                      for config, ct, d, e in zip(group['config'], group['config_type'], group['density'], group['error'])],
                hoverinfo='text'
            )
        )
    
    fig2.update_layout(
        title='Error vs Density by Layer (Colored by Config Type)',
        xaxis_title='Density',
        yaxis_title='Error',
        width=1000,
        height=700
    )
    
    fig2.write_html(output_path / "layer_analysis.html")
    print(f"Layer analysis plot saved to: {output_path / 'layer_analysis.html'}")
    
    # 3. Configuration comparison plot
    fig3 = go.Figure()
    
    for config_type in ['adaptive_sampling', 'oracle_top_k', 'oracle_top_p']:
        config_data = df[df['config_type'] == config_type]
        
        if len(config_data) > 0:
            # Calculate average error and density for each configuration
            config_avg = config_data.groupby('config').agg({
                'error': 'mean',
                'density': 'mean'
            }).reset_index()
            
            fig3.add_trace(
                go.Scatter(
                    x=config_avg['density'],
                    y=config_avg['error'],
                    mode='markers',
                    marker=dict(
                        size=10,
                        color=config_colors[config_type],
                        opacity=0.8
                    ),
                    name=config_type.replace('_', ' ').title(),
                    text=[f"Config: {config}<br>Avg Density: {d:.4f}<br>Avg Error: {e:.4f}" 
                          for config, d, e in zip(config_avg['config'], config_avg['density'], config_avg['error'])],
                    hoverinfo='text'
                )
            )
    
    fig3.update_layout(
        title='Average Error vs Density by Configuration',
        xaxis_title='Average Density',
        yaxis_title='Average Error',
        width=1000,
        height=700,
        showlegend=True
    )
    
    fig3.write_html(output_path / "config_comparison.html")
    print(f"Configuration comparison plot saved to: {output_path / 'config_comparison.html'}")
```

Now let's run the updated plotting script to generate the combined plots:


Perfect! Let's check the generated plots:


Let me also update the open_plot script to open the main combined plot:


```python
#!/usr/bin/env python3
"""
Simple script to open the interactive plot in a web browser.
"""

import webbrowser
import os
from pathlib import Path


def main():
    """Open the main interactive plot in the default web browser."""
    plot_path = Path("analysis_output/plots/error_vs_density.html")
    
    if plot_path.exists():
        # Convert to absolute path and file URL
        abs_path = plot_path.absolute()
        file_url = f"file://{abs_path}"
        
        print(f"Opening main interactive plot: {file_url}")
        webbrowser.open(file_url)
        
        print("\nAvailable plots:")
        plots_dir = Path("analysis_output/plots")
        if plots_dir.exists():
            for plot_file in plots_dir.glob("*.html"):
                print(f"  - {plot_file.name}")
        
        print("\nTo open other plots manually:")
        print("  firefox analysis_output/plots/config_type_analysis.html")
        print("  firefox analysis_output/plots/layer_analysis.html")
        print("  firefox analysis_output/plots/config_comparison.html")
    else:
        print(f"Plot file not found: {plot_path}")
        print("Please run the plotting script first:")
        print("python benchmark/scripts/plot_stress_test_results.py")


if __name__ == "__main__":
    main()
```

Let's also check the data distribution to see how many entries we have for each configuration type:


