# Planning benchmark executor implementation
_Exported on 7/16/2025 at 12:29:35 PDT from Cursor (1.2.4)_

---

**User**

Understand teh sparse-attention-hub. I want to add a BenchmarkExecutor implementation in benchmark/ Let us discuss the scope of implementaiton and plan . I have the following in mind. Create a ./cursor_chats/plans/benchmark_executor_plan.md -- create a comprehensive plan with minimalistic features to implement this class. Let us iterate on any particular topics we need to figure out / discuss. At this point only help me plan this out. Do not write any code

I have the following in mind currrently

Create a BenchmarkExecutor class with the following 

	""" orchestrates the benchmark execution -- creates benchmark stubs and schedules them depending on GPU availability
	    in parallel if multiple GPUs are available using multiprocessing  """

inputs:
	model_names: List[str]
	benchmark_list : [name:subset]
	generation_kwargs
	model_kwargs
	tokenizer_kwargs
	adapter_name: str = huggingface
	max_concurrent_runs: int # caps the number of multi processes to run uses GPUs 0-(max_concurrent_runs -1 )
	

responsibility:
	create model_names x benchmark_list different benchmark_stubs 
	schedule them using multi-processing on different GPUs
	when a benchmark_stub is picked to run -- benchmark is created and run.
	run all of them
	

what do you think

---

**Cursor**

I'll help you plan out the BenchmarkExecutor implementation. Let me first understand the current sparse-attention-hub structure and then create a comprehensive plan document.

Now I'll create a comprehensive plan document for the BenchmarkExecutor implementation. Let me understand your requirements and design a minimalistic yet powerful architecture.

```markdown
# BenchmarkExecutor Implementation Plan

## Overview

This document outlines the design and implementation plan for a new `BenchmarkExecutor` class that orchestrates parallel benchmark execution across multiple GPUs using multiprocessing. The goal is to efficiently run combinations of models and benchmarks in parallel while managing GPU resources.

## Current State Analysis

### Existing Components
- **`Benchmark` (Abstract Base Class)**: Defines interface with `run_benchmark(adapter, result_dir)`
- **`ModelAdapterHF`**: HuggingFace adapter that creates models and handles sparse attention
- **Simple `BenchmarkExecutor`**: Single model/benchmark execution with result storage
- **Available Benchmarks**: LongBench, MockBenchmark, AIME2024/2025, InfiniteBench, Loogle, etc.

### Current Limitations
- Sequential execution only
- No multi-GPU support
- No parallel model/benchmark combinations
- Limited resource management

## Proposed Architecture

### Core Design Principles
1. **Minimal Resource Overhead**: Each process should only load one model at a time
2. **GPU Isolation**: Each process gets exclusive access to a specific GPU
3. **Fault Tolerance**: Failed benchmark runs shouldn't crash other processes
4. **Result Aggregation**: Centralized collection and storage of all results
5. **Progress Tracking**: Real-time monitoring of execution progress

### Class Interface

```python
class BenchmarkExecutor:
    """Orchestrates parallel benchmark execution across multiple GPUs."""
    
    def __init__(
        self,
        max_concurrent_runs: int = 1,
        result_storage: Optional[ResultStorage] = None,
        base_result_dir: str = "./benchmark_results"
    ):
        """Initialize the executor.
        
        Args:
            max_concurrent_runs: Number of parallel processes (uses GPUs 0 to max_concurrent_runs-1)
            result_storage: Storage backend for results
            base_result_dir: Base directory for storing benchmark results
        """
    
    def run_benchmark_matrix(
        self,
        model_names: List[str],
        benchmark_configs: List[BenchmarkConfig], 
        adapter_config: AdapterConfig,
        generation_kwargs: Optional[Dict[str, Any]] = None
    ) -> ExecutionResults:
        """Run all combinations of models and benchmarks in parallel.
        
        Args:
            model_names: List of HuggingFace model names
            benchmark_configs: List of benchmark configurations 
            adapter_config: Configuration for model adapters
            generation_kwargs: Generation parameters for model inference
            
        Returns:
            Aggregated results from all benchmark runs
        """
```

### Configuration Classes

```python
@dataclass
class BenchmarkConfig:
    """Configuration for a benchmark execution."""
    benchmark_name: str  # e.g., "longbench", "mock_benchmark"
    subsets: Optional[List[str]] = None  # e.g., ["narrativeqa", "qasper"]
    
@dataclass 
class AdapterConfig:
    """Configuration for model adapters."""
    adapter_name: str = "huggingface"  # Future: support other adapters
    model_kwargs: Optional[Dict[str, Any]] = None
    tokenizer_kwargs: Optional[Dict[str, Any]] = None
    sparse_attention_config: Optional[SparseAttentionConfig] = None

@dataclass
class BenchmarkStub:
    """A single benchmark execution task."""
    model_name: str
    benchmark_config: BenchmarkConfig
    adapter_config: AdapterConfig
    generation_kwargs: Dict[str, Any]
    result_dir: str
    gpu_id: int
```

### Execution Flow

1. **Planning Phase**: Generate all benchmark stubs (model × benchmark combinations)
2. **Resource Allocation**: Assign GPU IDs to available processes
3. **Parallel Execution**: Distribute stubs across worker processes
4. **Progress Monitoring**: Track completion status and collect results
5. **Result Aggregation**: Combine results from all processes

## Implementation Details

### Phase 1: Core Multiprocessing Framework

#### Worker Process Function
```python
def _benchmark_worker(
    stub: BenchmarkStub,
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue
) -> None:
    """Worker function to execute a single benchmark in isolation."""
    # 1. Set GPU device 
    # 2. Create model adapter
    # 3. Instantiate benchmark
    # 4. Execute benchmark.run_benchmark()
    # 5. Report results/errors
```

#### Process Pool Management
- Use `multiprocessing.Pool` or `concurrent.futures.ProcessPoolExecutor`
- Implement process-safe result collection
- Handle worker failures gracefully

### Phase 2: GPU Resource Management

#### GPU Assignment Strategy
```python
def _assign_gpu_ids(self, num_stubs: int) -> List[int]:
    """Assign GPU IDs to benchmark stubs using round-robin."""
    # Round-robin assignment: [0, 1, ..., max_concurrent_runs-1, 0, 1, ...]
    return [i % self.max_concurrent_runs for i in range(num_stubs)]
```

#### Device Isolation
- Set `CUDA_VISIBLE_DEVICES` per process
- Ensure no GPU memory conflicts
- Handle GPU availability checking

### Phase 3: Progress Tracking & Results

#### Progress Monitoring
```python
class ExecutionProgress:
    """Tracks execution progress across all processes."""
    total_stubs: int
    completed_stubs: int
    failed_stubs: int
    current_executions: Dict[int, str]  # gpu_id -> current task description
```

#### Result Aggregation
```python
@dataclass
class ExecutionResults:
    """Aggregated results from benchmark execution."""
    execution_summary: Dict[str, Any]
    individual_results: List[BenchmarkResult]
    failed_executions: List[BenchmarkFailure]
    execution_time: float
```

## Directory Structure

```
benchmark_results/
├── execution_20250115_143022/           # Timestamped execution
│   ├── execution_summary.json          # Overall summary
│   ├── model_llama3.1_8b/              # Per-model directories
│   │   ├── longbench_narrativeqa/       # Per-benchmark subdirectories
│   │   │   ├── raw_results.csv
│   │   │   └── metrics.json
│   │   └── mock_benchmark/
│   │       ├── raw_results.csv
│   │       └── metrics.json
│   └── model_phi4_mini/
│       └── longbench_qasper/
│           ├── raw_results.csv
│           └── metrics.json
```

## Error Handling Strategy

### Process-Level Errors
- Model loading failures
- CUDA out-of-memory errors
- Dataset loading issues
- Generation timeouts

### Recovery Mechanisms
- Skip failed model/benchmark combinations
- Continue with remaining tasks
- Detailed error logging and reporting
- Optional retry logic for transient failures

## Configuration Examples

### Basic Usage
```python
executor = BenchmarkExecutor(max_concurrent_runs=2)

model_names = ["microsoft/Phi-4-mini-instruct", "meta-llama/Llama-3.1-8B-Instruct"]
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper"]),
    BenchmarkConfig("mock_benchmark")
]
adapter_config = AdapterConfig(
    model_kwargs={"torch_dtype": torch.bfloat16},
    sparse_attention_config=streaming_config
)

results = executor.run_benchmark_matrix(
    model_names=model_names,
    benchmark_configs=benchmark_configs, 
    adapter_config=adapter_config,
    generation_kwargs={"max_new_tokens": 256}
)
```

### Advanced Configuration
```python
# Custom result storage
storage = ResultStorage(storage_path="./my_results")
executor = BenchmarkExecutor(
    max_concurrent_runs=4,
    result_storage=storage,
    base_result_dir="./experiments/sparse_attention_eval"
)

# Multiple sparse attention configurations
configs = [
    AdapterConfig(sparse_attention_config=None),  # Dense baseline
    AdapterConfig(sparse_attention_config=streaming_config),
    AdapterConfig(sparse_attention_config=hash_attention_config)
]

# Batch execution with different configurations
for i, config in enumerate(configs):
    results = executor.run_benchmark_matrix(
        model_names=model_names,
        benchmark_configs=benchmark_configs,
        adapter_config=config
    )
```

## Future Extensions

### Phase 4: Advanced Features (Future)
1. **Dynamic GPU Allocation**: Add/remove GPUs during execution
2. **Checkpoint/Resume**: Save intermediate state for long-running experiments  
3. **Distributed Execution**: Support multiple machines
4. **Resource Monitoring**: GPU memory/utilization tracking
5. **Adaptive Scheduling**: Prioritize faster benchmarks
6. **Result Analysis**: Built-in comparison and visualization tools

### Extensibility Points
- **Adapter Support**: Plugin system for non-HuggingFace adapters
- **Custom Schedulers**: Alternative task distribution strategies
- **Result Exporters**: Custom result format support
- **Notification Systems**: Integration with monitoring tools

## Implementation Phases

### Phase 1: Minimal Viable Product (MVP)
- [ ] Basic `BenchmarkExecutor` class structure
- [ ] Simple multiprocessing with fixed GPU assignment
- [ ] Core benchmark stub generation and execution
- [ ] Basic result collection and storage
- [ ] Essential error handling

**Estimated Effort**: 2-3 days
**Goal**: Functional parallel execution of model/benchmark combinations

### Phase 2: Production Ready
- [ ] Robust error handling and recovery
- [ ] Progress tracking and monitoring
- [ ] Comprehensive result aggregation
- [ ] GPU resource validation
- [ ] Configuration validation and defaults

**Estimated Effort**: 2-3 days  
**Goal**: Reliable execution suitable for research experiments

### Phase 3: Quality & Polish
- [ ] Comprehensive unit tests
- [ ] Integration tests with real benchmarks
- [ ] Documentation and examples
- [ ] Performance optimization
- [ ] Memory management improvements

**Estimated Effort**: 2-3 days
**Goal**: Production-quality implementation ready for team use

## Discussion Points

### 1. GPU Memory Management
**Question**: How should we handle GPU memory between benchmark runs?
**Options**:
- Clear cache after each benchmark (safer, slower)
- Rely on automatic garbage collection (faster, riskier)
- Per-benchmark memory limits

**Recommendation**: Start with cache clearing, optimize later.

### 2. Process vs Thread Parallelism
**Question**: Should we use multiprocessing or multithreading?
**Analysis**: 
- **Multiprocessing Pros**: True parallelism, GPU isolation, fault tolerance
- **Multiprocessing Cons**: Memory overhead, IPC complexity
- **Threading Pros**: Lower overhead, shared memory
- **Threading Cons**: GIL limitations, shared GPU state issues

**Recommendation**: Multiprocessing for GPU isolation and fault tolerance.

### 3. Result Storage Strategy
**Question**: How should we organize results in the file system?
**Options**:
- Flat structure with naming conventions
- Hierarchical by model/benchmark
- Database-backed storage

**Recommendation**: Hierarchical file structure with JSON metadata for simplicity.

### 4. Configuration Complexity
**Question**: How complex should the configuration system be?
**Balance**: 
- Too simple: Limited flexibility
- Too complex: Hard to use

**Recommendation**: Start with simple dataclasses, add complexity as needed.

### 5. Benchmark Discovery
**Question**: Should benchmarks be auto-discovered or explicitly configured?
**Options**:
- Auto-discovery from benchmark registry
- Explicit benchmark class instantiation
- String-based benchmark names with factory

**Recommendation**: String-based names with factory pattern for simplicity.

## Next Steps

1. **Review and Discuss**: Gather feedback on architecture and design decisions
2. **Prioritize Features**: Decide which features are essential for MVP  
3. **Define Interfaces**: Finalize the public API surface
4. **Implementation Planning**: Break down into specific development tasks
5. **Testing Strategy**: Plan unit/integration testing approach

## Questions for Discussion

1. **GPU Requirements**: What's the typical GPU setup for your use case? (number of GPUs, memory constraints)
2. **Benchmark Scale**: How many model/benchmark combinations do you expect to run typically?
3. **Result Analysis**: What kind of result analysis/comparison features would be most valuable?
4. **Integration**: How should this integrate with existing sparse-attention-hub workflows?
5. **Performance Requirements**: What are the performance expectations? (throughput, memory usage)

This plan provides a solid foundation for implementing a robust, parallel benchmark execution system while maintaining simplicity and extensibility. The phased approach allows for iterative development and early validation of core concepts.
```

I've created a comprehensive plan for the BenchmarkExecutor implementation at `./cursor_chats/plans/benchmark_executor_plan.md`. Here's a summary of the key design decisions and discussion points:

## Plan Summary

### Core Architecture
Your vision aligns well with the existing sparse-attention-hub structure. I've designed a **parallel GPU-orchestrated BenchmarkExecutor** that builds on the existing `Benchmark` and `ModelAdapterHF` components while adding sophisticated resource management.

### Key Design Decisions

1. **Multiprocessing over Threading**: For true GPU isolation and fault tolerance
2. **GPU Round-Robin Assignment**: Simple but effective resource allocation
3. **Hierarchical Result Storage**: Organized by execution → model → benchmark
4. **Configuration-Driven**: Simple dataclasses for flexibility without complexity

### Interface Design
```python
class BenchmarkExecutor:
    def run_benchmark_matrix(
        self,
        model_names: List[str],                    # Your model_names
        benchmark_configs: List[BenchmarkConfig],   # Your benchmark_list
        adapter_config: AdapterConfig,             # Your model/tokenizer_kwargs + adapter_name
        generation_kwargs: Optional[Dict] = None   # Your generation_kwargs
    ) -> ExecutionResults:
```

### Key Components
- **BenchmarkStub**: Individual execution task (model × benchmark combination)
- **AdapterConfig**: Consolidates model_kwargs, tokenizer_kwargs, sparse_attention_config
- **BenchmarkConfig**: Benchmark name + subset configuration  
- **ExecutionResults**: Aggregated results with comprehensive metadata

## Discussion Points I'd Like Your Input On:

### 1. **GPU Setup & Scale**
- How many GPUs do you typically have available?
- What's a typical scale for your experiments? (e.g., 3 models × 5 benchmarks = 15 combinations)

### 2. **Benchmark Configuration**
Your `benchmark_list: [name:subset]` concept maps to:
```python
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper"]),  # name:subset
    BenchmarkConfig("mock_benchmark"),  # name only (all subsets)
]
```
Does this structure work for your use cases?

### 3. **Resource Management Strategy**
Should we prioritize:
- **Memory Safety**: Clear GPU cache after each benchmark (slower but safer)
- **Speed**: Rely on automatic cleanup (faster but riskier)
- **Hybrid**: Smart cleanup based on available memory

### 4. **Error Handling Approach**
How should failed benchmark runs be handled?
- Skip and continue with remaining tasks (recommended)
- Retry with different configurations
- Fail entire execution

### 5. **Result Analysis Features**
What kind of result aggregation/comparison would be most valuable?
- Simple summary tables
- Performance comparisons across models/benchmarks
- Integration with existing plotting systems

## Next Steps

1. **Review the detailed plan** in `cursor_chats/plans/benchmark_executor_plan.md`
2. **Discuss any concerns** or modifications to the architecture
3. **Prioritize features** for the MVP (I suggest starting with Phase 1: Core multiprocessing)
4. **Finalize the interface** based on your feedback

The plan follows your original vision while integrating cleanly with the existing codebase. The phased approach means we can get a working prototype quickly and iterate based on real usage patterns.

What are your thoughts on the overall direction? Any specific areas you'd like to modify or discuss further?

```python
class BenchmarkExecutor:
    def run_benchmark_matrix(
        self,
        model_names: List[str],                    # Your model_names
        benchmark_configs: List[BenchmarkConfig],   # Your benchmark_list
        adapter_config: AdapterConfig,             # Your model/tokenizer_kwargs + adapter_name
        generation_kwargs: Optional[Dict] = None   # Your generation_kwargs
    ) -> ExecutionResults:
```

```python
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper"]),  # name:subset
    BenchmarkConfig("mock_benchmark"),  # name only (all subsets)
]
```

---

**User**

I added comments. evaluate my comments and then rework the plan

---

**Cursor**

I'll review your comments and update the plan accordingly. Your feedback makes the design much more practical and efficient. Let me rework the plan with your modifications.

```markdown
### Current Limitations
- Sequential execution only
- No multi-GPU support
- No parallel model/benchmark combinations
- Limited resource management

->

### Current Limitations
- Sequential execution only
- No multi-GPU support
- No parallel model/benchmark combinations
- Limited resource management
- No resumable execution (re-runs completed benchmarks)
```

```markdown
### Core Design Principles
1. **Minimal Resource Overhead**: Each process should only load one model at a time
2. **GPU Isolation**: Each process gets exclusive access to a specific GPU
3. **Fault Tolerance**: Failed benchmark runs shouldn't crash other processes
4. **Result Aggregation**: Centralized collection and storage of all results #aditya(if a directory is present for a particular setting.. do not run that benchmark_stub)
5. **Progress Tracking**: Real-time monitoring of execution progress

->

### Core Design Principles
1. **Minimal Resource Overhead**: Each process should only load one model at a time
2. **Dynamic GPU Allocation**: Processes acquire GPUs dynamically as they become available
3. **Fault Tolerance**: Failed benchmark runs shouldn't crash other processes
4. **Resumable Execution**: Skip benchmark stubs that already have results (directory exists)
5. **GPU Validation**: Ensure model adapters can properly initialize on assigned GPUs
6. **Progress Tracking**: Real-time monitoring of execution progress
```

```markdown
    def __init__(
        self,
        max_concurrent_runs: int = 1,
        result_storage: Optional[ResultStorage] = None, #aditya(remove result storage -- only work with result_dir)
        base_result_dir: str = "./benchmark_results"
    ):
        """Initialize the executor.
        
        Args:
            max_concurrent_runs: Number of parallel processes (uses GPUs 0 to max_concurrent_runs-1)
            result_storage: Storage backend for results
            base_result_dir: Base directory for storing benchmark results
        """

->

    def __init__(
        self,
        max_concurrent_runs: int = 1,
        base_result_dir: str = "./benchmark_results"
    ):
        """Initialize the executor.
        
        Args:
            max_concurrent_runs: Maximum number of parallel processes to run
            base_result_dir: Base directory for storing benchmark results
        """
```

```markdown
### Execution Flow

1. **Planning Phase**: Generate all benchmark stubs (model × benchmark combinations)
2. **Resource Allocation**: Assign GPU IDs to available processes #aditya(check if adapter can be initialized on a particular GPU . i.e. model is hosted on that GPU and process_requests puts tokens on that GPU only)
3. **Parallel Execution**: Distribute stubs across worker processes
4. **Progress Monitoring**: Track completion status and collect results
5. **Result Aggregation**: Combine results from all processes

->

### Execution Flow

1. **Planning Phase**: Generate all benchmark stubs (model × benchmark combinations)
2. **Resumability Check**: Filter out stubs that already have result directories
3. **GPU Discovery**: Detect available GPUs and validate accessibility
4. **Dynamic Allocation**: Workers request GPUs from a shared pool as needed
5. **GPU Validation**: Test model adapter initialization on assigned GPU before execution
6. **Parallel Execution**: Distribute remaining stubs across worker processes
7. **Progress Monitoring**: Track completion status and collect results
8. **Result Aggregation**: Combine results from all processes
```

```markdown
#### Worker Process Function
```python
def _benchmark_worker(
    stub: BenchmarkStub,
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue
) -> None:
    """Worker function to execute a single benchmark in isolation."""
    # 1. Set GPU device 
    # 2. Create model adapter
    # 3. Instantiate benchmark
    # 4. Execute benchmark.run_benchmark()
    # 5. Report results/errors
```

#### Process Pool Management
- Use `multiprocessing.Pool` or `concurrent.futures.ProcessPoolExecutor`
- Implement process-safe result collection
- Handle worker failures gracefully

->

#### Worker Process Function
```python
def _benchmark_worker(
    stub_queue: multiprocessing.Queue,
    gpu_pool: multiprocessing.Queue,
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue
) -> None:
    """Worker function to execute benchmarks with dynamic GPU allocation."""
    while True:
        # 1. Get next benchmark stub from queue
        # 2. Acquire GPU from pool
        # 3. Validate model adapter can initialize on GPU
        # 4. Create model adapter and benchmark instance
        # 5. Execute benchmark.run_benchmark()
        # 6. Release GPU back to pool
        # 7. Report results/errors
```

#### Process Pool Management
- Use `multiprocessing.Process` with shared queues for dynamic work distribution
- GPU pool as shared `multiprocessing.Queue` for dynamic allocation
- Implement process-safe result collection
- Handle worker failures gracefully and GPU cleanup
```

```markdown
### Phase 2: GPU Resource Management #aditya(do not use static GPU allocation. some processes might run faster than others.)

#### GPU Assignment Strategy
```python
def _assign_gpu_ids(self, num_stubs: int) -> List[int]:
    """Assign GPU IDs to benchmark stubs using round-robin."""
    # Round-robin assignment: [0, 1, ..., max_concurrent_runs-1, 0, 1, ...]
    return [i % self.max_concurrent_runs for i in range(num_stubs)]
```

#### Device Isolation
- Set `CUDA_VISIBLE_DEVICES` per process
- Ensure no GPU memory conflicts
- Handle GPU availability checking

->

### Phase 2: Dynamic GPU Resource Management

#### GPU Pool Management
```python
def _create_gpu_pool(self) -> multiprocessing.Queue:
    """Create a shared pool of available GPU IDs."""
    gpu_pool = multiprocessing.Queue()
    available_gpus = self._discover_available_gpus()
    for gpu_id in available_gpus:
        gpu_pool.put(gpu_id)
    return gpu_pool

def _discover_available_gpus(self) -> List[int]:
    """Discover and validate available GPUs."""
    # Check torch.cuda.device_count()
    # Validate each GPU is accessible
    # Return list of usable GPU IDs (0, 1, 2, ...)
```

#### GPU Validation Strategy
```python
def _validate_gpu_for_model(self, gpu_id: int, model_name: str) -> bool:
    """Test if model can be loaded on specific GPU."""
    # Temporarily set CUDA_VISIBLE_DEVICES
    # Try to create a minimal ModelAdapterHF instance
    # Return True if successful, False if failed
```

#### Device Isolation
- Workers set `CUDA_VISIBLE_DEVICES` to single GPU before model loading
- Automatic GPU release back to pool after benchmark completion
- Handle GPU memory cleanup between benchmarks
```

```markdown
### Phase 3: Progress Tracking & Results

#### Progress Monitoring
```python
class ExecutionProgress:
    """Tracks execution progress across all processes."""
    total_stubs: int
    completed_stubs: int
    failed_stubs: int
    current_executions: Dict[int, str]  # gpu_id -> current task description
```

->

### Phase 3: Progress Tracking & Results

#### Resumability Detection
```python
def _filter_existing_results(self, stubs: List[BenchmarkStub]) -> List[BenchmarkStub]:
    """Filter out benchmark stubs that already have result directories."""
    filtered_stubs = []
    for stub in stubs:
        result_dir = self._get_result_directory(stub)
        if not Path(result_dir).exists():
            filtered_stubs.append(stub)
        else:
            print(f"Skipping {stub.model_name}/{stub.benchmark_config.benchmark_name} - results exist")
    return filtered_stubs
```

#### Progress Monitoring
```python
class ExecutionProgress:
    """Tracks execution progress across all processes."""
    total_stubs: int
    skipped_stubs: int  # Already completed (resumability)
    completed_stubs: int
    failed_stubs: int
    current_executions: Dict[int, str]  # gpu_id -> current task description
```
```

```markdown
### Basic Usage
```python
executor = BenchmarkExecutor(max_concurrent_runs=2)

model_names = ["microsoft/Phi-4-mini-instruct", "meta-llama/Llama-3.1-8B-Instruct"]
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper"]),
    BenchmarkConfig("mock_benchmark")
]
adapter_config = AdapterConfig(
    model_kwargs={"torch_dtype": torch.bfloat16},
    sparse_attention_config=streaming_config
)

results = executor.run_benchmark_matrix(
    model_names=model_names,
    benchmark_configs=benchmark_configs, 
    adapter_config=adapter_config,
    generation_kwargs={"max_new_tokens": 256}
)
```

->

### Basic Usage
```python
executor = BenchmarkExecutor(
    max_concurrent_runs=2,
    base_result_dir="./my_benchmark_results"
)

model_names = ["microsoft/Phi-4-mini-instruct", "meta-llama/Llama-3.1-8B-Instruct"]
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper"]),
    BenchmarkConfig("mock_benchmark")
]
adapter_config = AdapterConfig(
    model_kwargs={"torch_dtype": torch.bfloat16},
    sparse_attention_config=streaming_config
)

results = executor.run_benchmark_matrix(
    model_names=model_names,
    benchmark_configs=benchmark_configs, 
    adapter_config=adapter_config,
    generation_kwargs={"max_new_tokens": 256}
)

# Results automatically saved to directory structure
# Resumable: re-running skips completed benchmarks
```
```

```markdown
### Advanced Configuration #aditya(do not use result storage -- remove this file / class)
```python
# Custom result storage
storage = ResultStorage(storage_path="./my_results")
executor = BenchmarkExecutor(
    max_concurrent_runs=4,
    result_storage=storage,
    base_result_dir="./experiments/sparse_attention_eval"
)

# Multiple sparse attention configurations
configs = [
    AdapterConfig(sparse_attention_config=None),  # Dense baseline
    AdapterConfig(sparse_attention_config=streaming_config),
    AdapterConfig(sparse_attention_config=hash_attention_config)
]

# Batch execution with different configurations
for i, config in enumerate(configs):
    results = executor.run_benchmark_matrix(
        model_names=model_names,
        benchmark_configs=benchmark_configs,
        adapter_config=config
    )
```

->

### Advanced Configuration - Multiple Sparse Attention Comparisons
```python
executor = BenchmarkExecutor(
    max_concurrent_runs=4,
    base_result_dir="./experiments/sparse_attention_comparison"
)

# Multiple sparse attention configurations for comparison
configs = [
    ("dense_baseline", AdapterConfig(sparse_attention_config=None)),
    ("streaming_llm", AdapterConfig(sparse_attention_config=streaming_config)),
    ("hash_attention", AdapterConfig(sparse_attention_config=hash_attention_config))
]

# Run comprehensive comparison study
model_names = ["meta-llama/Llama-3.1-8B-Instruct", "microsoft/Phi-4-mini-instruct"]
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper", "hotpotqa"]),
    BenchmarkConfig("mock_benchmark")
]

all_results = {}
for config_name, adapter_config in configs:
    print(f"Running configuration: {config_name}")
    
    # Each config gets its own result subdirectory
    results = executor.run_benchmark_matrix(
        model_names=model_names,
        benchmark_configs=benchmark_configs,
        adapter_config=adapter_config,
        generation_kwargs={"max_new_tokens": 512}
    )
    all_results[config_name] = results

# Results organized by configuration, model, and benchmark
# Resumable across different configurations
```
```

```markdown
## Directory Structure #aditya(directory structure looks good. make sure that is no existing code for result directory creation. If there is just use this one.)

```
benchmark_results/
├── execution_20250115_143022/           # Timestamped execution
│   ├── execution_summary.json          # Overall summary
│   ├── model_llama3.1_8b/              # Per-model directories
│   │   ├── longbench_narrativeqa/       # Per-benchmark subdirectories
│   │   │   ├── raw_results.csv
│   │   │   └── metrics.json
│   │   └── mock_benchmark/
│   │       ├── raw_results.csv
│   │       └── metrics.json
│   └── model_phi4_mini/
│       └── longbench_qasper/
│           ├── raw_results.csv
│           └── metrics.json
```

->

## Directory Structure

**Note**: This will be the standard directory structure we implement. No existing directory creation code will be reused - fresh implementation.

```
benchmark_results/
├── execution_20250115_143022/           # Timestamped execution
│   ├── execution_summary.json          # Overall summary
│   ├── model_llama3.1_8b/              # Per-model directories (sanitized names)
│   │   ├── longbench_narrativeqa/       # Per-benchmark subdirectories
│   │   │   ├── raw_results.csv          # Required by Benchmark.run_benchmark()
│   │   │   └── metrics.json             # Required by Benchmark.run_benchmark()
│   │   └── mock_benchmark/
│   │       ├── raw_results.csv
│   │       └── metrics.json
│   └── model_phi4_mini/
│       └── longbench_qasper/
│           ├── raw_results.csv
│           └── metrics.json
```

**Resumability**: If a `model_name/benchmark_name` directory exists, skip that benchmark stub.
```

```markdown
### Phase 1: Minimal Viable Product (MVP)
- [ ] Basic `BenchmarkExecutor` class structure
- [ ] Simple multiprocessing with fixed GPU assignment
- [ ] Core benchmark stub generation and execution
- [ ] Basic result collection and storage
- [ ] Essential error handling

**Estimated Effort**: 2-3 days
**Goal**: Functional parallel execution of model/benchmark combinations

->

### Phase 1: Minimal Viable Product (MVP)
- [ ] Basic `BenchmarkExecutor` class structure (no ResultStorage dependency)
- [ ] Dynamic GPU pool management with worker queues
- [ ] Core benchmark stub generation and filtering (resumability)
- [ ] GPU validation before model loading
- [ ] Basic result directory management
- [ ] Essential error handling and GPU cleanup

**Estimated Effort**: 2-3 days
**Goal**: Functional parallel execution with dynamic GPU allocation and resumability
```

```markdown
### Phase 2: Production Ready
- [ ] Robust error handling and recovery
- [ ] Progress tracking and monitoring
- [ ] Comprehensive result aggregation
- [ ] GPU resource validation
- [ ] Configuration validation and defaults

**Estimated Effort**: 2-3 days  
**Goal**: Reliable execution suitable for research experiments

->

### Phase 2: Production Ready
- [ ] Robust error handling and recovery
- [ ] Progress tracking and monitoring (including resumability stats)
- [ ] Comprehensive result aggregation and summary generation
- [ ] GPU resource validation and availability checking
- [ ] Configuration validation and sensible defaults
- [ ] Performance optimization for large benchmark matrices

**Estimated Effort**: 2-3 days  
**Goal**: Reliable execution suitable for research experiments with production-grade robustness
```

```markdown
## Discussion Points

### 1. GPU Memory Management
**Question**: How should we handle GPU memory between benchmark runs?
**Options**:
- Clear cache after each benchmark (safer, slower)
- Rely on automatic garbage collection (faster, riskier)
- Per-benchmark memory limits

**Recommendation**: Start with cache clearing, optimize later.

->

## Key Changes Based on Feedback

### Implemented Changes
1. **✅ Removed ResultStorage**: Simplified to directory-based result management only
2. **✅ Added Resumability**: Skip benchmark stubs with existing result directories  
3. **✅ Dynamic GPU Allocation**: Workers request GPUs from shared pool (no static assignment)
4. **✅ GPU Validation**: Test model loading on GPU before benchmark execution
5. **✅ Fresh Implementation**: No reuse of existing executor.py code

### Architecture Improvements
- **Queue-based Work Distribution**: More efficient than static task assignment
- **Fault Tolerance**: GPU failures don't block other workers
- **Resource Efficiency**: GPUs released immediately after use
- **Better Scalability**: Handles variable benchmark execution times

## Discussion Points

### 1. GPU Memory Management
**Question**: How should we handle GPU memory between benchmark runs?
**Options**:
- Clear cache after each benchmark (safer, slower)
- Rely on automatic garbage collection (faster, riskier)
- Per-benchmark memory limits

**Recommendation**: Start with cache clearing, optimize later with memory monitoring.
```

```markdown
## Questions for Discussion

1. **GPU Requirements**: What's the typical GPU setup for your use case? (number of GPUs, memory constraints)
2. **Benchmark Scale**: How many model/benchmark combinations do you expect to run typically?
3. **Result Analysis**: What kind of result analysis/comparison features would be most valuable?
4. **Integration**: How should this integrate with existing sparse-attention-hub workflows?
5. **Performance Requirements**: What are the performance expectations? (throughput, memory usage)

->

## Questions for Discussion

1. **GPU Requirements**: What's the typical GPU setup for your use case? (number of GPUs, memory constraints)
2. **Benchmark Scale**: How many model/benchmark combinations do you expect to run typically?
3. **Resumability Granularity**: Should resumability be at benchmark level or individual subset level?
4. **GPU Validation Depth**: How thorough should the GPU validation be? (minimal test vs full model loading)
5. **Result Directory Naming**: Any preferences for model name sanitization? (handle special characters, slashes, etc.)
6. **Error Recovery**: Should failed benchmarks be automatically retried or require manual intervention?
7. **Progress Reporting**: What level of progress detail is needed? (console output, log files, real-time dashboards)
```

```markdown
This plan provides a solid foundation for implementing a robust, parallel benchmark execution system while maintaining simplicity and extensibility. The phased approach allows for iterative development and early validation of core concepts.

->

## Summary

This reworked plan addresses all feedback points and provides a much more practical implementation approach:

### Core Strengths
- **Resumable Execution**: Never re-run completed benchmarks
- **Dynamic Resource Allocation**: Efficient GPU utilization regardless of benchmark timing
- **Simplified Architecture**: No unnecessary storage abstraction layers
- **Robust Validation**: GPU compatibility testing before expensive model loading
- **Fresh Implementation**: Clean, purpose-built solution

### Implementation Priority
1. **Phase 1**: Basic dynamic execution with resumability (MVP)
2. **Phase 2**: Production robustness and monitoring
3. **Phase 3**: Polish and optimization

This design balances simplicity with the sophisticated resource management needed for large-scale benchmark execution across multiple GPUs.
```

Perfect! I've reworked the plan based on all your comments. Here's a summary of the key changes I made:

## ✅ **Implemented All Your Feedback**

### 1. **Fresh Start** 
- I see you've cleared out `executor.py` - the plan now assumes a completely fresh implementation
- Should I also remove `benchmark/storage.py` since you want to eliminate ResultStorage entirely?

### 2. **Resumable Execution**
- Added resumability detection that skips benchmark stubs with existing result directories
- This means you can restart failed experiments without re-running completed benchmarks

### 3. **Dynamic GPU Allocation**
- Replaced static round-robin assignment with a dynamic GPU pool
- Workers request GPUs as needed and release them immediately after use
- Much more efficient for variable benchmark execution times

### 4. **GPU Validation**
- Added validation step to test model adapter initialization on specific GPUs
- Prevents expensive model loading failures during execution

### 5. **Simplified Architecture**
- Removed all ResultStorage dependencies
- Directory-based result management only
- Cleaner, more maintainable design

## **Updated Core Features**

### **New Execution Flow**
1. Generate benchmark stubs
2. **Filter out existing results** (resumability)
3. **Discover and validate available GPUs**
4. **Dynamic worker allocation** from GPU pool
5. Execute remaining benchmarks in parallel

### **Queue-Based Architecture**
```python
def _benchmark_worker(
    stub_queue: multiprocessing.Queue,     # Dynamic work distribution
    gpu_pool: multiprocessing.Queue,       # Shared GPU resource pool
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue
):
```

### **Smart Resource Management**
- GPU compatibility testing before model loading
- Automatic GPU cleanup and release
- No static assignments that waste resources

## **Key Questions for Finalization**

1. **Storage Cleanup**: Should I delete `benchmark/storage.py` since we're removing ResultStorage?

2. **Resumability Granularity**: Should we check for existing results at:
   - Benchmark level: `model_name/benchmark_name/` 
   - Subset level: `model_name/benchmark_name_subset/`

3. **GPU Validation Depth**: 
   - **Light**: Just check GPU accessibility
   - **Full**: Actually try loading a small model to validate memory/compatibility

4. **Directory Naming**: How should we handle model names with special characters?
   - `meta-llama/Llama-3.1-8B-Instruct` → `model_meta_llama_Llama_3.1_8B_Instruct`

The updated plan is now much more practical and addresses all your concerns. The dynamic allocation approach will be significantly more efficient than the original static assignment, especially when benchmarks have varying execution times.

Ready to move forward with implementation, or would you like to discuss any of these design decisions further?

```python
def _benchmark_worker(
    stub_queue: multiprocessing.Queue,     # Dynamic work distribution
    gpu_pool: multiprocessing.Queue,       # Shared GPU resource pool
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue
):
```

---

**User**

1. Yes
2. subset level
3. take list of GPUs to use  as input and max_concurrent_runs and then try to distribute processes on all GPUs. if it crashes with OOM thats users responsibility and they will reduce amx_concurrent_runs
4. yes. thats a good idea

---

**Cursor**

Perfect! I'll update the plan with your decisions and clean up the unnecessary files. Let me implement these changes:

```markdown
    def __init__(
        self,
        max_concurrent_runs: int = 1,
        base_result_dir: str = "./benchmark_results"
    ):
        """Initialize the executor.
        
        Args:
            max_concurrent_runs: Maximum number of parallel processes to run
            base_result_dir: Base directory for storing benchmark results
        """

->

    def __init__(
        self,
        gpu_ids: List[int],
        max_concurrent_runs: int,
        base_result_dir: str = "./benchmark_results"
    ):
        """Initialize the executor.
        
        Args:
            gpu_ids: List of GPU IDs to use for execution (e.g., [0, 1, 2, 3])
            max_concurrent_runs: Maximum number of parallel processes to run across GPUs
            base_result_dir: Base directory for storing benchmark results
            
        Note:
            If OOM errors occur, user should reduce max_concurrent_runs.
            Processes will be distributed across all specified GPU IDs.
        """
```

```markdown
#### GPU Pool Management
```python
def _create_gpu_pool(self) -> multiprocessing.Queue:
    """Create a shared pool of available GPU IDs."""
    gpu_pool = multiprocessing.Queue()
    available_gpus = self._discover_available_gpus()
    for gpu_id in available_gpus:
        gpu_pool.put(gpu_id)
    return gpu_pool

def _discover_available_gpus(self) -> List[int]:
    """Discover and validate available GPUs."""
    # Check torch.cuda.device_count()
    # Validate each GPU is accessible
    # Return list of usable GPU IDs (0, 1, 2, ...)
```

->

#### GPU Pool Management
```python
def _create_gpu_pool(self) -> multiprocessing.Queue:
    """Create a shared pool from user-specified GPU IDs."""
    gpu_pool = multiprocessing.Queue()
    self._validate_gpu_availability()
    for gpu_id in self.gpu_ids:
        gpu_pool.put(gpu_id)
    return gpu_pool

def _validate_gpu_availability(self) -> None:
    """Validate that specified GPUs are accessible."""
    # Check torch.cuda.device_count() >= max(self.gpu_ids)
    # Validate each GPU in self.gpu_ids is accessible
    # Raise error if any GPU is unavailable
```
```

```markdown
### Execution Flow

1. **Planning Phase**: Generate all benchmark stubs (model × benchmark combinations)
2. **Resumability Check**: Filter out stubs that already have result directories
3. **GPU Discovery**: Detect available GPUs and validate accessibility
4. **Dynamic Allocation**: Workers request GPUs from a shared pool as needed
5. **GPU Validation**: Test model adapter initialization on assigned GPU before execution
6. **Parallel Execution**: Distribute remaining stubs across worker processes
7. **Progress Monitoring**: Track completion status and collect results
8. **Result Aggregation**: Combine results from all processes

->

### Execution Flow

1. **Planning Phase**: Generate all benchmark stubs (model × benchmark × subset combinations)
2. **Resumability Check**: Filter out stubs that already have result directories (subset-level)
3. **GPU Validation**: Validate that specified GPU IDs are accessible
4. **GPU Pool Creation**: Create shared pool from user-specified GPU IDs
5. **Dynamic Allocation**: Workers request GPUs from pool as needed
6. **Model Validation**: Test model adapter initialization on assigned GPU before execution
7. **Parallel Execution**: Distribute remaining stubs across worker processes
8. **Progress Monitoring**: Track completion status and collect results
9. **Result Aggregation**: Combine results from all processes
```

```markdown
#### Resumability Detection
```python
def _filter_existing_results(self, stubs: List[BenchmarkStub]) -> List[BenchmarkStub]:
    """Filter out benchmark stubs that already have result directories."""
    filtered_stubs = []
    for stub in stubs:
        result_dir = self._get_result_directory(stub)
        if not Path(result_dir).exists():
            filtered_stubs.append(stub)
        else:
            print(f"Skipping {stub.model_name}/{stub.benchmark_config.benchmark_name} - results exist")
    return filtered_stubs
```

->

#### Resumability Detection (Subset-Level)
```python
def _filter_existing_results(self, stubs: List[BenchmarkStub]) -> List[BenchmarkStub]:
    """Filter out benchmark stubs that already have result directories at subset level."""
    filtered_stubs = []
    for stub in stubs:
        result_dir = self._get_result_directory(stub)  # Includes subset in path
        if not Path(result_dir).exists():
            filtered_stubs.append(stub)
        else:
            subset_info = f"_{stub.benchmark_config.subset}" if stub.benchmark_config.subset else ""
            print(f"Skipping {stub.model_name}/{stub.benchmark_config.benchmark_name}{subset_info} - results exist")
    return filtered_stubs

def _sanitize_model_name(self, model_name: str) -> str:
    """Sanitize model name for use in directory paths."""
    # Replace special characters: / \ : * ? " < > |
    # meta-llama/Llama-3.1-8B-Instruct -> meta_llama_Llama_3.1_8B_Instruct
    return model_name.replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
```
```

```markdown
@dataclass
class BenchmarkConfig:
    """Configuration for a benchmark execution."""
    benchmark_name: str  # e.g., "longbench", "mock_benchmark"
    subsets: Optional[List[str]] = None  # e.g., ["narrativeqa", "qasper"]

->

@dataclass
class BenchmarkConfig:
    """Configuration for a benchmark execution."""
    benchmark_name: str  # e.g., "longbench", "mock_benchmark"
    subsets: Optional[List[str]] = None  # e.g., ["narrativeqa", "qasper"]
    
    # Note: During stub generation, this expands to individual BenchmarkStub per subset
    # Each BenchmarkStub will have a single subset value for resumability
```

```markdown
@dataclass
class BenchmarkStub:
    """A single benchmark execution task."""
    model_name: str
    benchmark_config: BenchmarkConfig
    adapter_config: AdapterConfig
    generation_kwargs: Dict[str, Any]
    result_dir: str
    gpu_id: int

->

@dataclass
class BenchmarkStub:
    """A single benchmark execution task (model × benchmark × subset)."""
    model_name: str
    benchmark_name: str  # e.g., "longbench"
    subset: Optional[str]  # e.g., "narrativeqa" or None for full benchmark
    adapter_config: AdapterConfig
    generation_kwargs: Dict[str, Any]
    result_dir: str  # Full path including subset information
```

```markdown
```
benchmark_results/
├── execution_20250115_143022/           # Timestamped execution
│   ├── execution_summary.json          # Overall summary
│   ├── model_llama3.1_8b/              # Per-model directories (sanitized names)
│   │   ├── longbench_narrativeqa/       # Per-benchmark subdirectories
│   │   │   ├── raw_results.csv          # Required by Benchmark.run_benchmark()
│   │   │   └── metrics.json             # Required by Benchmark.run_benchmark()
│   │   └── mock_benchmark/
│   │       ├── raw_results.csv
│   │       └── metrics.json
│   └── model_phi4_mini/
│       └── longbench_qasper/
│           ├── raw_results.csv
│           └── metrics.json
```

**Resumability**: If a `model_name/benchmark_name` directory exists, skip that benchmark stub.

->

```
benchmark_results/
├── execution_20250115_143022/           # Timestamped execution
│   ├── execution_summary.json          # Overall summary
│   ├── meta_llama_Llama_3.1_8B_Instruct/  # Per-model directories (sanitized names)
│   │   ├── longbench_narrativeqa/       # Per-benchmark-subset subdirectories
│   │   │   ├── raw_results.csv          # Required by Benchmark.run_benchmark()
│   │   │   └── metrics.json             # Required by Benchmark.run_benchmark()
│   │   ├── longbench_qasper/            # Separate directory per subset
│   │   │   ├── raw_results.csv
│   │   │   └── metrics.json
│   │   └── mock_benchmark/              # Full benchmark (no subsets)
│   │       ├── raw_results.csv
│   │       └── metrics.json
│   └── microsoft_Phi_4_mini_instruct/   # Sanitized model name
│       └── longbench_hotpotqa/
│           ├── raw_results.csv
│           └── metrics.json
```

**Resumability**: If a `sanitized_model_name/benchmark_name_subset/` directory exists, skip that specific benchmark stub.
```

```markdown
### Basic Usage
```python
executor = BenchmarkExecutor(
    max_concurrent_runs=2,
    base_result_dir="./my_benchmark_results"
)

model_names = ["microsoft/Phi-4-mini-instruct", "meta-llama/Llama-3.1-8B-Instruct"]
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper"]),
    BenchmarkConfig("mock_benchmark")
]
adapter_config = AdapterConfig(
    model_kwargs={"torch_dtype": torch.bfloat16},
    sparse_attention_config=streaming_config
)

results = executor.run_benchmark_matrix(
    model_names=model_names,
    benchmark_configs=benchmark_configs, 
    adapter_config=adapter_config,
    generation_kwargs={"max_new_tokens": 256}
)

# Results automatically saved to directory structure
# Resumable: re-running skips completed benchmarks
```

->

### Basic Usage
```python
# Specify which GPUs to use and max concurrent processes
executor = BenchmarkExecutor(
    gpu_ids=[0, 1],  # Use GPUs 0 and 1
    max_concurrent_runs=4,  # Run up to 4 processes across these GPUs
    base_result_dir="./my_benchmark_results"
)

model_names = ["microsoft/Phi-4-mini-instruct", "meta-llama/Llama-3.1-8B-Instruct"]
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper"]),  # Creates 2 stubs
    BenchmarkConfig("mock_benchmark")  # Creates 1 stub (no subsets)
]
adapter_config = AdapterConfig(
    model_kwargs={"torch_dtype": torch.bfloat16},
    sparse_attention_config=streaming_config
)

# This creates 6 total stubs: 2 models × (2 longbench subsets + 1 mock_benchmark)
results = executor.run_benchmark_matrix(
    model_names=model_names,
    benchmark_configs=benchmark_configs, 
    adapter_config=adapter_config,
    generation_kwargs={"max_new_tokens": 256}
)

# Results automatically saved with subset-level directories
# Resumable: re-running skips completed benchmark stubs
# If OOM occurs, reduce max_concurrent_runs
```
```

```markdown
### Advanced Configuration - Multiple Sparse Attention Comparisons
```python
executor = BenchmarkExecutor(
    max_concurrent_runs=4,
    base_result_dir="./experiments/sparse_attention_comparison"
)

->

### Advanced Configuration - Multiple Sparse Attention Comparisons
```python
# Use all 4 available GPUs with high concurrency
executor = BenchmarkExecutor(
    gpu_ids=[0, 1, 2, 3],  # Specify all GPUs to use
    max_concurrent_runs=8,  # High concurrency across 4 GPUs
    base_result_dir="./experiments/sparse_attention_comparison"
)
```

```markdown
### Implemented Changes
1. **✅ Removed ResultStorage**: Simplified to directory-based result management only
2. **✅ Added Resumability**: Skip benchmark stubs with existing result directories  
3. **✅ Dynamic GPU Allocation**: Workers request GPUs from shared pool (no static assignment)
4. **✅ GPU Validation**: Test model loading on GPU before benchmark execution
5. **✅ Fresh Implementation**: No reuse of existing executor.py code

->

### Implemented Changes
1. **✅ Removed ResultStorage**: Simplified to directory-based result management only (storage.py deleted)
2. **✅ Subset-Level Resumability**: Skip benchmark stubs with existing result directories at subset granularity
3. **✅ User-Specified GPU Lists**: Take gpu_ids as input rather than auto-discovery
4. **✅ Dynamic GPU Allocation**: Workers request GPUs from shared pool (no static assignment)
5. **✅ GPU Validation**: Test model loading on GPU before benchmark execution
6. **✅ Model Name Sanitization**: Handle special characters in model names for directory paths
7. **✅ Fresh Implementation**: No reuse of existing executor.py code
```

```markdown
## Questions for Discussion

1. **GPU Requirements**: What's the typical GPU setup for your use case? (number of GPUs, memory constraints)
2. **Benchmark Scale**: How many model/benchmark combinations do you expect to run typically?
3. **Resumability Granularity**: Should resumability be at benchmark level or individual subset level?
4. **GPU Validation Depth**: How thorough should the GPU validation be? (minimal test vs full model loading)
5. **Result Directory Naming**: Any preferences for model name sanitization? (handle special characters, slashes, etc.)
6. **Error Recovery**: Should failed benchmarks be automatically retried or require manual intervention?
7. **Progress Reporting**: What level of progress detail is needed? (console output, log files, real-time dashboards)

->

## Finalized Design Decisions ✅

1. **✅ Storage**: Remove ResultStorage, use directory-based management only
2. **✅ Resumability**: Subset-level granularity
3. **✅ GPU Management**: User specifies gpu_ids list, max_concurrent_runs controls processes
4. **✅ Model Names**: Sanitize special characters for directory paths

## Remaining Questions for Implementation

1. **GPU Validation Depth**: How thorough should the GPU validation be? (minimal test vs full model loading)
2. **Error Recovery**: Should failed benchmarks be automatically retried or require manual intervention?
3. **Progress Reporting**: What level of progress detail is needed? (console output, log files, real-time dashboards)
4. **Benchmark Scale**: What's the typical scale of experiments? (helps optimize queue sizes, memory management)
```

```python
"""Benchmark execution and management."""

from typing import Any, Dict, List, Optional

from .base import Benchmark
from .storage import ResultStorage


class BenchmarkExecutor:
    """Executes benchmarks and manages results."""

->

"""Benchmark execution and management."""

from typing import Any, Dict, List, Optional

from .base import Benchmark


class BenchmarkExecutor:
    """Orchestrates parallel benchmark execution across multiple GPUs."""
```

Perfect! I've updated the plan to implement all your feedback. Here's a summary of the changes:

## ✅ **All Feedback Implemented**

### 1. **Removed ResultStorage (storage.py deleted)**
- Deleted `benchmark/storage.py` entirely 
- Removed import from `executor.py`
- Simplified to pure directory-based result management

### 2. **Subset-Level Resumability**
- **BenchmarkStub** now represents `model × benchmark × subset` (not just `model × benchmark`)
- Each subset gets its own result directory: `sanitized_model_name/benchmark_name_subset/`
- Resumability checks specific subset directories, not just benchmark-level

### 3. **User-Specified GPU Management**
```python
BenchmarkExecutor(
    gpu_ids=[0, 1, 2, 3],        # User specifies which GPUs to use
    max_concurrent_runs=8,       # Controls process count across GPUs
    base_result_dir="./results"
)
```
- No auto-discovery - user explicitly provides GPU list
- Dynamic allocation from the specified GPU pool
- OOM responsibility placed on user (reduce `max_concurrent_runs` if needed)

### 4. **Model Name Sanitization**
- Handles special characters: `meta-llama/Llama-3.1-8B-Instruct` → `meta_llama_Llama_3.1_8B_Instruct`
- Safe for all filesystems

## **Updated Architecture**

### **Execution Flow**
1. Generate stubs: `2 models × 3 subsets = 6 individual BenchmarkStubs`
2. **Filter existing**: Skip stubs with existing `model/benchmark_subset/` directories
3. **Validate GPUs**: Check user-specified GPU IDs are accessible
4. **Dynamic execution**: Workers pull stubs from queue, acquire GPUs as needed

### **Directory Structure**
```
benchmark_results/
├── execution_20250115_143022/
│   ├── meta_llama_Llama_3.1_8B_Instruct/
│   │   ├── longbench_narrativeqa/         # Subset-level resumability
│   │   ├── longbench_qasper/              # Each subset separate
│   │   └── mock_benchmark/                # Full benchmark (no subsets)
│   └── microsoft_Phi_4_mini_instruct/
│       └── longbench_hotpotqa/
```

### **Usage Example**
```python
executor = BenchmarkExecutor(
    gpu_ids=[0, 1],              # Explicit GPU specification
    max_concurrent_runs=4,       # Process limit
    base_result_dir="./results"
)

# Creates 6 stubs: 2 models × (2 longbench subsets + 1 mock_benchmark)
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper"]),
    BenchmarkConfig("mock_benchmark")
]
```

## **Ready for Implementation**

The plan now perfectly matches your requirements:
- ✅ Fresh start (no old executor code reuse)
- ✅ Dynamic GPU allocation (no static assignment)
- ✅ Subset-level resumability 
- ✅ User-controlled GPU specification
- ✅ Model name sanitization
- ✅ No ResultStorage complexity

**Next Steps**: 
1. Ready to start **Phase 1** implementation
2. Any final clarifications on the remaining questions?
3. Proceed with coding the MVP?

The design is now much more practical and efficient than the original static allocation approach!

```python
BenchmarkExecutor(
    gpu_ids=[0, 1, 2, 3],        # User specifies which GPUs to use
    max_concurrent_runs=8,       # Controls process count across GPUs
    base_result_dir="./results"
)
```

```plaintext
benchmark_results/
├── execution_20250115_143022/
│   ├── meta_llama_Llama_3.1_8B_Instruct/
│   │   ├── longbench_narrativeqa/         # Subset-level resumability
│   │   ├── longbench_qasper/              # Each subset separate
│   │   └── mock_benchmark/                # Full benchmark (no subsets)
│   └── microsoft_Phi_4_mini_instruct/
│       └── longbench_hotpotqa/
```

```python
executor = BenchmarkExecutor(
    gpu_ids=[0, 1],              # Explicit GPU specification
    max_concurrent_runs=4,       # Process limit
    base_result_dir="./results"
)

# Creates 6 stubs: 2 models × (2 longbench subsets + 1 mock_benchmark)
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper"]),
    BenchmarkConfig("mock_benchmark")
]
```

---

**User**

We need to add sparse_attention_config also as a part of matrix, (model x sparse_attention_config x benchmark: subset_combinations
Also, we need to modify the result directory to include sparse_config_name

---

**Cursor**

Excellent point! Adding sparse_attention_config as a matrix dimension is crucial for research comparing different attention mechanisms. Let me update the plan to support `model × sparse_attention_config × benchmark × subset` combinations.

```markdown
    def run_benchmark_matrix(
        self,
        model_names: List[str],
        benchmark_configs: List[BenchmarkConfig], 
        adapter_config: AdapterConfig,
        generation_kwargs: Optional[Dict[str, Any]] = None
    ) -> ExecutionResults:
        """Run all combinations of models and benchmarks in parallel.
        
        Args:
            model_names: List of HuggingFace model names
            benchmark_configs: List of benchmark configurations 
            adapter_config: Configuration for model adapters
            generation_kwargs: Generation parameters for model inference
            
        Returns:
            Aggregated results from all benchmark runs
        """

->

    def run_benchmark_matrix(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig], 
        adapter_config: AdapterConfig,
        generation_kwargs: Optional[Dict[str, Any]] = None
    ) -> ExecutionResults:
        """Run all combinations of models, sparse attention configs, and benchmarks in parallel.
        
        Args:
            model_names: List of HuggingFace model names
            sparse_attention_configs: List of (config_name, config) tuples
                                    e.g., [("dense", None), ("streaming", streaming_config)]
            benchmark_configs: List of benchmark configurations 
            adapter_config: Base configuration for model adapters (without sparse attention)
            generation_kwargs: Generation parameters for model inference
            
        Returns:
            Aggregated results from all benchmark runs
        """
```

```markdown
@dataclass 
class AdapterConfig:
    """Configuration for model adapters."""
    adapter_name: str = "huggingface"  # Future: support other adapters
    model_kwargs: Optional[Dict[str, Any]] = None
    tokenizer_kwargs: Optional[Dict[str, Any]] = None
    sparse_attention_config: Optional[SparseAttentionConfig] = None

->

@dataclass 
class AdapterConfig:
    """Base configuration for model adapters (sparse attention config is part of matrix)."""
    adapter_name: str = "huggingface"  # Future: support other adapters
    model_kwargs: Optional[Dict[str, Any]] = None
    tokenizer_kwargs: Optional[Dict[str, Any]] = None
    # Note: sparse_attention_config is now part of the execution matrix, not base config
```

```markdown
@dataclass
class BenchmarkStub:
    """A single benchmark execution task (model × benchmark × subset)."""
    model_name: str
    benchmark_name: str  # e.g., "longbench"
    subset: Optional[str]  # e.g., "narrativeqa" or None for full benchmark
    adapter_config: AdapterConfig
    generation_kwargs: Dict[str, Any]
    result_dir: str  # Full path including subset information

->

@dataclass
class BenchmarkStub:
    """A single benchmark execution task (model × sparse_config × benchmark × subset)."""
    model_name: str
    sparse_config_name: str  # e.g., "dense", "streaming", "hash_attention"
    sparse_attention_config: Optional[SparseAttentionConfig]  # None for dense
    benchmark_name: str  # e.g., "longbench"
    subset: Optional[str]  # e.g., "narrativeqa" or None for full benchmark
    adapter_config: AdapterConfig  # Base config without sparse attention
    generation_kwargs: Dict[str, Any]
    result_dir: str  # Full path including sparse_config_name and subset information
```

```markdown
```
benchmark_results/
├── execution_20250115_143022/           # Timestamped execution
│   ├── execution_summary.json          # Overall summary
│   ├── meta_llama_Llama_3.1_8B_Instruct/  # Per-model directories (sanitized names)
│   │   ├── longbench_narrativeqa/       # Per-benchmark-subset subdirectories
│   │   │   ├── raw_results.csv          # Required by Benchmark.run_benchmark()
│   │   │   └── metrics.json             # Required by Benchmark.run_benchmark()
│   │   ├── longbench_qasper/            # Separate directory per subset
│   │   │   ├── raw_results.csv
│   │   │   └── metrics.json
│   │   └── mock_benchmark/              # Full benchmark (no subsets)
│   │       ├── raw_results.csv
│   │       └── metrics.json
│   └── microsoft_Phi_4_mini_instruct/   # Sanitized model name
│       └── longbench_hotpotqa/
│           ├── raw_results.csv
│           └── metrics.json
```

**Resumability**: If a `sanitized_model_name/benchmark_name_subset/` directory exists, skip that specific benchmark stub.

->

```
benchmark_results/
├── execution_20250115_143022/           # Timestamped execution
│   ├── execution_summary.json          # Overall summary
│   ├── meta_llama_Llama_3.1_8B_Instruct/  # Per-model directories (sanitized names)
│   │   ├── dense/                       # Per-sparse-config subdirectories
│   │   │   ├── longbench_narrativeqa/   # Per-benchmark-subset subdirectories
│   │   │   │   ├── raw_results.csv      # Required by Benchmark.run_benchmark()
│   │   │   │   └── metrics.json         # Required by Benchmark.run_benchmark()
│   │   │   ├── longbench_qasper/        # Separate directory per subset
│   │   │   │   ├── raw_results.csv
│   │   │   │   └── metrics.json
│   │   │   └── mock_benchmark/          # Full benchmark (no subsets)
│   │   │       ├── raw_results.csv
│   │   │       └── metrics.json
│   │   └── streaming/                   # Different sparse attention config
│   │       ├── longbench_narrativeqa/
│   │       │   ├── raw_results.csv
│   │       │   └── metrics.json
│   │       └── longbench_qasper/
│   │           ├── raw_results.csv
│   │           └── metrics.json
│   └── microsoft_Phi_4_mini_instruct/   # Sanitized model name
│       ├── dense/
│       │   └── longbench_hotpotqa/
│       │       ├── raw_results.csv
│       │       └── metrics.json
│       └── hash_attention/
│           └── mock_benchmark/
│               ├── raw_results.csv
│               └── metrics.json
```

**Resumability**: If a `sanitized_model_name/sparse_config_name/benchmark_name_subset/` directory exists, skip that specific benchmark stub.
```

```markdown
def _filter_existing_results(self, stubs: List[BenchmarkStub]) -> List[BenchmarkStub]:
    """Filter out benchmark stubs that already have result directories at subset level."""
    filtered_stubs = []
    for stub in stubs:
        result_dir = self._get_result_directory(stub)  # Includes subset in path
        if not Path(result_dir).exists():
            filtered_stubs.append(stub)
        else:
            subset_info = f"_{stub.benchmark_config.subset}" if stub.benchmark_config.subset else ""
            print(f"Skipping {stub.model_name}/{stub.benchmark_config.benchmark_name}{subset_info} - results exist")
    return filtered_stubs

->

def _filter_existing_results(self, stubs: List[BenchmarkStub]) -> List[BenchmarkStub]:
    """Filter out benchmark stubs that already have result directories at subset level."""
    filtered_stubs = []
    for stub in stubs:
        result_dir = self._get_result_directory(stub)  # Includes sparse_config and subset in path
        if not Path(result_dir).exists():
            filtered_stubs.append(stub)
        else:
            subset_info = f"_{stub.subset}" if stub.subset else ""
            print(f"Skipping {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}{subset_info} - results exist")
    return filtered_stubs
```

```markdown
# Specify which GPUs to use and max concurrent processes
executor = BenchmarkExecutor(
    gpu_ids=[0, 1],  # Use GPUs 0 and 1
    max_concurrent_runs=4,  # Run up to 4 processes across these GPUs
    base_result_dir="./my_benchmark_results"
)

model_names = ["microsoft/Phi-4-mini-instruct", "meta-llama/Llama-3.1-8B-Instruct"]
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper"]),  # Creates 2 stubs
    BenchmarkConfig("mock_benchmark")  # Creates 1 stub (no subsets)
]
adapter_config = AdapterConfig(
    model_kwargs={"torch_dtype": torch.bfloat16},
    sparse_attention_config=streaming_config
)

# This creates 6 total stubs: 2 models × (2 longbench subsets + 1 mock_benchmark)
results = executor.run_benchmark_matrix(
    model_names=model_names,
    benchmark_configs=benchmark_configs, 
    adapter_config=adapter_config,
    generation_kwargs={"max_new_tokens": 256}
)

->

# Specify which GPUs to use and max concurrent processes
executor = BenchmarkExecutor(
    gpu_ids=[0, 1],  # Use GPUs 0 and 1
    max_concurrent_runs=4,  # Run up to 4 processes across these GPUs
    base_result_dir="./my_benchmark_results"
)

model_names = ["microsoft/Phi-4-mini-instruct", "meta-llama/Llama-3.1-8B-Instruct"]
sparse_attention_configs = [
    ("dense", None),  # Dense baseline
    ("streaming", streaming_config),  # StreamingLLM config
]
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper"]),  # Creates 2 subset stubs
    BenchmarkConfig("mock_benchmark")  # Creates 1 stub (no subsets)
]
adapter_config = AdapterConfig(
    model_kwargs={"torch_dtype": torch.bfloat16}
    # Note: sparse_attention_config removed - now part of matrix
)

# This creates 12 total stubs: 2 models × 2 sparse_configs × (2 longbench subsets + 1 mock_benchmark)
results = executor.run_benchmark_matrix(
    model_names=model_names,
    sparse_attention_configs=sparse_attention_configs,
    benchmark_configs=benchmark_configs, 
    adapter_config=adapter_config,
    generation_kwargs={"max_new_tokens": 256}
)
```

```markdown
# Use all 4 available GPUs with high concurrency
executor = BenchmarkExecutor(
    gpu_ids=[0, 1, 2, 3],  # Specify all GPUs to use
    max_concurrent_runs=8,  # High concurrency across 4 GPUs
    base_result_dir="./experiments/sparse_attention_comparison"
)

# Multiple sparse attention configurations for comparison
configs = [
    ("dense_baseline", AdapterConfig(sparse_attention_config=None)),
    ("streaming_llm", AdapterConfig(sparse_attention_config=streaming_config)),
    ("hash_attention", AdapterConfig(sparse_attention_config=hash_attention_config))
]

# Run comprehensive comparison study
model_names = ["meta-llama/Llama-3.1-8B-Instruct", "microsoft/Phi-4-mini-instruct"]
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper", "hotpotqa"]),
    BenchmarkConfig("mock_benchmark")
]

all_results = {}
for config_name, adapter_config in configs:
    print(f"Running configuration: {config_name}")
    
    # Each config gets its own result subdirectory
    results = executor.run_benchmark_matrix(
        model_names=model_names,
        benchmark_configs=benchmark_configs,
        adapter_config=adapter_config,
        generation_kwargs={"max_new_tokens": 512}
    )
    all_results[config_name] = results

# Results organized by configuration, model, and benchmark
# Resumable across different configurations

->

# Use all 4 available GPUs with high concurrency
executor = BenchmarkExecutor(
    gpu_ids=[0, 1, 2, 3],  # Specify all GPUs to use
    max_concurrent_runs=8,  # High concurrency across 4 GPUs
    base_result_dir="./experiments/sparse_attention_comparison"
)

# Comprehensive sparse attention configuration comparison
model_names = ["meta-llama/Llama-3.1-8B-Instruct", "microsoft/Phi-4-mini-instruct"]
sparse_attention_configs = [
    ("dense", None),  # Dense baseline
    ("streaming", streaming_config),  # StreamingLLM
    ("hash_attention", hash_attention_config),  # Hash attention
    ("sink_local", sink_local_config),  # Sink + Local masking
]
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper", "hotpotqa"]),
    BenchmarkConfig("mock_benchmark")
]
adapter_config = AdapterConfig(
    model_kwargs={"torch_dtype": torch.bfloat16},
    tokenizer_kwargs={"padding_side": "left"}
)

# This creates 32 stubs: 2 models × 4 sparse_configs × (3 longbench subsets + 1 mock_benchmark)
# All run in single execution with automatic resumability
results = executor.run_benchmark_matrix(
    model_names=model_names,
    sparse_attention_configs=sparse_attention_configs,
    benchmark_configs=benchmark_configs,
    adapter_config=adapter_config,
    generation_kwargs={"max_new_tokens": 512}
)

# Results organized by model/sparse_config/benchmark hierarchy
# Resumable: can restart and skip completed combinations
# Perfect for comprehensive attention mechanism studies
```

```markdown
# This creates 12 total stubs: 2 models × 2 sparse_configs × (2 longbench subsets + 1 mock_benchmark)
results = executor.run_benchmark_matrix(
    model_names=model_names,
    sparse_attention_configs=sparse_attention_configs,
    benchmark_configs=benchmark_configs, 
    adapter_config=adapter_config,
    generation_kwargs={"max_new_tokens": 256}
)

# Results automatically saved with subset-level directories
# Resumable: re-running skips completed benchmark stubs
# If OOM occurs, reduce max_concurrent_runs

->

# This creates 12 total stubs: 2 models × 2 sparse_configs × (2 longbench subsets + 1 mock_benchmark)
results = executor.run_benchmark_matrix(
    model_names=model_names,
    sparse_attention_configs=sparse_attention_configs,
    benchmark_configs=benchmark_configs, 
    adapter_config=adapter_config,
    generation_kwargs={"max_new_tokens": 256}
)

# Results automatically saved with sparse_config/subset-level directories
# Resumable: re-running skips completed model/sparse_config/benchmark/subset combinations
# If OOM occurs, reduce max_concurrent_runs
```

```markdown
### Implemented Changes
1. **✅ Removed ResultStorage**: Simplified to directory-based result management only (storage.py deleted)
2. **✅ Subset-Level Resumability**: Skip benchmark stubs with existing result directories at subset granularity
3. **✅ User-Specified GPU Lists**: Take gpu_ids as input rather than auto-discovery
4. **✅ Dynamic GPU Allocation**: Workers request GPUs from shared pool (no static assignment)
5. **✅ GPU Validation**: Test model loading on GPU before benchmark execution
6. **✅ Model Name Sanitization**: Handle special characters in model names for directory paths
7. **✅ Fresh Implementation**: No reuse of existing executor.py code

->

### Implemented Changes
1. **✅ Removed ResultStorage**: Simplified to directory-based result management only (storage.py deleted)
2. **✅ Sparse Attention Config Matrix**: Added sparse_attention_config as matrix dimension (model × sparse_config × benchmark × subset)
3. **✅ Subset-Level Resumability**: Skip benchmark stubs with existing result directories at sparse_config/subset granularity
4. **✅ User-Specified GPU Lists**: Take gpu_ids as input rather than auto-discovery
5. **✅ Dynamic GPU Allocation**: Workers request GPUs from shared pool (no static assignment)
6. **✅ GPU Validation**: Test model loading on GPU before benchmark execution
7. **✅ Model Name Sanitization**: Handle special characters in model names for directory paths
8. **✅ Fresh Implementation**: No reuse of existing executor.py code
```

```markdown
### Core Strengths
- **Resumable Execution**: Never re-run completed benchmarks
- **Dynamic Resource Allocation**: Efficient GPU utilization regardless of benchmark timing
- **Simplified Architecture**: No unnecessary storage abstraction layers
- **Robust Validation**: GPU compatibility testing before expensive model loading
- **Fresh Implementation**: Clean, purpose-built solution

->

### Core Strengths
- **Comprehensive Matrix Support**: Full model × sparse_config × benchmark × subset combinations
- **Resumable Execution**: Never re-run completed combinations at granular level
- **Dynamic Resource Allocation**: Efficient GPU utilization regardless of benchmark timing
- **Simplified Architecture**: No unnecessary storage abstraction layers
- **Robust Validation**: GPU compatibility testing before expensive model loading
- **Research-Focused**: Perfect for attention mechanism comparison studies
- **Fresh Implementation**: Clean, purpose-built solution
```

```markdown
## Finalized Design Decisions ✅

1. **✅ Storage**: Remove ResultStorage, use directory-based management only
2. **✅ Resumability**: Subset-level granularity
3. **✅ GPU Management**: User specifies gpu_ids list, max_concurrent_runs controls processes
4. **✅ Model Names**: Sanitize special characters for directory paths

->

## Finalized Design Decisions ✅

1. **✅ Storage**: Remove ResultStorage, use directory-based management only
2. **✅ Matrix Design**: Support model × sparse_config × benchmark × subset combinations
3. **✅ Resumability**: Sparse_config/subset-level granularity
4. **✅ GPU Management**: User specifies gpu_ids list, max_concurrent_runs controls processes
5. **✅ Model Names**: Sanitize special characters for directory paths
```

```markdown
def _benchmark_worker(
    stub_queue: multiprocessing.Queue,
    gpu_pool: multiprocessing.Queue,
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue
) -> None:
    """Worker function to execute benchmarks with dynamic GPU allocation."""
    while True:
        # 1. Get next benchmark stub from queue
        # 2. Acquire GPU from pool
        # 3. Validate model adapter can initialize on GPU
        # 4. Create model adapter and benchmark instance
        # 5. Execute benchmark.run_benchmark()
        # 6. Release GPU back to pool
        # 7. Report results/errors

->

def _benchmark_worker(
    stub_queue: multiprocessing.Queue,
    gpu_pool: multiprocessing.Queue,
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue
) -> None:
    """Worker function to execute benchmarks with dynamic GPU allocation."""
    while True:
        # 1. Get next benchmark stub from queue
        # 2. Acquire GPU from pool
        # 3. Validate model adapter can initialize on GPU
        # 4. Create model adapter with stub's sparse_attention_config
        # 5. Create benchmark instance with stub's subset configuration
        # 6. Execute benchmark.run_benchmark(adapter, stub.result_dir)
        # 7. Release GPU back to pool
        # 8. Report results/errors
```

Excellent! I've updated the plan to include `sparse_attention_config` as a matrix dimension. Here's what's changed:

## ✅ **Updated Matrix Design**

### **New Matrix Dimensions**
```
model × sparse_attention_config × benchmark × subset
```

### **API Changes**

**Before:**
```python
executor.run_benchmark_matrix(
    model_names=model_names,
    benchmark_configs=benchmark_configs,
    adapter_config=adapter_config,  # included sparse_attention_config
)
```

**After:**
```python
executor.run_benchmark_matrix(
    model_names=model_names,
    sparse_attention_configs=[  # NEW: part of matrix now
        ("dense", None),
        ("streaming", streaming_config), 
        ("hash_attention", hash_attention_config),
    ],
    benchmark_configs=benchmark_configs,
    adapter_config=adapter_config,  # no longer includes sparse_attention_config
)
```

### **Updated Directory Structure**
```
benchmark_results/
├── execution_20250115_143022/
│   ├── meta_llama_Llama_3.1_8B_Instruct/
│   │   ├── dense/                       # Sparse config level
│   │   │   ├── longbench_narrativeqa/   # Benchmark + subset level
│   │   │   ├── longbench_qasper/
│   │   │   └── mock_benchmark/
│   │   ├── streaming/                   # Different sparse config
│   │   │   ├── longbench_narrativeqa/
│   │   │   └── longbench_qasper/
│   │   └── hash_attention/
│   │       └── mock_benchmark/
```

### **Enhanced Resumability**
- **Path**: `model_name/sparse_config_name/benchmark_name_subset/`
- **Granularity**: Skip specific model + sparse config + benchmark + subset combinations
- **Example**: Skip `llama/dense/longbench_narrativeqa/` but run `llama/streaming/longbench_narrativeqa/`

### **Research Benefits**
- **Comprehensive Studies**: Compare multiple attention mechanisms across models and benchmarks
- **Efficient Execution**: All configurations run in parallel across GPUs
- **Perfect Resumability**: Restart failed experiments without re-running completed combinations
- **Organized Results**: Clear separation of attention mechanism results

### **Scale Example**
```python
# 2 models × 4 sparse_configs × 4 benchmarks = 32 total combinations
model_names = ["llama-8b", "phi-4-mini"]
sparse_attention_configs = [
    ("dense", None),
    ("streaming", streaming_config),
    ("hash_attention", hash_config), 
    ("sink_local", sink_local_config)
]
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper", "hotpotqa"])  # 3 subsets
    BenchmarkConfig("mock_benchmark")  # 1 benchmark
]
# Total: 2 × 4 × (3 + 1) = 32 benchmark stubs
```

This design is perfect for your research needs - you can now easily compare dense vs sparse attention mechanisms across multiple models and benchmarks in a single execution run!

Ready to proceed with implementation of this enhanced matrix design?

```plaintext
model × sparse_attention_config × benchmark × subset
```

```python
executor.run_benchmark_matrix(
    model_names=model_names,
    benchmark_configs=benchmark_configs,
    adapter_config=adapter_config,  # included sparse_attention_config
)
```

```python
executor.run_benchmark_matrix(
    model_names=model_names,
    sparse_attention_configs=[  # NEW: part of matrix now
        ("dense", None),
        ("streaming", streaming_config), 
        ("hash_attention", hash_attention_config),
    ],
    benchmark_configs=benchmark_configs,
    adapter_config=adapter_config,  # no longer includes sparse_attention_config
)
```

```plaintext
benchmark_results/
├── execution_20250115_143022/
│   ├── meta_llama_Llama_3.1_8B_Instruct/
│   │   ├── dense/                       # Sparse config level
│   │   │   ├── longbench_narrativeqa/   # Benchmark + subset level
│   │   │   ├── longbench_qasper/
│   │   │   └── mock_benchmark/
│   │   ├── streaming/                   # Different sparse config
│   │   │   ├── longbench_narrativeqa/
│   │   │   └── longbench_qasper/
│   │   └── hash_attention/
│   │       └── mock_benchmark/
```

```python
# 2 models × 4 sparse_configs × 4 benchmarks = 32 total combinations
model_names = ["llama-8b", "phi-4-mini"]
sparse_attention_configs = [
    ("dense", None),
    ("streaming", streaming_config),
    ("hash_attention", hash_config), 
    ("sink_local", sink_local_config)
]
benchmark_configs = [
    BenchmarkConfig("longbench", ["narrativeqa", "qasper", "hotpotqa"])  # 3 subsets
    BenchmarkConfig("mock_benchmark")  # 1 benchmark
]
# Total: 2 × 4 × (3 + 1) = 32 benchmark stubs
```

---

**User**

Okay let us create a extensive to_do_list.md to define phases and sequential task order for this plan

---

**Cursor**

I'll create a comprehensive to-do list that breaks down the BenchmarkExecutor implementation into sequential, actionable tasks across the three phases we defined.

```markdown
# BenchmarkExecutor Implementation To-Do List

## Overview

Comprehensive task breakdown for implementing the BenchmarkExecutor with parallel GPU execution and matrix support for `model × sparse_attention_config × benchmark × subset` combinations.

**Target Timeline**: 6-9 days (2-3 days per phase)
**Architecture**: Fresh implementation with dynamic GPU allocation and resumability

---

## Phase 1: Minimal Viable Product (MVP)
**Goal**: Functional parallel execution with dynamic GPU allocation and resumability  
**Estimated Effort**: 2-3 days

### Task 1.1: Core Data Structures and Configuration Classes
**Priority**: High | **Dependencies**: None | **Effort**: 4-6 hours

- [ ] **1.1.1** Create `benchmark/executor_config.py`
  - [ ] Define `BenchmarkConfig` dataclass
    - [ ] `benchmark_name: str`
    - [ ] `subsets: Optional[List[str]]`
    - [ ] Validation methods
  - [ ] Define `AdapterConfig` dataclass (without sparse_attention_config)
    - [ ] `adapter_name: str = "huggingface"`
    - [ ] `model_kwargs: Optional[Dict[str, Any]]`
    - [ ] `tokenizer_kwargs: Optional[Dict[str, Any]]`
  - [ ] Define `BenchmarkStub` dataclass
    - [ ] All required fields including `sparse_config_name`, `sparse_attention_config`
    - [ ] `result_dir` path construction logic
  - [ ] Define `ExecutionResults` and `ExecutionProgress` dataclasses

- [ ] **1.1.2** Create benchmark factory registry
  - [ ] Function to instantiate benchmark classes by name
  - [ ] Support for LongBench, MockBenchmark, etc.
  - [ ] Error handling for unknown benchmark names

**Deliverable**: Working configuration classes and benchmark instantiation

### Task 1.2: Directory Management and Name Sanitization
**Priority**: High | **Dependencies**: 1.1 | **Effort**: 2-3 hours

- [ ] **1.2.1** Implement model name sanitization
  - [ ] `_sanitize_model_name()` method
  - [ ] Handle special characters: `/ \ : * ? " < > |`
  - [ ] Test with real model names (e.g., `meta-llama/Llama-3.1-8B-Instruct`)

- [ ] **1.2.2** Implement result directory structure
  - [ ] `_get_result_directory(stub)` method
  - [ ] Format: `base_dir/execution_timestamp/sanitized_model/sparse_config/benchmark_subset/`
  - [ ] Directory creation logic
  - [ ] Path validation

**Deliverable**: Robust directory management with sanitized names

### Task 1.3: Benchmark Stub Generation
**Priority**: High | **Dependencies**: 1.1, 1.2 | **Effort**: 3-4 hours

- [ ] **1.3.1** Implement matrix expansion logic
  - [ ] `_generate_benchmark_stubs()` method
  - [ ] Expand model_names × sparse_attention_configs × benchmark_configs × subsets
  - [ ] Handle benchmarks with no subsets (run full benchmark)
  - [ ] Generate unique result_dir for each stub

- [ ] **1.3.2** Implement resumability filtering
  - [ ] `_filter_existing_results()` method
  - [ ] Check if `result_dir` exists for each stub
  - [ ] Log skipped combinations
  - [ ] Return filtered list of pending stubs

**Deliverable**: Matrix expansion and resumability detection

### Task 1.4: GPU Management and Validation
**Priority**: High | **Dependencies**: None | **Effort**: 4-5 hours

- [ ] **1.4.1** Implement GPU validation
  - [ ] `_validate_gpu_availability()` method
  - [ ] Check `torch.cuda.device_count()`
  - [ ] Validate each GPU in `self.gpu_ids` is accessible
  - [ ] Error handling for unavailable GPUs

- [ ] **1.4.2** Implement GPU pool management
  - [ ] `_create_gpu_pool()` method
  - [ ] Create `multiprocessing.Queue` with available GPU IDs
  - [ ] GPU acquisition and release logic
  - [ ] Handle GPU pool exhaustion

- [ ] **1.4.3** Implement model adapter validation
  - [ ] `_validate_gpu_for_model()` method (minimal version)
  - [ ] Test GPU accessibility without full model loading
  - [ ] CUDA_VISIBLE_DEVICES environment variable handling

**Deliverable**: GPU pool management and basic validation

### Task 1.5: Core BenchmarkExecutor Class Structure
**Priority**: High | **Dependencies**: 1.1-1.4 | **Effort**: 3-4 hours

- [ ] **1.5.1** Implement BenchmarkExecutor class
  - [ ] `__init__()` method with gpu_ids, max_concurrent_runs, base_result_dir
  - [ ] Class initialization and validation
  - [ ] Member variable setup

- [ ] **1.5.2** Implement main orchestration method
  - [ ] `run_benchmark_matrix()` method signature
  - [ ] Parameter validation
  - [ ] Call stub generation, filtering, and GPU validation
  - [ ] Integration point for worker processes (placeholder)

**Deliverable**: Basic BenchmarkExecutor class with main entry point

### Task 1.6: Worker Process Implementation
**Priority**: High | **Dependencies**: 1.1-1.5 | **Effort**: 6-8 hours

- [ ] **1.6.1** Implement worker function structure
  - [ ] `_benchmark_worker()` function signature
  - [ ] Queue handling (stub_queue, gpu_pool, result_queue, error_queue)
  - [ ] Basic worker loop with exception handling

- [ ] **1.6.2** Implement GPU acquisition and release
  - [ ] GPU acquisition from pool with timeout
  - [ ] CUDA_VISIBLE_DEVICES setting per process
  - [ ] GPU release back to pool after completion
  - [ ] GPU cleanup on worker failure

- [ ] **1.6.3** Implement model adapter creation
  - [ ] Create ModelAdapterHF with stub's sparse_attention_config
  - [ ] Handle adapter initialization errors
  - [ ] Memory cleanup after benchmark completion

- [ ] **1.6.4** Implement benchmark execution
  - [ ] Instantiate benchmark class by name
  - [ ] Configure benchmark with subset if specified
  - [ ] Execute `benchmark.run_benchmark(adapter, result_dir)`
  - [ ] Handle benchmark execution errors

**Deliverable**: Working benchmark execution in isolated processes

### Task 1.7: Process Pool Management and Communication
**Priority**: High | **Dependencies**: 1.6 | **Effort**: 4-5 hours

- [ ] **1.7.1** Implement multiprocessing setup
  - [ ] Create and manage worker processes
  - [ ] Queue creation and management
  - [ ] Process startup and shutdown logic

- [ ] **1.7.2** Implement work distribution
  - [ ] Populate stub_queue with filtered stubs
  - [ ] Handle queue exhaustion and worker termination
  - [ ] Graceful shutdown mechanisms

- [ ] **1.7.3** Implement result collection
  - [ ] Basic result aggregation from result_queue
  - [ ] Error handling from error_queue
  - [ ] Process completion tracking

**Deliverable**: End-to-end parallel execution with basic result collection

### Task 1.8: Basic Error Handling and Cleanup
**Priority**: Medium | **Dependencies**: 1.7 | **Effort**: 2-3 hours

- [ ] **1.8.1** Implement essential error handling
  - [ ] Worker process failure recovery
  - [ ] GPU memory cleanup on errors
  - [ ] Queue communication errors

- [ ] **1.8.2** Implement resource cleanup
  - [ ] Process termination on main thread exit
  - [ ] GPU pool cleanup
  - [ ] Temporary resource cleanup

**Deliverable**: Basic fault tolerance and resource management

### Task 1.9: MVP Integration Testing
**Priority**: High | **Dependencies**: 1.1-1.8 | **Effort**: 3-4 hours

- [ ] **1.9.1** Create MVP test script
  - [ ] Simple test with 2 models, 2 sparse configs, MockBenchmark
  - [ ] Test resumability by running twice
  - [ ] Verify directory structure creation

- [ ] **1.9.2** Test GPU allocation
  - [ ] Test with single GPU
  - [ ] Test with multiple GPUs
  - [ ] Test GPU validation failures

- [ ] **1.9.3** Fix critical MVP issues
  - [ ] Address blocking issues found in testing
  - [ ] Basic performance validation

**Deliverable**: Working MVP ready for Phase 2 enhancements

---

## Phase 2: Production Ready
**Goal**: Reliable execution suitable for research experiments with production-grade robustness  
**Estimated Effort**: 2-3 days

### Task 2.1: Enhanced Progress Tracking and Monitoring
**Priority**: High | **Dependencies**: Phase 1 | **Effort**: 4-5 hours

- [ ] **2.1.1** Implement comprehensive progress tracking
  - [ ] Real-time progress updates with completion percentages
  - [ ] Track total, completed, failed, and skipped stubs
  - [ ] Current execution status per GPU
  - [ ] Estimated time remaining calculations

- [ ] **2.1.2** Implement detailed logging
  - [ ] Structured logging with log levels
  - [ ] Per-benchmark execution logs
  - [ ] GPU allocation and release logging
  - [ ] Performance metrics logging (execution times)

- [ ] **2.1.3** Implement progress reporting
  - [ ] Console progress bar with tqdm or similar
  - [ ] Periodic status summaries
  - [ ] Optional log file output

**Deliverable**: Comprehensive monitoring and progress tracking

### Task 2.2: Robust Error Handling and Recovery
**Priority**: High | **Dependencies**: 2.1 | **Effort**: 5-6 hours

- [ ] **2.2.1** Implement advanced error categorization
  - [ ] Model loading failures (OOM, invalid model, etc.)
  - [ ] Benchmark execution failures
  - [ ] GPU-specific errors
  - [ ] Dataset loading failures

- [ ] **2.2.2** Implement error recovery strategies
  - [ ] Automatic retry logic for transient failures
  - [ ] Graceful degradation for persistent failures
  - [ ] Dead worker process detection and replacement
  - [ ] GPU failure isolation and recovery

- [ ] **2.2.3** Implement comprehensive error reporting
  - [ ] Detailed error logs with stack traces
  - [ ] Error summary in final results
  - [ ] Failed stub identification for manual retry

**Deliverable**: Production-grade error handling and recovery

### Task 2.3: Enhanced GPU Resource Management
**Priority**: Medium | **Dependencies**: 2.1 | **Effort**: 3-4 hours

- [ ] **2.3.1** Implement advanced GPU validation
  - [ ] Memory availability checking
  - [ ] GPU utilization monitoring
  - [ ] Model compatibility testing (optional deep validation)

- [ ] **2.3.2** Implement GPU allocation optimization
  - [ ] Smart GPU assignment based on memory requirements
  - [ ] GPU load balancing
  - [ ] Memory fragmentation handling

- [ ] **2.3.3** Implement GPU monitoring
  - [ ] Real-time GPU memory usage tracking
  - [ ] GPU utilization metrics
  - [ ] Memory leak detection

**Deliverable**: Advanced GPU resource management

### Task 2.4: Result Aggregation and Summary Generation
**Priority**: High | **Dependencies**: 2.1 | **Effort**: 4-5 hours

- [ ] **2.4.1** Implement comprehensive result aggregation
  - [ ] Collect metrics from all benchmark runs
  - [ ] Generate execution summary statistics
  - [ ] Performance analysis (execution times, GPU utilization)

- [ ] **2.4.2** Implement result summary generation
  - [ ] Generate `execution_summary.json` with comprehensive metadata
  - [ ] Include benchmark completion status
  - [ ] Include performance metrics and timing data
  - [ ] Include error summaries and failed stub information

- [ ] **2.4.3** Implement result validation
  - [ ] Verify all expected result files exist
  - [ ] Validate result file formats
  - [ ] Generate completeness reports

**Deliverable**: Comprehensive result aggregation and reporting

### Task 2.5: Configuration Validation and Defaults
**Priority**: Medium | **Dependencies**: 2.1-2.4 | **Effort**: 2-3 hours

- [ ] **2.5.1** Implement configuration validation
  - [ ] Validate GPU IDs against available hardware
  - [ ] Validate model names and accessibility
  - [ ] Validate benchmark names and configurations
  - [ ] Validate sparse attention configurations

- [ ] **2.5.2** Implement sensible defaults
  - [ ] Default model_kwargs for common configurations
  - [ ] Default generation_kwargs
  - [ ] Default directory naming conventions

- [ ] **2.5.3** Implement configuration recommendations
  - [ ] Suggest max_concurrent_runs based on available GPUs
  - [ ] Memory usage estimates and warnings
  - [ ] Performance optimization suggestions

**Deliverable**: Robust configuration validation and user guidance

### Task 2.6: Performance Optimization
**Priority**: Medium | **Dependencies**: 2.1-2.5 | **Effort**: 3-4 hours

- [ ] **2.6.1** Optimize memory management
  - [ ] Implement torch.cuda.empty_cache() strategically
  - [ ] Optimize model loading and unloading
  - [ ] Memory usage profiling and optimization

- [ ] **2.6.2** Optimize queue management
  - [ ] Tune queue sizes for optimal performance
  - [ ] Implement efficient work distribution
  - [ ] Minimize serialization overhead

- [ ] **2.6.3** Optimize I/O operations
  - [ ] Batch directory creation
  - [ ] Optimize result file writing
  - [ ] Minimize filesystem operations

**Deliverable**: Optimized performance for large-scale experiments

### Task 2.7: Production Integration Testing
**Priority**: High | **Dependencies**: 2.1-2.6 | **Effort**: 4-5 hours

- [ ] **2.7.1** Create comprehensive test scenarios
  - [ ] Large-scale test (4 models × 4 sparse configs × 3 benchmarks)
  - [ ] Error injection testing (simulated failures)
  - [ ] Resource constraint testing (limited GPU memory)

- [ ] **2.7.2** Test resumability at scale
  - [ ] Interrupt and resume large experiments
  - [ ] Verify partial completion handling
  - [ ] Test with corrupted result directories

- [ ] **2.7.3** Performance benchmarking
  - [ ] Measure throughput and efficiency
  - [ ] Compare with sequential execution
  - [ ] Memory usage profiling

**Deliverable**: Production-ready implementation with performance validation

---

## Phase 3: Quality & Polish
**Goal**: Production-quality implementation ready for team use  
**Estimated Effort**: 2-3 days

### Task 3.1: Comprehensive Unit Testing
**Priority**: High | **Dependencies**: Phase 2 | **Effort**: 6-8 hours

- [ ] **3.1.1** Create unit tests for core components
  - [ ] Test configuration classes and validation
  - [ ] Test stub generation and matrix expansion
  - [ ] Test directory management and name sanitization
  - [ ] Test GPU pool management

- [ ] **3.1.2** Create unit tests for worker functions
  - [ ] Test benchmark worker in isolation
  - [ ] Mock GPU allocation and model loading
  - [ ] Test error handling paths
  - [ ] Test result collection

- [ ] **3.1.3** Create unit tests for result aggregation
  - [ ] Test result collection and aggregation
  - [ ] Test summary generation
  - [ ] Test resumability logic

**Deliverable**: Comprehensive unit test suite with >90% coverage

### Task 3.2: Integration and End-to-End Testing
**Priority**: High | **Dependencies**: 3.1 | **Effort**: 4-5 hours

- [ ] **3.2.1** Create integration tests
  - [ ] Test with real ModelAdapterHF integration
  - [ ] Test with real benchmark implementations (MockBenchmark)
  - [ ] Test multi-GPU scenarios

- [ ] **3.2.2** Create end-to-end test scenarios
  - [ ] Full workflow tests with real models (small models for speed)
  - [ ] Resumability testing with real file systems
  - [ ] Error recovery testing with real failures

- [ ] **3.2.3** Create regression tests
  - [ ] Test backward compatibility
  - [ ] Test with different sparse attention configurations
  - [ ] Test with various benchmark types

**Deliverable**: Comprehensive integration and e2e test suite

### Task 3.3: Documentation and Examples
**Priority**: High | **Dependencies**: 3.1-3.2 | **Effort**: 4-6 hours

- [ ] **3.3.1** Create comprehensive documentation
  - [ ] API documentation with docstrings
  - [ ] Usage guide with examples
  - [ ] Configuration reference
  - [ ] Troubleshooting guide

- [ ] **3.3.2** Create example scripts
  - [ ] Basic usage example
  - [ ] Advanced configuration example
  - [ ] Research study template
  - [ ] Error handling examples

- [ ] **3.3.3** Create integration documentation
  - [ ] Integration with existing sparse-attention-hub workflows
  - [ ] Best practices for large-scale experiments
  - [ ] Performance tuning guide

**Deliverable**: Complete documentation and example library

### Task 3.4: Code Quality and Standards
**Priority**: Medium | **Dependencies**: 3.1-3.3 | **Effort**: 3-4 hours

- [ ] **3.4.1** Implement code quality improvements
  - [ ] Add comprehensive type annotations
  - [ ] Add docstrings following Google style
  - [ ] Code formatting with black/isort
  - [ ] Linting with pylint/flake8

- [ ] **3.4.2** Implement logging improvements
  - [ ] Structured logging with proper levels
  - [ ] Performance logging for optimization
  - [ ] Debug logging for troubleshooting

- [ ] **3.4.3** Implement security and safety
  - [ ] Input validation and sanitization
  - [ ] File path validation
  - [ ] Resource limit enforcement

**Deliverable**: High-quality, maintainable codebase

### Task 3.5: Performance Optimization and Tuning
**Priority**: Medium | **Dependencies**: 3.1-3.4 | **Effort**: 3-4 hours

- [ ] **3.5.1** Profile and optimize bottlenecks
  - [ ] Memory usage optimization
  - [ ] CPU utilization optimization
  - [ ] I/O optimization

- [ ] **3.5.2** Implement advanced features
  - [ ] Adaptive concurrency based on available resources
  - [ ] Smart work scheduling (prioritize faster benchmarks)
  - [ ] Memory-aware GPU allocation

- [ ] **3.5.3** Benchmark performance improvements
  - [ ] Compare optimized vs baseline performance
  - [ ] Document performance characteristics
  - [ ] Create performance regression tests

**Deliverable**: Optimized implementation with documented performance

### Task 3.6: Final Integration and Deployment
**Priority**: High | **Dependencies**: 3.1-3.5 | **Effort**: 2-3 hours

- [ ] **3.6.1** Final integration with sparse-attention-hub
  - [ ] Update benchmark/__init__.py exports
  - [ ] Ensure compatibility with existing code
  - [ ] Update project documentation

- [ ] **3.6.2** Create deployment checklist
  - [ ] Installation requirements
  - [ ] Configuration validation
  - [ ] Quick start guide

- [ ] **3.6.3** Final testing and validation
  - [ ] Run complete test suite
  - [ ] Validate with real research scenarios
  - [ ] Performance acceptance testing

**Deliverable**: Production-ready BenchmarkExecutor ready for team use

---

## Dependencies and Critical Path

### Critical Path Tasks (Must Complete in Order)
1. **1.1** → **1.2** → **1.3** → **1.5** → **1.6** → **1.7** (Core functionality)
2. **1.4** (GPU management) → **1.6** (Worker implementation)
3. **1.8** → **1.9** (MVP completion)
4. **Phase 1** → **2.1** → **2.2** → **2.7** (Production ready)
5. **Phase 2** → **3.1** → **3.2** → **3.6** (Quality and deployment)

### Parallelizable Tasks
- **1.4** (GPU management) can be developed in parallel with **1.1-1.3**
- **2.3** (Advanced GPU management) can be parallel with **2.4** (Result aggregation)
- **3.1** (Unit tests) can be parallel with **3.3** (Documentation)

### Risk Mitigation
- **GPU Integration**: Start GPU validation early (Task 1.4)
- **Multiprocessing Complexity**: Allocate extra time for Task 1.7
- **Performance Issues**: Include buffer time in Phase 2 optimization tasks

## Success Criteria

### Phase 1 Success Criteria
- [ ] Can execute model × sparse_config × benchmark matrix in parallel
- [ ] Resumability works correctly
- [ ] Basic GPU allocation functions
- [ ] Results saved in correct directory structure

### Phase 2 Success Criteria
- [ ] Handles errors gracefully without crashing
- [ ] Provides meaningful progress tracking
- [ ] Optimized for research-scale workloads
- [ ] Comprehensive result aggregation

### Phase 3 Success Criteria
- [ ] >90% test coverage
- [ ] Complete documentation
- [ ] Production-ready code quality
- [ ] Team can use without assistance

This comprehensive to-do list provides a clear roadmap for implementing the BenchmarkExecutor with all the features we've planned. Each task is scoped to be completable in a reasonable timeframe while building toward the final goal.
```

Perfect! I've created a comprehensive **515-line to-do list** at `cursor_chats/plans/benchmark_executor_todo_list.md` that provides a detailed implementation roadmap for the BenchmarkExecutor.

## 📋 **To-Do List Overview**

### **Structure**
- **3 Phases** with clear goals and estimated timelines
- **19 Major Tasks** broken into **60+ Sub-tasks**
- **Dependencies** and critical path analysis
- **Success criteria** for each phase

### **Phase Breakdown**

#### **Phase 1: MVP (2-3 days)**
- **9 Tasks** focused on core functionality
- Key deliverables: Working parallel execution, resumability, basic GPU management
- Critical path: Data structures → Stub generation → Worker processes → Integration

#### **Phase 2: Production Ready (2-3 days)**  
- **7 Tasks** focused on robustness and monitoring
- Key deliverables: Error handling, progress tracking, result aggregation, optimization
- Focus: Make it reliable for research experiments

#### **Phase 3: Quality & Polish (2-3 days)**
- **6 Tasks** focused on testing, documentation, and deployment
- Key deliverables: >90% test coverage, complete documentation, production code quality
- Focus: Team-ready implementation

### **Key Features of the Plan**

#### **📊 Detailed Task Structure**
Each task includes:
- **Priority level** (High/Medium)
- **Dependencies** (what must be completed first)
- **Effort estimate** (hours)
- **Specific deliverables**
- **Sub-task breakdowns** with checkboxes

#### **🔄 Critical Path Analysis**
- Identifies **sequential dependencies** that can't be parallelized
- Suggests **parallelizable tasks** to optimize development time
- **Risk mitigation** strategies for complex components

#### **✅ Success Criteria**
Clear definitions of what "done" looks like for each phase:
- **Phase 1**: Basic functionality working
- **Phase 2**: Production reliability
- **Phase 3**: Team-ready quality

### **Example Task Detail**
```
### Task 1.6: Worker Process Implementation
**Priority**: High | **Dependencies**: 1.1-1.5 | **Effort**: 6-8 hours

- [ ] 1.6.1 Implement worker function structure
- [ ] 1.6.2 Implement GPU acquisition and release  
- [ ] 1.6.3 Implement model adapter creation
- [ ] 1.6.4 Implement benchmark execution

**Deliverable**: Working benchmark execution in isolated processes
```

### **Strategic Benefits**

1. **🎯 Clear Roadmap**: Every task builds logically toward the final goal
2. **🔀 Flexible Execution**: Some tasks can be parallelized for faster development
3. **🛡️ Risk Management**: Critical path identified, buffer time included
4. **📈 Progressive Delivery**: Each phase delivers working functionality
5. **🔍 Quality Focus**: Testing and documentation built into the plan

### **Ready for Implementation**

The to-do list is designed to be:
- **Actionable**: Each task is specific and completable
- **Measurable**: Clear deliverables and success criteria
- **Realistic**: Time estimates based on complexity
- **Sequential**: Dependencies clearly mapped

You now have a comprehensive implementation guide that will take you from zero to a production-ready BenchmarkExecutor with all the features we've planned:

✅ **Model × sparse_config × benchmark × subset** matrix support  
✅ **Dynamic GPU allocation** with worker queues  
✅ **Subset-level resumability**  
✅ **Comprehensive error handling**  
✅ **Progress tracking and monitoring**  
✅ **Production-grade quality**  

Ready to start Phase 1, or would you like to discuss any specific tasks or modify the plan?

```plaintext
### Task 1.6: Worker Process Implementation
**Priority**: High | **Dependencies**: 1.1-1.5 | **Effort**: 6-8 hours

- [ ] 1.6.1 Implement worker function structure
- [ ] 1.6.2 Implement GPU acquisition and release  
- [ ] 1.6.3 Implement model adapter creation
- [ ] 1.6.4 Implement benchmark execution

**Deliverable**: Working benchmark execution in isolated processes
```

---

**User**

Let us begin with Phase 1 implementation. Complete "phase 1 task 1.1" and mark it as complete when done

---

**Cursor**

I'll begin implementing Phase 1 Task 1.1. Let me start by activating the conda environment and exploring the current benchmark structure to understand what we're working with.

Now I'll create the executor_config.py file with all the required dataclasses and benchmark factory registry. Based on my exploration, I understand the existing benchmark structure and how to integrate with it.

```python
"""Configuration classes and utilities for BenchmarkExecutor.

This module provides configuration dataclasses and factory functions for orchestrating
parallel benchmark execution across multiple GPUs using multiprocessing.
"""

import os
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from sparse_attention_hub.sparse_attention.base import SparseAttentionConfig

from .base import Benchmark


@dataclass
class BenchmarkConfig:
    """Configuration for a benchmark execution.
    
    During stub generation, this expands to individual BenchmarkStub per subset.
    Each BenchmarkStub will have a single subset value for resumability.
    
    Attributes:
        benchmark_name: Name of the benchmark (e.g., "longbench", "mock_benchmark")
        subsets: Optional list of dataset subsets to run (e.g., ["narrativeqa", "qasper"])
                If None, runs all available subsets for the benchmark
    
    Example:
        >>> config = BenchmarkConfig("longbench", ["narrativeqa", "qasper"])
        >>> # This will create 2 BenchmarkStub instances during matrix expansion
    """
    
    benchmark_name: str
    subsets: Optional[List[str]] = None
    
    def __post_init__(self) -> None:
        """Validate benchmark configuration after initialization.
        
        Raises:
            ValueError: If benchmark_name is empty or invalid.
        """
        if not self.benchmark_name or not self.benchmark_name.strip():
            raise ValueError("benchmark_name cannot be empty")
        
        self.benchmark_name = self.benchmark_name.strip()
        
        if self.subsets is not None:
            # Remove empty strings and strip whitespace
            self.subsets = [subset.strip() for subset in self.subsets if subset.strip()]
            if not self.subsets:
                self.subsets = None
    
    def validate_with_benchmark_instance(self, benchmark: Benchmark) -> None:
        """Validate subsets against actual benchmark instance.
        
        Args:
            benchmark: Benchmark instance to validate against
            
        Raises:
            ValueError: If any subset is not available in the benchmark.
        """
        if self.subsets is not None:
            available_datasets: List[str] = benchmark.get_available_datasets()
            invalid_subsets: set[str] = set(self.subsets) - set(available_datasets)
            if invalid_subsets:
                raise ValueError(
                    f"Invalid subsets for {self.benchmark_name}: {invalid_subsets}. "
                    f"Available datasets: {available_datasets}"
                )


@dataclass
class AdapterConfig:
    """Base configuration for model adapters (sparse attention config is part of matrix).
    
    Note: sparse_attention_config is now part of the execution matrix, not base config.
    This allows testing different sparse attention configurations with the same adapter base.
    
    Attributes:
        adapter_name: Name of the adapter type (default: "huggingface")
        model_kwargs: Additional keyword arguments for model creation
        tokenizer_kwargs: Additional keyword arguments for tokenizer creation
    
    Example:
        >>> config = AdapterConfig(
        ...     model_kwargs={"torch_dtype": torch.bfloat16},
        ...     tokenizer_kwargs={"padding_side": "left"}
        ... )
    """
    
    adapter_name: str = "huggingface"
    model_kwargs: Optional[Dict[str, Any]] = None
    tokenizer_kwargs: Optional[Dict[str, Any]] = None
    
    def __post_init__(self) -> None:
        """Initialize default values and validate configuration."""
        if self.model_kwargs is None:
            self.model_kwargs = {}
        if self.tokenizer_kwargs is None:
            self.tokenizer_kwargs = {}
        
        if not self.adapter_name or not self.adapter_name.strip():
            raise ValueError("adapter_name cannot be empty")
        
        self.adapter_name = self.adapter_name.strip()


@dataclass
class BenchmarkStub:
    """A single benchmark execution task (model × sparse_config × benchmark × subset).
    
    This represents one atomic execution unit that will be processed by a worker process.
    Each stub corresponds to a specific combination of model, sparse attention config,
    benchmark, and subset (if applicable).
    
    Attributes:
        model_name: HuggingFace model name (e.g., "meta-llama/Llama-3.1-8B-Instruct")
        sparse_config_name: Human-readable name for sparse config (e.g., "dense", "streaming")
        sparse_attention_config: SparseAttentionConfig instance or None for dense
        benchmark_name: Name of the benchmark (e.g., "longbench")
        subset: Specific subset name (e.g., "narrativeqa") or None for full benchmark
        adapter_config: Base adapter configuration (without sparse attention)
        generation_kwargs: Parameters for model inference/generation
        result_dir: Full path to directory where results should be saved
    
    Example:
        >>> stub = BenchmarkStub(
        ...     model_name="microsoft/Phi-4-mini-instruct",
        ...     sparse_config_name="dense",
        ...     sparse_attention_config=None,
        ...     benchmark_name="longbench",
        ...     subset="narrativeqa",
        ...     adapter_config=adapter_config,
        ...     generation_kwargs={"max_new_tokens": 256},
        ...     result_dir="/path/to/results/phi4_dense_longbench_narrativeqa"
        ... )
    """
    
    model_name: str
    sparse_config_name: str
    sparse_attention_config: Optional[SparseAttentionConfig]
    benchmark_name: str
    subset: Optional[str]
    adapter_config: AdapterConfig
    generation_kwargs: Dict[str, Any]
    result_dir: str
    
    def __post_init__(self) -> None:
        """Validate benchmark stub configuration."""
        if not self.model_name or not self.model_name.strip():
            raise ValueError("model_name cannot be empty")
        if not self.sparse_config_name or not self.sparse_config_name.strip():
            raise ValueError("sparse_config_name cannot be empty")
        if not self.benchmark_name or not self.benchmark_name.strip():
            raise ValueError("benchmark_name cannot be empty")
        if not self.result_dir or not self.result_dir.strip():
            raise ValueError("result_dir cannot be empty")
        
        # Trim whitespace
        self.model_name = self.model_name.strip()
        self.sparse_config_name = self.sparse_config_name.strip()
        self.benchmark_name = self.benchmark_name.strip()
        self.result_dir = self.result_dir.strip()
        
        if self.subset is not None:
            self.subset = self.subset.strip() if self.subset.strip() else None


@dataclass
class BenchmarkResult:
    """Result from a single benchmark execution.
    
    Attributes:
        stub: The benchmark stub that was executed
        metrics: Evaluation metrics returned by benchmark.run_benchmark()
        execution_time: Time taken to execute the benchmark in seconds
        success: Whether the benchmark executed successfully
        error_message: Error message if execution failed
    """
    
    stub: BenchmarkStub
    metrics: Optional[Dict[str, Any]]
    execution_time: float
    success: bool
    error_message: Optional[str] = None


@dataclass
class BenchmarkFailure:
    """Information about a failed benchmark execution.
    
    Attributes:
        stub: The benchmark stub that failed
        error_message: Description of the failure
        error_type: Type of error that occurred
        execution_time: Time spent before failure in seconds
    """
    
    stub: BenchmarkStub
    error_message: str
    error_type: str
    execution_time: float


@dataclass
class ExecutionProgress:
    """Tracks execution progress across all processes.
    
    Attributes:
        total_stubs: Total number of benchmark stubs to execute
        skipped_stubs: Number of stubs skipped due to existing results (resumability)
        completed_stubs: Number of stubs completed successfully
        failed_stubs: Number of stubs that failed execution
        current_executions: Mapping of GPU ID to current task description
    """
    
    total_stubs: int
    skipped_stubs: int = 0
    completed_stubs: int = 0
    failed_stubs: int = 0
    current_executions: Dict[int, str] = field(default_factory=dict)
    
    @property
    def pending_stubs(self) -> int:
        """Calculate number of stubs pending execution."""
        return self.total_stubs - self.skipped_stubs - self.completed_stubs - self.failed_stubs
    
    @property
    def completion_percentage(self) -> float:
        """Calculate completion percentage (0-100)."""
        if self.total_stubs == 0:
            return 100.0
        return (self.completed_stubs + self.failed_stubs) / (self.total_stubs - self.skipped_stubs) * 100.0


@dataclass
class ExecutionResults:
    """Aggregated results from benchmark execution.
    
    Attributes:
        execution_summary: Summary metadata about the execution
        individual_results: List of successful benchmark results
        failed_executions: List of failed benchmark executions
        execution_time: Total execution time in seconds
        progress: Final progress statistics
    """
    
    execution_summary: Dict[str, Any]
    individual_results: List[BenchmarkResult]
    failed_executions: List[BenchmarkFailure]
    execution_time: float
    progress: ExecutionProgress


def create_benchmark_instance(
    benchmark_name: str, 
    subsets: Optional[List[str]] = None
) -> Benchmark:
    """Factory function to create benchmark instances by name.
    
    This function provides a centralized way to instantiate benchmark classes
    based on string names, supporting all available benchmarks in the system.
    
    Args:
        benchmark_name: Name of the benchmark to create
        subsets: Optional list of subsets to run for the benchmark
        
    Returns:
        Instantiated benchmark object
        
    Raises:
        ValueError: If benchmark_name is not recognized
        ImportError: If benchmark module cannot be imported
        
    Example:
        >>> benchmark = create_benchmark_instance("longbench", ["narrativeqa", "qasper"])
        >>> benchmark = create_benchmark_instance("mock_benchmark")
    """
    # Normalize benchmark name
    benchmark_name = benchmark_name.strip().lower()
    
    # Registry mapping benchmark names to their import paths and class names
    benchmark_registry: Dict[str, tuple[str, str]] = {
        "longbench": ("benchmark.longbench", "LongBench"),
        "mock_benchmark": ("benchmark.mock_benchmark", "MockBenchmark"),
        "aime2024": ("benchmark.AIME2024", "AIME2024"),
        "aime2025": ("benchmark.AIME2025", "AIME2025"),
        "infinite_bench": ("benchmark.infinite_bench", "InfiniteBench"),
        "infbench": ("benchmark.infinite_bench", "InfiniteBench"),  # Alias
        "infinitebench": ("benchmark.infinite_bench", "InfiniteBench"),  # Alias
        "longbenchv2": ("benchmark.longbenchv2", "LongBenchV2"),
        "loogle": ("benchmark.loogle", "Loogle"),
        "ruler": ("benchmark.ruler", "Ruler"),
        "zero_scrolls": ("benchmark.zero_scrolls", "ZeroScrolls"),
        "zeroscrolls": ("benchmark.zero_scrolls", "ZeroScrolls"),  # Alias
    }
    
    if benchmark_name not in benchmark_registry:
        available_benchmarks: List[str] = sorted(benchmark_registry.keys())
        raise ValueError(
            f"Unknown benchmark '{benchmark_name}'. Available benchmarks: {available_benchmarks}"
        )
    
    module_path, class_name = benchmark_registry[benchmark_name]
    
    try:
        # Dynamic import of the benchmark module
        import importlib
        module = importlib.import_module(module_path)
        benchmark_class = getattr(module, class_name)
        
        # Create instance with optional subsets
        benchmark_instance: Benchmark = benchmark_class(subsets_to_run=subsets)
        
        return benchmark_instance
        
    except ImportError as e:
        raise ImportError(f"Failed to import benchmark '{benchmark_name}' from {module_path}: {e}")
    except AttributeError as e:
        raise ImportError(f"Benchmark class '{class_name}' not found in {module_path}: {e}")
    except Exception as e:
        raise ValueError(f"Failed to create benchmark '{benchmark_name}': {e}")


def get_available_benchmarks() -> List[str]:
    """Get list of all available benchmark names.
    
    Returns:
        Sorted list of benchmark names that can be used with create_benchmark_instance()
        
    Example:
        >>> benchmarks = get_available_benchmarks()
        >>> print(benchmarks)
        ['aime2024', 'aime2025', 'infinite_bench', 'longbench', 'mock_benchmark', ...]
    """
    # Extract benchmark names from the registry in create_benchmark_instance
    benchmark_names: List[str] = [
        "longbench", "mock_benchmark", "aime2024", "aime2025", 
        "infinite_bench", "longbenchv2", "loogle", "ruler", "zero_scrolls"
    ]
    
    return sorted(benchmark_names)


def validate_benchmark_config(config: BenchmarkConfig) -> None:
    """Validate a benchmark configuration against available benchmarks.
    
    This function checks if the benchmark exists and if specified subsets are valid.
    
    Args:
        config: BenchmarkConfig to validate
        
    Raises:
        ValueError: If benchmark or subsets are invalid
        
    Example:
        >>> config = BenchmarkConfig("longbench", ["narrativeqa"])
        >>> validate_benchmark_config(config)  # Passes validation
        >>> 
        >>> config = BenchmarkConfig("invalid_benchmark")
        >>> validate_benchmark_config(config)  # Raises ValueError
    """
    try:
        # Create temporary benchmark instance to validate
        benchmark = create_benchmark_instance(config.benchmark_name, subsets=None)
        
        # Validate subsets if specified
        config.validate_with_benchmark_instance(benchmark)
        
    except Exception as e:
        raise ValueError(f"Invalid benchmark configuration: {e}")


def sanitize_model_name(model_name: str) -> str:
    """Sanitize model name for use in directory paths.
    
    Replaces special characters that are not filesystem-safe with underscores.
    This ensures model names can be used as directory names across different operating systems.
    
    Args:
        model_name: Original model name (e.g., "meta-llama/Llama-3.1-8B-Instruct")
        
    Returns:
        Sanitized model name safe for filesystem use
        
    Example:
        >>> sanitized = sanitize_model_name("meta-llama/Llama-3.1-8B-Instruct")
        >>> print(sanitized)
        "meta_llama_Llama_3.1_8B_Instruct"
    """
    # Characters that need to be replaced: / \ : * ? " < > |
    special_chars: str = r'\/\:*?"<>|'
    
    sanitized: str = model_name
    for char in special_chars:
        sanitized = sanitized.replace(char, '_')
    
    # Remove multiple consecutive underscores
    while '__' in sanitized:
        sanitized = sanitized.replace('__', '_')
    
    # Remove leading/trailing underscores
    sanitized = sanitized.strip('_')
    
    return sanitized


def construct_result_directory(
    base_result_dir: str,
    execution_timestamp: str,
    model_name: str,
    sparse_config_name: str,
    benchmark_name: str,
    subset: Optional[str] = None
) -> str:
    """Construct result directory path following the standard hierarchy.
    
    Creates a path following the pattern:
    base_dir/execution_timestamp/sanitized_model/sparse_config/benchmark_subset/
    
    Args:
        base_result_dir: Base directory for all benchmark results
        execution_timestamp: Timestamp string for this execution (e.g., "20250115_143022")
        model_name: Original model name (will be sanitized)
        sparse_config_name: Name of sparse attention configuration
        benchmark_name: Name of the benchmark
        subset: Optional subset name (appended to benchmark_name with underscore)
        
    Returns:
        Complete path to result directory
        
    Example:
        >>> path = construct_result_directory(
        ...     "./results", "20250115_143022", "meta-llama/Llama-3.1-8B", 
        ...     "dense", "longbench", "narrativeqa"
        ... )
        >>> print(path)
        "./results/execution_20250115_143022/meta_llama_Llama_3.1_8B/dense/longbench_narrativeqa"
    """
    sanitized_model: str = sanitize_model_name(model_name)
    
    # Construct benchmark directory name
    benchmark_dir: str = benchmark_name
    if subset is not None:
        benchmark_dir = f"{benchmark_name}_{subset}"
    
    # Build path components
    path_components: List[str] = [
        base_result_dir,
        f"execution_{execution_timestamp}",
        sanitized_model,
        sparse_config_name,
        benchmark_dir
    ]
    
    result_path: str = os.path.join(*path_components)
    return str(Path(result_path).resolve())
```

Perfect! Now let me update the to-do list to mark Task 1.1 as complete. I've successfully created the core configuration classes and benchmark factory registry.

Let me test the implementation to ensure everything works correctly:

Excellent! **Phase 1 Task 1.1 is now complete**! 🎉

## ✅ Task 1.1: Core Data Structures and Configuration Classes - COMPLETED

I have successfully implemented all required components:

### ✅ 1.1.1 Created `benchmark/executor_config.py` with:
- **BenchmarkConfig** dataclass with `benchmark_name`, `subsets`, and validation methods
- **AdapterConfig** dataclass with `adapter_name`, `model_kwargs`, `tokenizer_kwargs` (without sparse_attention_config as planned)
- **BenchmarkStub** dataclass with all required fields including `sparse_config_name`, `sparse_attention_config`, and `result_dir` path construction logic
- **ExecutionResults** and **ExecutionProgress** dataclasses for result aggregation and progress tracking
- **BenchmarkResult** and **BenchmarkFailure** dataclasses for detailed result management

### ✅ 1.1.2 Created benchmark factory registry with:
- **`create_benchmark_instance()`** function to instantiate benchmarks by name  
- Support for all available benchmarks: LongBench, MockBenchmark, AIME2024, AIME2025, InfiniteBench, LongBenchV2, Loogle, Ruler, ZeroScrolls
- Comprehensive error handling for unknown benchmark names
- **`get_available_benchmarks()`** utility function
- **`validate_benchmark_config()`** function for configuration validation

### ✅ Additional Features Implemented:
- **`sanitize_model_name()`** for filesystem-safe model names  
- **`construct_result_directory()`** for standardized directory structure
- Comprehensive type annotations following workspace rules
- Google-style docstrings for all classes and methods
- Input validation and error handling throughout

### ✅ Testing Results:
- All imports work correctly
- Configuration classes create successfully
- Benchmark factory creates actual benchmark instances
- Available benchmarks list returns correctly: `['aime2024', 'aime2025', 'infinite_bench', 'longbench', 'longbenchv2', 'loogle', 'mock_benchmark', 'ruler', 'zero_scrolls']`
- Model name sanitization works properly

**Deliverable**: ✅ Working configuration classes and benchmark instantiation ready for the next phase

The foundation is now solid and ready for **Task 1.2: Directory Management and Name Sanitization**. All the core data structures are in place to support the full benchmark executor matrix execution system.

---

**User**

Mark it as complete in todo file

---

**Cursor**

I'll mark Task 1.1.1 and 1.1.2 as completed in the todo file since we've successfully implemented the core data structures and benchmark factory registry.

```markdown
- [ ] **1.1.1** Create `benchmark/executor_config.py`
  - [ ] Define `BenchmarkConfig` dataclass
    - [ ] `benchmark_name: str`
    - [ ] `subsets: Optional[List[str]]`
    - [ ] Validation methods
  - [ ] Define `AdapterConfig` dataclass (without sparse_attention_config)
    - [ ] `adapter_name: str = "huggingface"`
    - [ ] `model_kwargs: Optional[Dict[str, Any]]`
    - [ ] `tokenizer_kwargs: Optional[Dict[str, Any]]`
  - [ ] Define `BenchmarkStub` dataclass
    - [ ] All required fields including `sparse_config_name`, `sparse_attention_config`
    - [ ] `result_dir` path construction logic
  - [ ] Define `ExecutionResults` and `ExecutionProgress` dataclasses

- [ ] **1.1.2** Create benchmark factory registry
  - [ ] Function to instantiate benchmark classes by name
  - [ ] Support for LongBench, MockBenchmark, etc.
  - [ ] Error handling for unknown benchmark names

->

- [x] **1.1.1** Create `benchmark/executor_config.py`
  - [x] Define `BenchmarkConfig` dataclass
    - [x] `benchmark_name: str`
    - [x] `subsets: Optional[List[str]]`
    - [x] Validation methods
  - [x] Define `AdapterConfig` dataclass (without sparse_attention_config)
    - [x] `adapter_name: str = "huggingface"`
    - [x] `model_kwargs: Optional[Dict[str, Any]]`
    - [x] `tokenizer_kwargs: Optional[Dict[str, Any]]`
  - [x] Define `BenchmarkStub` dataclass
    - [x] All required fields including `sparse_config_name`, `sparse_attention_config`
    - [x] `result_dir` path construction logic
  - [x] Define `ExecutionResults` and `ExecutionProgress` dataclasses

- [x] **1.1.2** Create benchmark factory registry
  - [x] Function to instantiate benchmark classes by name
  - [x] Support for LongBench, MockBenchmark, etc.
  - [x] Error handling for unknown benchmark names
```

Perfect! I've marked Task 1.1.1 and Task 1.1.2 as completed in the todo file. Both subtasks are now marked with `[x]` to reflect that we've successfully implemented:

✅ **Task 1.1.1**: Created `benchmark/executor_config.py` with all required dataclasses:
- `BenchmarkConfig` with validation methods
- `AdapterConfig` (without sparse_attention_config as per matrix design) 
- `BenchmarkStub` with all required fields including sparse_config_name and result_dir logic
- `ExecutionResults` and `ExecutionProgress` dataclasses

✅ **Task 1.1.2**: Created benchmark factory registry:
- `create_benchmark_instance()` function with support for all 9 available benchmarks
- Dynamic import system supporting LongBench, MockBenchmark, AIME2024/2025, etc.
- Comprehensive error handling for unknown benchmark names

The foundation is now complete and ready for **Task 1.2: Directory Management and Name Sanitization**. The next step would be implementing the model name sanitization and result directory structure logic.

---

**User**

Move on to Task 1.2 

---

**Cursor**

I'll now implement Task 1.2: Directory Management and Name Sanitization. Let me first check the current state of our `executor_config.py` file and then add the required functionality.

I can see that I've already implemented most of Task 1.2 functionality in the `executor_config.py` file! Let me test the current implementation and then add the missing pieces.

Excellent! The sanitization and directory construction functions are working perfectly. Now I need to add the missing pieces for Task 1.2: directory creation logic and path validation. Let me add these functions to complete the task.

```python
// ... existing code ...

def construct_result_directory(
    base_result_dir: str,
    execution_timestamp: str,
    model_name: str,
    sparse_config_name: str,
    benchmark_name: str,
    subset: Optional[str] = None
) -> str:
    """Construct result directory path following the standard hierarchy.
    
    Creates a path following the pattern:
    base_dir/execution_timestamp/sanitized_model/sparse_config/benchmark_subset/
    
    Args:
        base_result_dir: Base directory for all benchmark results
        execution_timestamp: Timestamp string for this execution (e.g., "20250115_143022")
        model_name: Original model name (will be sanitized)
        sparse_config_name: Name of sparse attention configuration
        benchmark_name: Name of the benchmark
        subset: Optional subset name (appended to benchmark_name with underscore)
        
    Returns:
        Complete path to result directory
        
    Example:
        >>> path = construct_result_directory(
        ...     "./results", "20250115_143022", "meta-llama/Llama-3.1-8B", 
        ...     "dense", "longbench", "narrativeqa"
        ... )
        >>> print(path)
        "./results/execution_20250115_143022/meta_llama_Llama_3.1_8B/dense/longbench_narrativeqa"
    """
    sanitized_model: str = sanitize_model_name(model_name)
    
    # Construct benchmark directory name
    benchmark_dir: str = benchmark_name
    if subset is not None:
        benchmark_dir = f"{benchmark_name}_{subset}"
    
    # Build path components
    path_components: List[str] = [
        base_result_dir,
        f"execution_{execution_timestamp}",
        sanitized_model,
        sparse_config_name,
        benchmark_dir
    ]
    
    result_path: str = os.path.join(*path_components)
    return str(Path(result_path).resolve())


def create_result_directory(
    result_dir: str,
    create_parents: bool = True
) -> bool:
    """Create result directory with proper error handling.
    
    Creates the specified directory and all necessary parent directories.
    This function is safe to call multiple times on the same path.
    
    Args:
        result_dir: Path to the directory to create
        create_parents: Whether to create parent directories if they don't exist
        
    Returns:
        True if directory was created or already exists, False if creation failed
        
    Raises:
        ValueError: If result_dir is empty or invalid
        PermissionError: If insufficient permissions to create directory
        OSError: If directory creation fails for other reasons
        
    Example:
        >>> success = create_result_directory("/path/to/results/model_experiment")
        >>> print(success)  # True if successful
    """
    if not result_dir or not result_dir.strip():
        raise ValueError("result_dir cannot be empty")
    
    result_path = Path(result_dir.strip())
    
    try:
        # Create directory and parents if needed
        result_path.mkdir(parents=create_parents, exist_ok=True)
        
        # Verify directory was created and is writable
        if not result_path.exists():
            return False
        
        if not result_path.is_dir():
            raise OSError(f"Path exists but is not a directory: {result_path}")
        
        # Test write permissions by creating a temporary file
        test_file = result_path / ".write_test"
        try:
            test_file.touch()
            test_file.unlink()  # Remove test file
        except (PermissionError, OSError) as e:
            raise PermissionError(f"Directory created but not writable: {result_path}") from e
        
        return True
        
    except PermissionError:
        raise
    except OSError as e:
        raise OSError(f"Failed to create directory {result_path}: {e}") from e


def validate_result_directory_path(
    result_dir: str,
    check_parent_writable: bool = True
) -> None:
    """Validate that a result directory path is valid and can be created.
    
    Performs validation without actually creating the directory.
    
    Args:
        result_dir: Path to validate
        check_parent_writable: Whether to check if parent directory is writable
        
    Raises:
        ValueError: If path is invalid or unsafe
        PermissionError: If parent directory is not writable
        OSError: If path validation fails
        
    Example:
        >>> validate_result_directory_path("/valid/path/to/results")  # Passes
        >>> validate_result_directory_path("")  # Raises ValueError
    """
    if not result_dir or not result_dir.strip():
        raise ValueError("result_dir cannot be empty")
    
    result_path = Path(result_dir.strip())
    
    # Check for potentially dangerous paths
    try:
        # Resolve to absolute path to detect path traversal attempts
        absolute_path = result_path.resolve()
        
        # Basic security check - ensure path doesn't try to escape reasonable bounds
        if ".." in str(result_path):
            # Allow .. only if resolved path is still reasonable
            path_str = str(absolute_path)
            if any(dangerous in path_str for dangerous in ["/etc", "/usr", "/bin", "/sbin", "/sys", "/proc"]):
                raise ValueError(f"Path appears to target system directories: {result_path}")
        
    except (OSError, RuntimeError) as e:
        raise ValueError(f"Invalid path format: {result_path}") from e
    
    # Check if parent directory exists and is writable (if requested)
    if check_parent_writable:
        parent_dir = result_path.parent
        
        if not parent_dir.exists():
            # Check if we can create the parent directories
            try:
                # Test by checking the first existing parent
                existing_parent = parent_dir
                while not existing_parent.exists() and existing_parent.parent != existing_parent:
                    existing_parent = existing_parent.parent
                
                if existing_parent.exists() and not os.access(existing_parent, os.W_OK):
                    raise PermissionError(f"Cannot write to parent directory tree: {existing_parent}")
                    
            except (OSError, AttributeError) as e:
                raise PermissionError(f"Cannot validate parent directory permissions: {parent_dir}") from e
        
        elif not os.access(parent_dir, os.W_OK):
            raise PermissionError(f"Parent directory is not writable: {parent_dir}")


def ensure_result_directory(
    result_dir: str,
    validate_first: bool = True
) -> str:
    """Ensure result directory exists and is ready for use.
    
    This is a convenience function that combines validation and creation.
    
    Args:
        result_dir: Path to the result directory
        validate_first: Whether to validate the path before creating
        
    Returns:
        Absolute path to the created directory
        
    Raises:
        ValueError: If path validation fails
        PermissionError: If directory cannot be created due to permissions
        OSError: If directory creation fails
        
    Example:
        >>> abs_path = ensure_result_directory("./results/experiment_1")
        >>> print(abs_path)  # "/absolute/path/to/results/experiment_1"
    """
    if validate_first:
        validate_result_directory_path(result_dir, check_parent_writable=True)
    
    success = create_result_directory(result_dir, create_parents=True)
    if not success:
        raise OSError(f"Failed to create result directory: {result_dir}")
    
    # Return absolute path
    return str(Path(result_dir).resolve())
```

Now let me test the new directory creation and validation functions to ensure they work correctly:

Perfect! All the directory management functions are working correctly. Let me test the complete integration with BenchmarkStub to make sure everything works together:

