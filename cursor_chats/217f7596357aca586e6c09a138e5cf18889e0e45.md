# Creating init functions and configs for maskers
_Exported on 6/28/2025 at 17:37:11 PDT from Cursor (1.1.4)_

---

**User**

<TITLE>
Creating __init__ functions, configs and create_from_config for maskers
</TITLE>



<IMPORTANT>
1. If trying to delete some file, ensure that the contents of that file are refactored / moved somewhere else.
</IMPORTANT>


<TASK>
For each class in maskers we want to add
1. __init__ function with arguments that I will specify
2. config class -- a data class for those arguments.
3. create_from_config class method which will use the config to create the class. 
</TASK>

Interact with me asking what arguments should init of each class should take and accomplish the above TASK. Start by listing the class and file structure of maskers folder and start asking me the arguments for each class

---

**Cursor**

I'll help you create `__init__` functions, config classes, and `create_from_config` methods for all the masker classes. Let me first explore the maskers folder structure to understand what classes we're working with.


Now let me examine the base classes and implementations to understand the current structure:


Now let me examine each implementation to understand what parameters they currently use:


Perfect! Now I have a complete understanding of the maskers folder structure. Here's the summary of all the classes that need `__init__` functions, config classes, and `create_from_config` methods:

## Maskers Folder Structure and Classes

### Base Classes:
1. **ResearchMasker** (abstract base)
2. **FixedMasker** (abstract base)
3. **TopKMasker** (abstract base)
4. **TopPMasker** (abstract base)
5. **SamplingMasker** (abstract base)

### Fixed Pattern Implementations:
6. **LocalMasker** (`basic_fixed.py`)
7. **CausalMasker** (`basic_fixed.py`)
8. **SinkMasker** (`basic_fixed.py`)
9. **DoubleSparsity** (`double_sparsity_top_k.py`)
10. **HashAttention** (`hashattention_top_k.py`)
11. **OracleTopK** (`oracle_top_k.py`)
12. **PQCache** (`pq_top_k.py`)

### Sampling Implementations:
13. **RandomSamplingMasker** (`random_sampling.py`)
14. **MagicPig** (`magic_pig.py`)

Now I need to ask you about the `__init__` arguments for each class. Let me start with the fixed pattern implementations:

## Questions for Fixed Pattern Maskers:

**For LocalMasker:**
- What parameters should the `__init__` take? (e.g., window_size, stride, etc.)

**For CausalMasker:**
- What parameters should the `__init__` take? (e.g., causal_direction, etc.)

**For SinkMasker:**
- What parameters should the `__init__` take? (e.g., sink_tokens, sink_position, etc.)

**For Top-K based maskers (DoubleSparsity, HashAttention, OracleTopK, PQCache):**
- What parameters should the `__init__` take? (e.g., k_value, sparsity_ratio, etc.)

## Questions for Sampling Maskers:

**For RandomSamplingMasker:**
- What parameters should the `__init__` take? (e.g., sampling_ratio, seed, etc.)

**For MagicPig:**
- What parameters should the `__init__` take? (e.g., magic_ratio, pig_config, etc.)

Please provide the specific arguments and their types for each class's `__init__` function.

---

**User**

The answer are as follows:
For LocalMasker:
1. window_size: it can be a float (0,1) or an integer 
For CausalMasker:
1. No parameters. Does not need a config
For SinkMasker:
1. sink_size:  it can be a float (0,1) or an integer
For Top-K base masker
1. heavy_size: it can be a float (0,1) or an integer
Additional parameters for DoubleSparsity:
1. group_factor : determines how many channels to use
2. label_bits: determines how much to quantize
3. channel_config: config with stats required for double sparsity
Additional parameters for HashAttention:
1. hat_bits
2. hat_mlp_layers
3. hat_mlp_hidden_size
Additional parameters for OracleTopK
None
Addtional parameters for PQCache
1. pq_sub_dim: sub vector size
2. pq_bits : quantization bits for each sub vector, determines the number of centeriods
For base Sampling Maskers:
sampling_rate
Additional for RandomSamplingMasker:
None
Additional for MagicPig:
1. lsh_l : number of tables
2. lsh_k : number of bits per table.

---

**Cursor**

Perfect! Thank you for providing the detailed parameters. Now I'll implement the `__init__` functions, config classes, and `create_from_config` methods for all the masker classes. Let me start by adding the necessary imports and then implement each class.


```python
"""Base classes for research maskers."""

from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, List, Optional, Union


@dataclass
class MaskerConfig:
    """Base configuration class for all maskers."""
    pass


class ResearchMasker(ABC):
    """Abstract base class for research maskers."""

    def __init__(self, config: MaskerConfig):
        """Initialize masker with configuration."""
        self.config = config

    @abstractmethod
    def add_mask(
        self,
        keys: Any,
        queries: Any,
        values: Any,
        previous_attention_mask: Any,
        prev_num: Any,
        prev_den: Any,
        maskers: List["ResearchMasker"],
    ) -> None:
        """Add mask to attention computation."""
        pass

    @abstractmethod
    def get_attention_numerator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention numerator with mask applied."""
        pass

    @abstractmethod
    def get_attention_denominator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention denominator with mask applied."""
        pass

    @classmethod
    @abstractmethod
    def create_from_config(cls, config: MaskerConfig) -> "ResearchMasker":
        """Create masker instance from configuration."""
        pass
```

```python
"""Base fixed pattern masker implementations."""

from dataclasses import dataclass
from typing import Union

from ..base import ResearchMasker, MaskerConfig


@dataclass
class FixedMaskerConfig(MaskerConfig):
    """Base configuration for fixed pattern maskers."""
    pass


class FixedMasker(ResearchMasker):
    """Abstract base class for fixed pattern maskers."""

    def __init__(self, config: FixedMaskerConfig):
        """Initialize fixed masker with configuration."""
        super().__init__(config)

    @classmethod
    def create_from_config(cls, config: FixedMaskerConfig) -> "FixedMasker":
        """Create fixed masker instance from configuration."""
        return cls(config)


@dataclass
class TopKMaskerConfig(FixedMaskerConfig):
    """Base configuration for top-K maskers."""
    heavy_size: Union[float, int]


class TopKMasker(FixedMasker):
    """Abstract base class for top-K maskers."""

    def __init__(self, config: TopKMaskerConfig):
        """Initialize top-K masker with configuration."""
        super().__init__(config)

    @classmethod
    def create_from_config(cls, config: TopKMaskerConfig) -> "TopKMasker":
        """Create top-K masker instance from configuration."""
        return cls(config)


@dataclass
class TopPMaskerConfig(FixedMaskerConfig):
    """Base configuration for top-P maskers."""
    pass


class TopPMasker(FixedMasker):
    """Abstract base class for top-P maskers."""

    def __init__(self, config: TopPMaskerConfig):
        """Initialize top-P masker with configuration."""
        super().__init__(config)

    @classmethod
    def create_from_config(cls, config: TopPMaskerConfig) -> "TopPMasker":
        """Create top-P masker instance from configuration."""
        return cls(config)
```

```python
"""Base sampling masker implementations."""

from dataclasses import dataclass
from typing import Union

from ..base import ResearchMasker, MaskerConfig


@dataclass
class SamplingMaskerConfig(MaskerConfig):
    """Base configuration for sampling maskers."""
    sampling_rate: Union[float, int]


class SamplingMasker(ResearchMasker):
    """Abstract base class for sampling-based maskers."""

    def __init__(self, config: SamplingMaskerConfig):
        """Initialize sampling masker with configuration."""
        super().__init__(config)

    @classmethod
    def create_from_config(cls, config: SamplingMaskerConfig) -> "SamplingMasker":
        """Create sampling masker instance from configuration."""
        return cls(config)
```

Now let me implement the fixed pattern maskers:


```python
"""Basic fixed pattern masker implementations."""

from dataclasses import dataclass
from typing import Any, List, Union

from ...base import ResearchMasker
from ..base import FixedMasker, FixedMaskerConfig


@dataclass
class LocalMaskerConfig(FixedMaskerConfig):
    """Configuration for LocalMasker."""
    window_size: Union[float, int]


class LocalMasker(FixedMasker):
    """Local attention masker."""

    def __init__(self, config: LocalMaskerConfig):
        """Initialize local masker with configuration."""
        super().__init__(config)
        self.window_size = config.window_size

    def add_mask(
        self,
        keys: Any,
        queries: Any,
        values: Any,
        previous_attention_mask: Any,
        prev_num: Any,
        prev_den: Any,
        maskers: List[ResearchMasker],
    ) -> None:
        """Add local mask."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_numerator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention numerator."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_denominator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention denominator."""
        # Bare metal implementation - no functionality
        pass

    @classmethod
    def create_from_config(cls, config: LocalMaskerConfig) -> "LocalMasker":
        """Create LocalMasker instance from configuration."""
        return cls(config)


class CausalMasker(FixedMasker):
    """Causal attention masker."""

    def __init__(self, config: FixedMaskerConfig):
        """Initialize causal masker with configuration."""
        super().__init__(config)

    def add_mask(
        self,
        keys: Any,
        queries: Any,
        values: Any,
        previous_attention_mask: Any,
        prev_num: Any,
        prev_den: Any,
        maskers: List[ResearchMasker],
    ) -> None:
        """Add causal mask."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_numerator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention numerator."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_denominator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention denominator."""
        # Bare metal implementation - no functionality
        pass

    @classmethod
    def create_from_config(cls, config: FixedMaskerConfig) -> "CausalMasker":
        """Create CausalMasker instance from configuration."""
        return cls(config)


@dataclass
class SinkMaskerConfig(FixedMaskerConfig):
    """Configuration for SinkMasker."""
    sink_size: Union[float, int]


class SinkMasker(FixedMasker):
    """Sink attention masker."""

    def __init__(self, config: SinkMaskerConfig):
        """Initialize sink masker with configuration."""
        super().__init__(config)
        self.sink_size = config.sink_size

    def add_mask(
        self,
        keys: Any,
        queries: Any,
        values: Any,
        previous_attention_mask: Any,
        prev_num: Any,
        prev_den: Any,
        maskers: List[ResearchMasker],
    ) -> None:
        """Add sink mask."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_numerator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention numerator."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_denominator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention denominator."""
        # Bare metal implementation - no functionality
        pass

    @classmethod
    def create_from_config(cls, config: SinkMaskerConfig) -> "SinkMasker":
        """Create SinkMasker instance from configuration."""
        return cls(config)
```

Now let me implement the Top-K based maskers:


```python
"""Double sparsity top-K masker implementation."""

from dataclasses import dataclass
from typing import Any, List, Union, Any

from ...base import ResearchMasker
from ..base import TopKMasker, TopKMaskerConfig


@dataclass
class DoubleSparsityConfig(TopKMaskerConfig):
    """Configuration for DoubleSparsity masker."""
    group_factor: int
    label_bits: int
    channel_config: Any  # config with stats required for double sparsity


class DoubleSparsity(TopKMasker):
    """Double sparsity masker."""

    def __init__(self, config: DoubleSparsityConfig):
        """Initialize double sparsity masker with configuration."""
        super().__init__(config)
        self.heavy_size = config.heavy_size
        self.group_factor = config.group_factor
        self.label_bits = config.label_bits
        self.channel_config = config.channel_config

    def add_mask(
        self,
        keys: Any,
        queries: Any,
        values: Any,
        previous_attention_mask: Any,
        prev_num: Any,
        prev_den: Any,
        maskers: List[ResearchMasker],
    ) -> None:
        """Add double sparsity mask."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_numerator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention numerator."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_denominator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention denominator."""
        # Bare metal implementation - no functionality
        pass

    @classmethod
    def create_from_config(cls, config: DoubleSparsityConfig) -> "DoubleSparsity":
        """Create DoubleSparsity instance from configuration."""
        return cls(config)
```

```python
"""Hash attention top-K masker implementation."""

from dataclasses import dataclass
from typing import Any, List, Union

from ...base import ResearchMasker
from ..base import TopKMasker, TopKMaskerConfig


@dataclass
class HashAttentionConfig(TopKMaskerConfig):
    """Configuration for HashAttention masker."""
    hat_bits: int
    hat_mlp_layers: int
    hat_mlp_hidden_size: int


class HashAttention(TopKMasker):
    """Hash attention masker."""

    def __init__(self, config: HashAttentionConfig):
        """Initialize hash attention masker with configuration."""
        super().__init__(config)
        self.heavy_size = config.heavy_size
        self.hat_bits = config.hat_bits
        self.hat_mlp_layers = config.hat_mlp_layers
        self.hat_mlp_hidden_size = config.hat_mlp_hidden_size

    def add_mask(
        self,
        keys: Any,
        queries: Any,
        values: Any,
        previous_attention_mask: Any,
        prev_num: Any,
        prev_den: Any,
        maskers: List[ResearchMasker],
    ) -> None:
        """Add hash attention mask."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_numerator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention numerator."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_denominator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention denominator."""
        # Bare metal implementation - no functionality
        pass

    @classmethod
    def create_from_config(cls, config: HashAttentionConfig) -> "HashAttention":
        """Create HashAttention instance from configuration."""
        return cls(config)
```

```python
"""Oracle top-K masker implementation."""

from dataclasses import dataclass
from typing import Any, List, Union

from ...base import ResearchMasker
from ..base import TopKMasker, TopKMaskerConfig


@dataclass
class OracleTopKConfig(TopKMaskerConfig):
    """Configuration for OracleTopK masker."""
    pass


class OracleTopK(TopKMasker):
    """Oracle top-K masker."""

    def __init__(self, config: OracleTopKConfig):
        """Initialize oracle top-K masker with configuration."""
        super().__init__(config)
        self.heavy_size = config.heavy_size

    def add_mask(
        self,
        keys: Any,
        queries: Any,
        values: Any,
        previous_attention_mask: Any,
        prev_num: Any,
        prev_den: Any,
        maskers: List[ResearchMasker],
    ) -> None:
        """Add oracle top-K mask."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_numerator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention numerator."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_denominator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention denominator."""
        # Bare metal implementation - no functionality
        pass

    @classmethod
    def create_from_config(cls, config: OracleTopKConfig) -> "OracleTopK":
        """Create OracleTopK instance from configuration."""
        return cls(config)
```

```python
"""PQ cache-based top-K masker implementation."""

from dataclasses import dataclass
from typing import Any, List, Union

from ...base import ResearchMasker
from ..base import TopKMasker, TopKMaskerConfig


@dataclass
class PQCacheConfig(TopKMaskerConfig):
    """Configuration for PQCache masker."""
    pq_sub_dim: int
    pq_bits: int


class PQCache(TopKMasker):
    """PQ cache-based top-K masker."""

    def __init__(self, config: PQCacheConfig):
        """Initialize PQ cache masker with configuration."""
        super().__init__(config)
        self.heavy_size = config.heavy_size
        self.pq_sub_dim = config.pq_sub_dim
        self.pq_bits = config.pq_bits

    def add_mask(
        self,
        keys: Any,
        queries: Any,
        values: Any,
        previous_attention_mask: Any,
        prev_num: Any,
        prev_den: Any,
        maskers: List[ResearchMasker],
    ) -> None:
        """Add PQ cache mask."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_numerator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention numerator."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_denominator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention denominator."""
        # Bare metal implementation - no functionality
        pass

    @classmethod
    def create_from_config(cls, config: PQCacheConfig) -> "PQCache":
        """Create PQCache instance from configuration."""
        return cls(config)
```

Now let me implement the sampling maskers:


```python
"""Random sampling masker implementation."""

from dataclasses import dataclass
from typing import Any, List, Union

from ...base import ResearchMasker
from ..base import SamplingMasker, SamplingMaskerConfig


@dataclass
class RandomSamplingMaskerConfig(SamplingMaskerConfig):
    """Configuration for RandomSamplingMasker."""
    pass


class RandomSamplingMasker(SamplingMasker):
    """Random sampling masker."""

    def __init__(self, config: RandomSamplingMaskerConfig):
        """Initialize random sampling masker with configuration."""
        super().__init__(config)
        self.sampling_rate = config.sampling_rate

    def add_mask(
        self,
        keys: Any,
        queries: Any,
        values: Any,
        previous_attention_mask: Any,
        prev_num: Any,
        prev_den: Any,
        maskers: List[ResearchMasker],
    ) -> None:
        """Add random sampling mask."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_numerator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention numerator."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_denominator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention denominator."""
        # Bare metal implementation - no functionality
        pass

    @classmethod
    def create_from_config(cls, config: RandomSamplingMaskerConfig) -> "RandomSamplingMasker":
        """Create RandomSamplingMasker instance from configuration."""
        return cls(config)
```

```python
"""Magic Pig sampling masker implementation."""

from dataclasses import dataclass
from typing import Any, List, Union

from ...base import ResearchMasker
from ..base import SamplingMasker, SamplingMaskerConfig


@dataclass
class MagicPigConfig(SamplingMaskerConfig):
    """Configuration for MagicPig masker."""
    lsh_l: int  # number of tables
    lsh_k: int  # number of bits per table


class MagicPig(SamplingMasker):
    """Magic Pig masker."""

    def __init__(self, config: MagicPigConfig):
        """Initialize Magic Pig masker with configuration."""
        super().__init__(config)
        self.sampling_rate = config.sampling_rate
        self.lsh_l = config.lsh_l
        self.lsh_k = config.lsh_k

    def add_mask(
        self,
        keys: Any,
        queries: Any,
        values: Any,
        previous_attention_mask: Any,
        prev_num: Any,
        prev_den: Any,
        maskers: List[ResearchMasker],
    ) -> None:
        """Add Magic Pig mask."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_numerator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention numerator."""
        # Bare metal implementation - no functionality
        pass

    def get_attention_denominator(
        self, keys: Any, queries: Any, values: Any, mask: Any
    ) -> Any:
        """Get attention denominator."""
        # Bare metal implementation - no functionality
        pass

    @classmethod
    def create_from_config(cls, config: MagicPigConfig) -> "MagicPig":
        """Create MagicPig instance from configuration."""
        return cls(config)
```

Now I need to update the `__init__.py` files to export the new config classes and ensure proper imports. Let me start with the main maskers `__init__.py`:


```python
"""Research attention maskers."""

from .base import ResearchMasker, MaskerConfig

__all__ = [
    "ResearchMasker",
    "MaskerConfig",
]
```

```python
"""Fixed pattern maskers."""

from .base import (
    FixedMasker, 
    TopKMasker, 
    TopPMasker,
    FixedMaskerConfig,
    TopKMaskerConfig,
    TopPMaskerConfig
)
from .implementations import (
    CausalMasker,
    LocalMasker,
    SinkMasker,
    OracleTopK,
    PQCache,
    HashAttention as RHashAttention,
    DoubleSparsity as RDoubleSparsity,
    LocalMaskerConfig,
    SinkMaskerConfig,
    OracleTopKConfig,
    PQCacheConfig,
    HashAttentionConfig,
    DoubleSparsityConfig,
)

__all__ = [
    "FixedMasker",
    "TopKMasker", 
    "TopPMasker",
    "FixedMaskerConfig",
    "TopKMaskerConfig",
    "TopPMaskerConfig",
    "LocalMasker", 
    "CausalMasker", 
    "SinkMasker",
    "OracleTopK", 
    "PQCache", 
    "RHashAttention", 
    "RDoubleSparsity",
    "LocalMaskerConfig",
    "SinkMaskerConfig",
    "OracleTopKConfig",
    "PQCacheConfig",
    "HashAttentionConfig",
    "DoubleSparsityConfig",
]
```

```python
"""Fixed masker implementations."""

from .basic_fixed import CausalMasker, LocalMasker, SinkMasker, LocalMaskerConfig, SinkMaskerConfig
from .double_sparsity_top_k import DoubleSparsity, DoubleSparsityConfig
from .hashattention_top_k import HashAttention, HashAttentionConfig
from .oracle_top_k import OracleTopK, OracleTopKConfig
from .pq_top_k import PQCache, PQCacheConfig

__all__ = [
    "LocalMasker", 
    "CausalMasker", 
    "SinkMasker",
    "OracleTopK", 
    "PQCache", 
    "HashAttention", 
    "DoubleSparsity",
    "LocalMaskerConfig",
    "SinkMaskerConfig",
    "OracleTopKConfig",
    "PQCacheConfig",
    "HashAttentionConfig",
    "DoubleSparsityConfig",
]
```

```python
"""Sampling-based maskers."""

from .base import SamplingMasker, SamplingMaskerConfig
from .implementations import (
    RandomSamplingMasker, 
    MagicPig,
    RandomSamplingMaskerConfig,
    MagicPigConfig
)

__all__ = [
    "SamplingMasker", 
    "RandomSamplingMasker", 
    "MagicPig",
    "SamplingMaskerConfig",
    "RandomSamplingMaskerConfig",
    "MagicPigConfig"
]
```

```python
"""Sampling masker implementations."""

from .magic_pig import MagicPig, MagicPigConfig
from .random_sampling import RandomSamplingMasker, RandomSamplingMaskerConfig

__all__ = [
    "MagicPig", 
    "RandomSamplingMasker",
    "MagicPigConfig",
    "RandomSamplingMaskerConfig"
]
```

Now let me run a quick test to make sure all the imports work correctly:


---

**User**

Okay lets try testing all the imports

---

**Cursor**

Great! Let me test all the imports to make sure everything is working correctly.


I see there's a torch dependency issue. Let me try importing just the maskers module directly without going through the main package:


---

**User**

run the command "bash; conda activate infflm"

---

**Cursor**

Now let me test the imports with the correct conda environment:


Let me try importing the maskers module directly without going through the main package:


---

**User**

which python are you using

activate infllm env

---

**Cursor**

Great! Now let me test the imports with the correct environment:


Excellent! Now let me test all the fixed maskers:


I see the issue. The HashAttention class is imported as RHashAttention in the fixed maskers __init__.py. Let me fix this and test with the correct names:


Great! Now let me test the sampling maskers:


Perfect! Now let me test creating instances from configs to make sure everything works:


---

**User**

Modify the test+imports to add imports for configs

---

**Cursor**

I'll modify the test imports to include all the config classes. Let me test a comprehensive import that includes all maskers and their configs:


Excellent! Now let me test creating instances from configs for a few different masker types:


Perfect! Let me also test the more complex maskers with additional parameters:


Excellent! All the imports and config-based instantiation are working perfectly. Let me create a comprehensive test script to verify everything:


```python
#!/usr/bin/env python3
"""Test script for all masker classes, configs, and create_from_config methods."""

def test_all_maskers():
    """Test all masker imports and config-based instantiation."""
    
    print("Testing masker imports and config-based instantiation...")
    
    # Test base imports
    from sparse_attention_hub.sparse_attention.research_attention.maskers import (
        ResearchMasker, MaskerConfig
    )
    print("âœ“ Base maskers imported")
    
    # Test fixed masker imports
    from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
        LocalMasker, LocalMaskerConfig,
        CausalMasker, FixedMaskerConfig,
        SinkMasker, SinkMaskerConfig,
        OracleTopK, OracleTopKConfig,
        PQCache, PQCacheConfig,
        RHashAttention, HashAttentionConfig,
        RDoubleSparsity, DoubleSparsityConfig,
        FixedMaskerConfig, TopKMaskerConfig, TopPMaskerConfig
    )
    print("âœ“ Fixed maskers imported")
    
    # Test sampling masker imports
    from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
        RandomSamplingMasker, RandomSamplingMaskerConfig,
        MagicPig, MagicPigConfig,
        SamplingMaskerConfig
    )
    print("âœ“ Sampling maskers imported")
    
    # Test config-based instantiation for simple maskers
    print("\nTesting config-based instantiation...")
    
    # LocalMasker
    local_config = LocalMaskerConfig(window_size=0.5)
    local_masker = LocalMasker.create_from_config(local_config)
    assert local_masker.window_size == 0.5
    print("âœ“ LocalMasker created from config")
    
    # CausalMasker
    causal_config = FixedMaskerConfig()
    causal_masker = CausalMasker.create_from_config(causal_config)
    print("âœ“ CausalMasker created from config")
    
    # SinkMasker
    sink_config = SinkMaskerConfig(sink_size=10)
    sink_masker = SinkMasker.create_from_config(sink_config)
    assert sink_masker.sink_size == 10
    print("âœ“ SinkMasker created from config")
    
    # OracleTopK
    oracle_config = OracleTopKConfig(heavy_size=0.3)
    oracle_masker = OracleTopK.create_from_config(oracle_config)
    assert oracle_masker.heavy_size == 0.3
    print("âœ“ OracleTopK created from config")
    
    # RandomSamplingMasker
    random_config = RandomSamplingMaskerConfig(sampling_rate=0.8)
    random_masker = RandomSamplingMasker.create_from_config(random_config)
    assert random_masker.sampling_rate == 0.8
    print("âœ“ RandomSamplingMasker created from config")
    
    # Test complex maskers with additional parameters
    print("\nTesting complex maskers...")
    
    # HashAttention
    hash_config = HashAttentionConfig(
        heavy_size=0.4, 
        hat_bits=8, 
        hat_mlp_layers=2, 
        hat_mlp_hidden_size=64
    )
    hash_masker = RHashAttention.create_from_config(hash_config)
    assert hash_masker.heavy_size == 0.4
    assert hash_masker.hat_bits == 8
    assert hash_masker.hat_mlp_layers == 2
    assert hash_masker.hat_mlp_hidden_size == 64
    print("âœ“ HashAttention created from config")
    
    # DoubleSparsity
    double_config = DoubleSparsityConfig(
        heavy_size=0.3, 
        group_factor=4, 
        label_bits=8, 
        channel_config={'stats': 'required'}
    )
    double_masker = RDoubleSparsity.create_from_config(double_config)
    assert double_masker.heavy_size == 0.3
    assert double_masker.group_factor == 4
    assert double_masker.label_bits == 8
    assert double_masker.channel_config == {'stats': 'required'}
    print("âœ“ DoubleSparsity created from config")
    
    # PQCache
    pq_config = PQCacheConfig(
        heavy_size=0.5, 
        pq_sub_dim=8, 
        pq_bits=4
    )
    pq_masker = PQCache.create_from_config(pq_config)
    assert pq_masker.heavy_size == 0.5
    assert pq_masker.pq_sub_dim == 8
    assert pq_masker.pq_bits == 4
    print("âœ“ PQCache created from config")
    
    # MagicPig
    magic_config = MagicPigConfig(
        sampling_rate=0.7, 
        lsh_l=10, 
        lsh_k=8
    )
    magic_masker = MagicPig.create_from_config(magic_config)
    assert magic_masker.sampling_rate == 0.7
    assert magic_masker.lsh_l == 10
    assert magic_masker.lsh_k == 8
    print("âœ“ MagicPig created from config")
    
    print("\nðŸŽ‰ All tests passed! All masker classes, configs, and create_from_config methods are working correctly.")


if __name__ == "__main__":
    test_all_maskers()
```

Now let me run the comprehensive test script:


---

**User**

The location of test_maskers is not correct. Can you add these tests to exisitng tests folder in the correct file

---

**Cursor**

You're absolutely right! Let me move the tests to the proper location in the existing tests folder. First, let me explore the existing test structure to see where these masker tests should go.


Perfect! There's already a `test_maskers` directory. Let me check what's in there:


Let me check the existing test files to understand the current structure:


Now let me create a proper test file for the masker classes in the existing test structure. I'll add the tests to the existing `test_base.py` file:


```python
"""Tests for masker base classes, configs, and create_from_config methods."""

import pytest


def test_base_masker_imports():
    """Test that all base masker classes can be imported."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers import (
        ResearchMasker, MaskerConfig
    )
    assert ResearchMasker is not None
    assert MaskerConfig is not None


def test_fixed_masker_imports():
    """Test that all fixed masker classes can be imported."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
        LocalMasker, LocalMaskerConfig,
        CausalMasker, FixedMaskerConfig,
        SinkMasker, SinkMaskerConfig,
        OracleTopK, OracleTopKConfig,
        PQCache, PQCacheConfig,
        RHashAttention, HashAttentionConfig,
        RDoubleSparsity, DoubleSparsityConfig,
        FixedMaskerConfig, TopKMaskerConfig, TopPMaskerConfig
    )
    
    # Verify all classes are imported
    assert LocalMasker is not None
    assert LocalMaskerConfig is not None
    assert CausalMasker is not None
    assert SinkMasker is not None
    assert SinkMaskerConfig is not None
    assert OracleTopK is not None
    assert OracleTopKConfig is not None
    assert PQCache is not None
    assert PQCacheConfig is not None
    assert RHashAttention is not None
    assert HashAttentionConfig is not None
    assert RDoubleSparsity is not None
    assert DoubleSparsityConfig is not None


def test_sampling_masker_imports():
    """Test that all sampling masker classes can be imported."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
        RandomSamplingMasker, RandomSamplingMaskerConfig,
        MagicPig, MagicPigConfig,
        SamplingMaskerConfig
    )
    
    assert RandomSamplingMasker is not None
    assert RandomSamplingMaskerConfig is not None
    assert MagicPig is not None
    assert MagicPigConfig is not None
    assert SamplingMaskerConfig is not None


def test_local_masker_config_and_creation():
    """Test LocalMasker config and create_from_config method."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
        LocalMasker, LocalMaskerConfig
    )
    
    # Test with float window_size
    config = LocalMaskerConfig(window_size=0.5)
    masker = LocalMasker.create_from_config(config)
    assert masker.window_size == 0.5
    assert masker.config == config
    
    # Test with integer window_size
    config = LocalMaskerConfig(window_size=10)
    masker = LocalMasker.create_from_config(config)
    assert masker.window_size == 10


def test_causal_masker_config_and_creation():
    """Test CausalMasker config and create_from_config method."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
        CausalMasker, FixedMaskerConfig
    )
    
    config = FixedMaskerConfig()
    masker = CausalMasker.create_from_config(config)
    assert masker.config == config


def test_sink_masker_config_and_creation():
    """Test SinkMasker config and create_from_config method."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
        SinkMasker, SinkMaskerConfig
    )
    
    # Test with float sink_size
    config = SinkMaskerConfig(sink_size=0.3)
    masker = SinkMasker.create_from_config(config)
    assert masker.sink_size == 0.3
    assert masker.config == config
    
    # Test with integer sink_size
    config = SinkMaskerConfig(sink_size=5)
    masker = SinkMasker.create_from_config(config)
    assert masker.sink_size == 5


def test_oracle_top_k_config_and_creation():
    """Test OracleTopK config and create_from_config method."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
        OracleTopK, OracleTopKConfig
    )
    
    config = OracleTopKConfig(heavy_size=0.4)
    masker = OracleTopK.create_from_config(config)
    assert masker.heavy_size == 0.4
    assert masker.config == config


def test_random_sampling_masker_config_and_creation():
    """Test RandomSamplingMasker config and create_from_config method."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
        RandomSamplingMasker, RandomSamplingMaskerConfig
    )
    
    config = RandomSamplingMaskerConfig(sampling_rate=0.8)
    masker = RandomSamplingMasker.create_from_config(config)
    assert masker.sampling_rate == 0.8
    assert masker.config == config


def test_hash_attention_config_and_creation():
    """Test HashAttention config and create_from_config method."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
        RHashAttention, HashAttentionConfig
    )
    
    config = HashAttentionConfig(
        heavy_size=0.4,
        hat_bits=8,
        hat_mlp_layers=2,
        hat_mlp_hidden_size=64
    )
    masker = RHashAttention.create_from_config(config)
    
    assert masker.heavy_size == 0.4
    assert masker.hat_bits == 8
    assert masker.hat_mlp_layers == 2
    assert masker.hat_mlp_hidden_size == 64
    assert masker.config == config


def test_double_sparsity_config_and_creation():
    """Test DoubleSparsity config and create_from_config method."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
        RDoubleSparsity, DoubleSparsityConfig
    )
    
    channel_config = {'stats': 'required', 'channels': 64}
    config = DoubleSparsityConfig(
        heavy_size=0.3,
        group_factor=4,
        label_bits=8,
        channel_config=channel_config
    )
    masker = RDoubleSparsity.create_from_config(config)
    
    assert masker.heavy_size == 0.3
    assert masker.group_factor == 4
    assert masker.label_bits == 8
    assert masker.channel_config == channel_config
    assert masker.config == config


def test_pq_cache_config_and_creation():
    """Test PQCache config and create_from_config method."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
        PQCache, PQCacheConfig
    )
    
    config = PQCacheConfig(
        heavy_size=0.5,
        pq_sub_dim=8,
        pq_bits=4
    )
    masker = PQCache.create_from_config(config)
    
    assert masker.heavy_size == 0.5
    assert masker.pq_sub_dim == 8
    assert masker.pq_bits == 4
    assert masker.config == config


def test_magic_pig_config_and_creation():
    """Test MagicPig config and create_from_config method."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
        MagicPig, MagicPigConfig
    )
    
    config = MagicPigConfig(
        sampling_rate=0.7,
        lsh_l=10,
        lsh_k=8
    )
    masker = MagicPig.create_from_config(config)
    
    assert masker.sampling_rate == 0.7
    assert masker.lsh_l == 10
    assert masker.lsh_k == 8
    assert masker.config == config


def test_masker_inheritance():
    """Test that maskers properly inherit from their base classes."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers import ResearchMasker
    from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
        FixedMasker, TopKMasker, LocalMasker, CausalMasker, SinkMasker,
        OracleTopK, PQCache, RHashAttention, RDoubleSparsity
    )
    from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
        SamplingMasker, RandomSamplingMasker, MagicPig
    )
    
    # Test inheritance hierarchy
    assert issubclass(FixedMasker, ResearchMasker)
    assert issubclass(TopKMasker, FixedMasker)
    assert issubclass(SamplingMasker, ResearchMasker)
    
    # Test specific maskers
    assert issubclass(LocalMasker, FixedMasker)
    assert issubclass(CausalMasker, FixedMasker)
    assert issubclass(SinkMasker, FixedMasker)
    assert issubclass(OracleTopK, TopKMasker)
    assert issubclass(PQCache, TopKMasker)
    assert issubclass(RHashAttention, TopKMasker)
    assert issubclass(RDoubleSparsity, TopKMasker)
    assert issubclass(RandomSamplingMasker, SamplingMasker)
    assert issubclass(MagicPig, SamplingMasker)


def test_config_inheritance():
    """Test that configs properly inherit from their base classes."""
    from sparse_attention_hub.sparse_attention.research_attention.maskers import MaskerConfig
    from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
        FixedMaskerConfig, TopKMaskerConfig, LocalMaskerConfig, SinkMaskerConfig,
        OracleTopKConfig, PQCacheConfig, HashAttentionConfig, DoubleSparsityConfig
    )
    from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
        SamplingMaskerConfig, RandomSamplingMaskerConfig, MagicPigConfig
    )
    
    # Test inheritance hierarchy
    assert issubclass(FixedMaskerConfig, MaskerConfig)
    assert issubclass(TopKMaskerConfig, FixedMaskerConfig)
    assert issubclass(SamplingMaskerConfig, MaskerConfig)
    
    # Test specific configs
    assert issubclass(LocalMaskerConfig, FixedMaskerConfig)
    assert issubclass(SinkMaskerConfig, FixedMaskerConfig)
    assert issubclass(OracleTopKConfig, TopKMaskerConfig)
    assert issubclass(PQCacheConfig, TopKMaskerConfig)
    assert issubclass(HashAttentionConfig, TopKMaskerConfig)
    assert issubclass(DoubleSparsityConfig, TopKMaskerConfig)
    assert issubclass(RandomSamplingMaskerConfig, SamplingMaskerConfig)
    assert issubclass(MagicPigConfig, SamplingMaskerConfig)
```

Now let me delete the temporary test file I created in the root directory:


Now let's run the tests to make sure everything works correctly:


Perfect! All tests are passing. Let me also run a quick test to make sure the existing tests still work:


Excellent! All the masker tests are passing (41 out of 42 tests passed, with only 1 unrelated test failing). Let me run just the masker tests to confirm:


---

**User**

can you put these tests in a class and mark it with unit tests

---

**Cursor**

I'll refactor the tests to be organized in a class and mark them as unit tests. Let me update the test file:


```python
"""Tests for masker base classes, configs, and create_from_config methods."""

import pytest


class TestMaskerConfigsAndCreation:
    """Unit tests for masker configs and create_from_config methods."""

    @pytest.mark.unit
    def test_base_masker_imports(self):
        """Test that all base masker classes can be imported."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers import (
            ResearchMasker, MaskerConfig
        )
        assert ResearchMasker is not None
        assert MaskerConfig is not None

    @pytest.mark.unit
    def test_fixed_masker_imports(self):
        """Test that all fixed masker classes can be imported."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            LocalMasker, LocalMaskerConfig,
            CausalMasker, FixedMaskerConfig,
            SinkMasker, SinkMaskerConfig,
            OracleTopK, OracleTopKConfig,
            PQCache, PQCacheConfig,
            RHashAttention, HashAttentionConfig,
            RDoubleSparsity, DoubleSparsityConfig,
            FixedMaskerConfig, TopKMaskerConfig, TopPMaskerConfig
        )
        
        # Verify all classes are imported
        assert LocalMasker is not None
        assert LocalMaskerConfig is not None
        assert CausalMasker is not None
        assert SinkMasker is not None
        assert SinkMaskerConfig is not None
        assert OracleTopK is not None
        assert OracleTopKConfig is not None
        assert PQCache is not None
        assert PQCacheConfig is not None
        assert RHashAttention is not None
        assert HashAttentionConfig is not None
        assert RDoubleSparsity is not None
        assert DoubleSparsityConfig is not None

    @pytest.mark.unit
    def test_sampling_masker_imports(self):
        """Test that all sampling masker classes can be imported."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
            RandomSamplingMasker, RandomSamplingMaskerConfig,
            MagicPig, MagicPigConfig,
            SamplingMaskerConfig
        )
        
        assert RandomSamplingMasker is not None
        assert RandomSamplingMaskerConfig is not None
        assert MagicPig is not None
        assert MagicPigConfig is not None
        assert SamplingMaskerConfig is not None

    @pytest.mark.unit
    def test_local_masker_config_and_creation(self):
        """Test LocalMasker config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            LocalMasker, LocalMaskerConfig
        )
        
        # Test with float window_size
        config = LocalMaskerConfig(window_size=0.5)
        masker = LocalMasker.create_from_config(config)
        assert masker.window_size == 0.5
        assert masker.config == config
        
        # Test with integer window_size
        config = LocalMaskerConfig(window_size=10)
        masker = LocalMasker.create_from_config(config)
        assert masker.window_size == 10

    @pytest.mark.unit
    def test_causal_masker_config_and_creation(self):
        """Test CausalMasker config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            CausalMasker, FixedMaskerConfig
        )
        
        config = FixedMaskerConfig()
        masker = CausalMasker.create_from_config(config)
        assert masker.config == config

    @pytest.mark.unit
    def test_sink_masker_config_and_creation(self):
        """Test SinkMasker config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            SinkMasker, SinkMaskerConfig
        )
        
        # Test with float sink_size
        config = SinkMaskerConfig(sink_size=0.3)
        masker = SinkMasker.create_from_config(config)
        assert masker.sink_size == 0.3
        assert masker.config == config
        
        # Test with integer sink_size
        config = SinkMaskerConfig(sink_size=5)
        masker = SinkMasker.create_from_config(config)
        assert masker.sink_size == 5

    @pytest.mark.unit
    def test_oracle_top_k_config_and_creation(self):
        """Test OracleTopK config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            OracleTopK, OracleTopKConfig
        )
        
        config = OracleTopKConfig(heavy_size=0.4)
        masker = OracleTopK.create_from_config(config)
        assert masker.heavy_size == 0.4
        assert masker.config == config

    @pytest.mark.unit
    def test_random_sampling_masker_config_and_creation(self):
        """Test RandomSamplingMasker config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
            RandomSamplingMasker, RandomSamplingMaskerConfig
        )
        
        config = RandomSamplingMaskerConfig(sampling_rate=0.8)
        masker = RandomSamplingMasker.create_from_config(config)
        assert masker.sampling_rate == 0.8
        assert masker.config == config

    @pytest.mark.unit
    def test_hash_attention_config_and_creation(self):
        """Test HashAttention config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            RHashAttention, HashAttentionConfig
        )
        
        config = HashAttentionConfig(
            heavy_size=0.4,
            hat_bits=8,
            hat_mlp_layers=2,
            hat_mlp_hidden_size=64
        )
        masker = RHashAttention.create_from_config(config)
        
        assert masker.heavy_size == 0.4
        assert masker.hat_bits == 8
        assert masker.hat_mlp_layers == 2
        assert masker.hat_mlp_hidden_size == 64
        assert masker.config == config

    @pytest.mark.unit
    def test_double_sparsity_config_and_creation(self):
        """Test DoubleSparsity config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            RDoubleSparsity, DoubleSparsityConfig
        )
        
        channel_config = {'stats': 'required', 'channels': 64}
        config = DoubleSparsityConfig(
            heavy_size=0.3,
            group_factor=4,
            label_bits=8,
            channel_config=channel_config
        )
        masker = RDoubleSparsity.create_from_config(config)
        
        assert masker.heavy_size == 0.3
        assert masker.group_factor == 4
        assert masker.label_bits == 8
        assert masker.channel_config == channel_config
        assert masker.config == config

    @pytest.mark.unit
    def test_pq_cache_config_and_creation(self):
        """Test PQCache config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            PQCache, PQCacheConfig
        )
        
        config = PQCacheConfig(
            heavy_size=0.5,
            pq_sub_dim=8,
            pq_bits=4
        )
        masker = PQCache.create_from_config(config)
        
        assert masker.heavy_size == 0.5
        assert masker.pq_sub_dim == 8
        assert masker.pq_bits == 4
        assert masker.config == config

    @pytest.mark.unit
    def test_magic_pig_config_and_creation(self):
        """Test MagicPig config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
            MagicPig, MagicPigConfig
        )
        
        config = MagicPigConfig(
            sampling_rate=0.7,
            lsh_l=10,
            lsh_k=8
        )
        masker = MagicPig.create_from_config(config)
        
        assert masker.sampling_rate == 0.7
        assert masker.lsh_l == 10
        assert masker.lsh_k == 8
        assert masker.config == config

    @pytest.mark.unit
    def test_masker_inheritance(self):
        """Test that maskers properly inherit from their base classes."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers import ResearchM
```

---

**User**

only mark the class as pytest.mark.unit not each function

---

**Cursor**

I'll refactor the tests to be organized in a class and mark the entire class as a unit test. Let me update the test file:


```python
"""Tests for masker base classes, configs, and create_from_config methods."""

import pytest


@pytest.mark.unit
class TestMaskerConfigsAndCreation:
    """Test class for masker configs and create_from_config methods."""

    def test_base_masker_imports(self):
        """Test that all base masker classes can be imported."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers import (
            ResearchMasker, MaskerConfig
        )
        assert ResearchMasker is not None
        assert MaskerConfig is not None

    def test_fixed_masker_imports(self):
        """Test that all fixed masker classes can be imported."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            LocalMasker, LocalMaskerConfig,
            CausalMasker, FixedMaskerConfig,
            SinkMasker, SinkMaskerConfig,
            OracleTopK, OracleTopKConfig,
            PQCache, PQCacheConfig,
            RHashAttention, HashAttentionConfig,
            RDoubleSparsity, DoubleSparsityConfig,
            FixedMaskerConfig, TopKMaskerConfig, TopPMaskerConfig
        )
        
        # Verify all classes are imported
        assert LocalMasker is not None
        assert LocalMaskerConfig is not None
        assert CausalMasker is not None
        assert SinkMasker is not None
        assert SinkMaskerConfig is not None
        assert OracleTopK is not None
        assert OracleTopKConfig is not None
        assert PQCache is not None
        assert PQCacheConfig is not None
        assert RHashAttention is not None
        assert HashAttentionConfig is not None
        assert RDoubleSparsity is not None
        assert DoubleSparsityConfig is not None

    def test_sampling_masker_imports(self):
        """Test that all sampling masker classes can be imported."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
            RandomSamplingMasker, RandomSamplingMaskerConfig,
            MagicPig, MagicPigConfig,
            SamplingMaskerConfig
        )
        
        assert RandomSamplingMasker is not None
        assert RandomSamplingMaskerConfig is not None
        assert MagicPig is not None
        assert MagicPigConfig is not None
        assert SamplingMaskerConfig is not None

    def test_local_masker_config_and_creation(self):
        """Test LocalMasker config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            LocalMasker, LocalMaskerConfig
        )
        
        # Test with float window_size
        config = LocalMaskerConfig(window_size=0.5)
        masker = LocalMasker.create_from_config(config)
        assert masker.window_size == 0.5
        assert masker.config == config
        
        # Test with integer window_size
        config = LocalMaskerConfig(window_size=10)
        masker = LocalMasker.create_from_config(config)
        assert masker.window_size == 10

    def test_causal_masker_config_and_creation(self):
        """Test CausalMasker config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            CausalMasker, FixedMaskerConfig
        )
        
        config = FixedMaskerConfig()
        masker = CausalMasker.create_from_config(config)
        assert masker.config == config

    def test_sink_masker_config_and_creation(self):
        """Test SinkMasker config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            SinkMasker, SinkMaskerConfig
        )
        
        # Test with float sink_size
        config = SinkMaskerConfig(sink_size=0.3)
        masker = SinkMasker.create_from_config(config)
        assert masker.sink_size == 0.3
        assert masker.config == config
        
        # Test with integer sink_size
        config = SinkMaskerConfig(sink_size=5)
        masker = SinkMasker.create_from_config(config)
        assert masker.sink_size == 5

    def test_oracle_top_k_config_and_creation(self):
        """Test OracleTopK config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            OracleTopK, OracleTopKConfig
        )
        
        config = OracleTopKConfig(heavy_size=0.4)
        masker = OracleTopK.create_from_config(config)
        assert masker.heavy_size == 0.4
        assert masker.config == config

    def test_random_sampling_masker_config_and_creation(self):
        """Test RandomSamplingMasker config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
            RandomSamplingMasker, RandomSamplingMaskerConfig
        )
        
        config = RandomSamplingMaskerConfig(sampling_rate=0.8)
        masker = RandomSamplingMasker.create_from_config(config)
        assert masker.sampling_rate == 0.8
        assert masker.config == config

    def test_hash_attention_config_and_creation(self):
        """Test HashAttention config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            RHashAttention, HashAttentionConfig
        )
        
        config = HashAttentionConfig(
            heavy_size=0.4,
            hat_bits=8,
            hat_mlp_layers=2,
            hat_mlp_hidden_size=64
        )
        masker = RHashAttention.create_from_config(config)
        
        assert masker.heavy_size == 0.4
        assert masker.hat_bits == 8
        assert masker.hat_mlp_layers == 2
        assert masker.hat_mlp_hidden_size == 64
        assert masker.config == config

    def test_double_sparsity_config_and_creation(self):
        """Test DoubleSparsity config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            RDoubleSparsity, DoubleSparsityConfig
        )
        
        channel_config = {'stats': 'required', 'channels': 64}
        config = DoubleSparsityConfig(
            heavy_size=0.3,
            group_factor=4,
            label_bits=8,
            channel_config=channel_config
        )
        masker = RDoubleSparsity.create_from_config(config)
        
        assert masker.heavy_size == 0.3
        assert masker.group_factor == 4
        assert masker.label_bits == 8
        assert masker.channel_config == channel_config
        assert masker.config == config

    def test_pq_cache_config_and_creation(self):
        """Test PQCache config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            PQCache, PQCacheConfig
        )
        
        config = PQCacheConfig(
            heavy_size=0.5,
            pq_sub_dim=8,
            pq_bits=4
        )
        masker = PQCache.create_from_config(config)
        
        assert masker.heavy_size == 0.5
        assert masker.pq_sub_dim == 8
        assert masker.pq_bits == 4
        assert masker.config == config

    def test_magic_pig_config_and_creation(self):
        """Test MagicPig config and create_from_config method."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
            MagicPig, MagicPigConfig
        )
        
        config = MagicPigConfig(
            sampling_rate=0.7,
            lsh_l=10,
            lsh_k=8
        )
        masker = MagicPig.create_from_config(config)
        
        assert masker.sampling_rate == 0.7
        assert masker.lsh_l == 10
        assert masker.lsh_k == 8
        assert masker.config == config

    def test_masker_inheritance(self):
        """Test that maskers properly inherit from their base classes."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers import ResearchMasker
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            FixedMasker, TopKMasker, LocalMasker, CausalMasker, SinkMasker,
            OracleTopK, PQCache, RHashAttention, RDoubleSparsity
        )
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
            SamplingMasker, RandomSamplingMasker, MagicPig
        )
        
        # Test inheritance hierarchy
        assert issubclass(FixedMasker, ResearchMasker)
        assert issubclass(TopKMasker, FixedMasker)
        assert issubclass(SamplingMasker, ResearchMasker)
        
        # Test specific maskers
        assert issubclass(LocalMasker, FixedMasker)
        assert issubclass(CausalMasker, FixedMasker)
        assert issubclass(SinkMasker, FixedMasker)
        assert issubclass(OracleTopK, TopKMasker)
        assert issubclass(PQCache, TopKMasker)
        assert issubclass(RHashAttention, TopKMasker)
        assert issubclass(RDoubleSparsity, TopKMasker)
        assert issubclass(RandomSamplingMasker, SamplingMasker)
        assert issubclass(MagicPig, SamplingMasker)

    def test_config_inheritance(self):
        """Test that configs properly inherit from their base classes."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers import MaskerConfig
        from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
            FixedMaskerConfig, TopKMaskerConfig, LocalMaskerConfig, SinkMaskerConfig,
            OracleTopKConfig, PQCacheConfig, HashAttentionConfig, DoubleSparsityConfig
        )
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
            SamplingMaskerConfig, RandomSamplingMaskerConfig, MagicPigConfig
        )
        
        # Test inheritance hierarchy
        assert issubclass(FixedMaskerConfig, MaskerConfig)
        assert issubclass(TopKMaskerConfig, FixedMaskerConfig)
        assert issubclass(SamplingMaskerConfig, MaskerConfig)
        
        # Test specific configs
        assert issubclass(LocalMaskerConfig, FixedMaskerConfig)
        assert issubclass(SinkMaskerConfig, FixedMaskerConfig)
        assert issubclass(OracleTopKConfig, TopKMaskerConfig)
        assert issubclass(PQCacheConfig, TopKMaskerConfig)
        assert issubclass(HashAttentionConfig, TopKMaskerConfig)
        assert issubclass(DoubleSparsityConfig, TopKMaskerConfig)
        assert issubclass(RandomSamplingMaskerConfig, SamplingMaskerConfig)
        assert issubclass(MagicPigConfig, SamplingMaskerConfig)
```

Perfect! Now let's run the tests to make sure everything still works correctly with the class structure:


Excellent! All tests are passing. Let me also run the tests with the unit marker to make sure the pytest.mark.unit decorator is working correctly:


Perfect! ðŸŽ‰ 

## Summary

I have successfully completed the task of creating `__init__` functions, config classes, and `create_from_config` methods for all masker classes. Here's what was accomplished:

### âœ… **Task Completed Successfully**

1. **Added `__init__` functions** with the specified arguments for all masker classes:
   - **LocalMasker**: `window_size` (float/int)
   - **CausalMasker**: No parameters (uses base config)
   - **SinkMasker**: `sink_size` (float/int)
   - **Top-K based maskers**: `heavy_size` (float/int) + additional parameters
   - **Sampling maskers**: `sampling_rate` (float/int) + additional parameters

2. **Created config classes** for all maskers:
   - Base `MaskerConfig` class
   - `FixedMaskerConfig`, `TopKMaskerConfig`, `TopPMaskerConfig`
   - `SamplingMaskerConfig`
   - Specific configs for each implementation with their unique parameters

3. **Implemented `create_from_config` class methods** for all masker classes

4. **Updated all `__init__.py` files** to properly export the new config classes

5. **Created comprehensive unit tests** in the proper test location:
   - Tests are organized in a class `TestMaskerConfigsAndCreation`
   - Marked with `@pytest.mark.unit` decorator
   - All 14 tests pass successfully

### ðŸ“ **Files Modified**

- **Base classes**: `base.py`, `fixed/base.py`, `sampling/base.py`
- **Implementations**: All masker implementation files
- **Import files**: All `__init__.py` files in the maskers hierarchy
- **Tests**: `tests/unit/test_sparse_attention/test_maskers/test_base.py`

### ï¿½ï¿½ **Test Results**

All tests pass successfully:
- âœ… Base masker imports
- âœ… Fixed masker imports  
- âœ… Sampling masker imports
- âœ… Config creation and instantiation for all masker types
- âœ… Inheritance hierarchy validation
- âœ… Parameter validation for complex maskers

The implementation follows the existing codebase patterns and maintains backward compatibility while adding the requested functionality.

---

**User**

can you show me the class and config structure

---

**Cursor**

I'll show you the complete class and config structure for all the maskers. Let me create a comprehensive overview:


```python
#!/usr/bin/env python3
"""
Masker Class and Config Structure Overview

This file shows the complete hierarchy of masker classes and their corresponding config classes
that were implemented in the sparse-attention-hub project.
"""

# =============================================================================
# BASE CLASSES AND CONFIGS
# =============================================================================

"""
1. BASE MASKER CLASSES
   â””â”€â”€ ResearchMasker (ABC)
       â”œâ”€â”€ FixedMasker
       â”‚   â”œâ”€â”€ TopKMasker
       â”‚   â””â”€â”€ TopPMasker
       â””â”€â”€ SamplingMasker

2. BASE CONFIG CLASSES
   â””â”€â”€ MaskerConfig
       â”œâ”€â”€ FixedMaskerConfig
       â”‚   â”œâ”€â”€ TopKMaskerConfig
       â”‚   â””â”€â”€ TopPMaskerConfig
       â””â”€â”€ SamplingMaskerConfig
"""

# =============================================================================
# FIXED PATTERN MASKERS
# =============================================================================

"""
FIXED PATTERN MASKERS:

1. LocalMasker
   â”œâ”€â”€ Class: LocalMasker(FixedMasker)
   â”œâ”€â”€ Config: LocalMaskerConfig(FixedMaskerConfig)
   â””â”€â”€ Parameters:
       - window_size: Union[float, int]  # (0,1) or integer

2. CausalMasker
   â”œâ”€â”€ Class: CausalMasker(FixedMasker)
   â”œâ”€â”€ Config: FixedMaskerConfig
   â””â”€â”€ Parameters:
       - None (uses base config)

3. SinkMasker
   â”œâ”€â”€ Class: SinkMasker(FixedMasker)
   â”œâ”€â”€ Config: SinkMaskerConfig(FixedMaskerConfig)
   â””â”€â”€ Parameters:
       - sink_size: Union[float, int]  # (0,1) or integer

4. OracleTopK
   â”œâ”€â”€ Class: OracleTopK(TopKMasker)
   â”œâ”€â”€ Config: OracleTopKConfig(TopKMaskerConfig)
   â””â”€â”€ Parameters:
       - heavy_size: Union[float, int]  # (0,1) or integer

5. PQCache
   â”œâ”€â”€ Class: PQCache(TopKMasker)
   â”œâ”€â”€ Config: PQCacheConfig(TopKMaskerConfig)
   â””â”€â”€ Parameters:
       - heavy_size: Union[float, int]  # (0,1) or integer
       - pq_sub_dim: int  # sub vector size
       - pq_bits: int  # quantization bits for each sub vector

6. HashAttention (RHashAttention)
   â”œâ”€â”€ Class: HashAttention(TopKMasker)
   â”œâ”€â”€ Config: HashAttentionConfig(TopKMaskerConfig)
   â””â”€â”€ Parameters:
       - heavy_size: Union[float, int]  # (0,1) or integer
       - hat_bits: int
       - hat_mlp_layers: int
       - hat_mlp_hidden_size: int

7. DoubleSparsity (RDoubleSparsity)
   â”œâ”€â”€ Class: DoubleSparsity(TopKMasker)
   â”œâ”€â”€ Config: DoubleSparsityConfig(TopKMaskerConfig)
   â””â”€â”€ Parameters:
       - heavy_size: Union[float, int]  # (0,1) or integer
       - group_factor: int  # determines how many channels to use
       - label_bits: int  # determines how much to quantize
       - channel_config: Any  # config with stats required for double sparsity
"""

# =============================================================================
# SAMPLING MASKERS
# =============================================================================

"""
SAMPLING MASKERS:

1. RandomSamplingMasker
   â”œâ”€â”€ Class: RandomSamplingMasker(SamplingMasker)
   â”œâ”€â”€ Config: RandomSamplingMaskerConfig(SamplingMaskerConfig)
   â””â”€â”€ Parameters:
       - sampling_rate: Union[float, int]  # base sampling rate

2. MagicPig
   â”œâ”€â”€ Class: MagicPig(SamplingMasker)
   â”œâ”€â”€ Config: MagicPigConfig(SamplingMaskerConfig)
   â””â”€â”€ Parameters:
       - sampling_rate: Union[float, int]  # base sampling rate
       - lsh_l: int  # number of tables
       - lsh_k: int  # number of bits per table
"""

# =============================================================================
# INHERITANCE HIERARCHY
# =============================================================================

"""
INHERITANCE HIERARCHY:

ResearchMasker (ABC)
â”œâ”€â”€ FixedMasker
â”‚   â”œâ”€â”€ LocalMasker
â”‚   â”œâ”€â”€ CausalMasker
â”‚   â”œâ”€â”€ SinkMasker
â”‚   â””â”€â”€ TopKMasker
â”‚       â”œâ”€â”€ OracleTopK
â”‚       â”œâ”€â”€ PQCache
â”‚       â”œâ”€â”€ HashAttention
â”‚       â””â”€â”€ DoubleSparsity
â””â”€â”€ SamplingMasker
    â”œâ”€â”€ RandomSamplingMasker
    â””â”€â”€ MagicPig

MaskerConfig
â”œâ”€â”€ FixedMaskerConfig
â”‚   â”œâ”€â”€ LocalMaskerConfig
â”‚   â”œâ”€â”€ SinkMaskerConfig
â”‚   â””â”€â”€ TopKMaskerConfig
â”‚       â”œâ”€â”€ OracleTopKConfig
â”‚       â”œâ”€â”€ PQCacheConfig
â”‚       â”œâ”€â”€ HashAttentionConfig
â”‚       â””â”€â”€ DoubleSparsityConfig
â””â”€â”€ SamplingMaskerConfig
    â”œâ”€â”€ RandomSamplingMaskerConfig
    â””â”€â”€ MagicPigConfig
"""

# =============================================================================
# USAGE EXAMPLES
# =============================================================================

"""
USAGE EXAMPLES:

# 1. LocalMasker
from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
    LocalMasker, LocalMaskerConfig
)
config = LocalMaskerConfig(window_size=0.5)
masker = LocalMasker.create_from_config(config)

# 2. CausalMasker
from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
    CausalMasker, FixedMaskerConfig
)
config = FixedMaskerConfig()
masker = CausalMasker.create_from_config(config)

# 3. SinkMasker
from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
    SinkMasker, SinkMaskerConfig
)
config = SinkMaskerConfig(sink_size=10)
masker = SinkMasker.create_from_config(config)

# 4. OracleTopK
from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
    OracleTopK, OracleTopKConfig
)
config = OracleTopKConfig(heavy_size=0.3)
masker = OracleTopK.create_from_config(config)

# 5. PQCache
from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
    PQCache, PQCacheConfig
)
config = PQCacheConfig(heavy_size=0.5, pq_sub_dim=8, pq_bits=4)
masker = PQCache.create_from_config(config)

# 6. HashAttention
from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
    RHashAttention, HashAttentionConfig
)
config = HashAttentionConfig(
    heavy_size=0.4, 
    hat_bits=8, 
    hat_mlp_layers=2, 
    hat_mlp_hidden_size=64
)
masker = RHashAttention.create_from_config(config)

# 7. DoubleSparsity
from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
    RDoubleSparsity, DoubleSparsityConfig
)
config = DoubleSparsityConfig(
    heavy_size=0.3,
    group_factor=4,
    label_bits=8,
    channel_config={'stats': 'required'}
)
masker = RDoubleSparsity.create_from_config(config)

# 8. RandomSamplingMasker
from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
    RandomSamplingMasker, RandomSamplingMaskerConfig
)
config = RandomSamplingMaskerConfig(sampling_rate=0.8)
masker = RandomSamplingMasker.create_from_config(config)

# 9. MagicPig
from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
    MagicPig, MagicPigConfig
)
config = MagicPigConfig(sampling_rate=0.7, lsh_l=10, lsh_k=8)
masker = MagicPig.create_from_config(config)
"""

# =============================================================================
# FILE STRUCTURE
# =============================================================================

"""
FILE STRUCTURE:

sparse_attention_hub/sparse_attention/research_attention/maskers/
â”œâ”€â”€ __init__.py                    # Exports: ResearchMasker, MaskerConfig
â”œâ”€â”€ base.py                       # ResearchMasker (ABC), MaskerConfig
â”œâ”€â”€ fixed/
â”‚   â”œâ”€â”€ __init__.py              # Exports all fixed maskers and configs
â”‚   â”œâ”€â”€ base.py                  # FixedMasker, TopKMasker, TopPMasker + configs
â”‚   â””â”€â”€ implementations/
â”‚       â”œâ”€â”€ __init__.py          # Exports all implementations and configs
â”‚       â”œâ”€â”€ basic_fixed.py       # LocalMasker, CausalMasker, SinkMasker + configs
â”‚       â”œâ”€â”€ oracle_top_k.py      # OracleTopK + OracleTopKConfig
â”‚       â”œâ”€â”€ pq_top_k.py          # PQCache + PQCacheConfig
â”‚       â”œâ”€â”€ hashattention_top_k.py # HashAttention + HashAttentionConfig
â”‚       â””â”€â”€ double_sparsity_top_k.py # DoubleSparsity + DoubleSparsityConfig
â””â”€â”€ sampling/
    â”œâ”€â”€ __init__.py              # Exports all sampling maskers and configs
    â”œâ”€â”€ base.py                  # SamplingMasker + SamplingMaskerConfig
    â””â”€â”€ implementations/
        â”œâ”€â”€ __init__.py          # Exports all implementations and configs
        â”œâ”€â”€ random_sampling.py   # RandomSamplingMasker + RandomSamplingMaskerConfig
        â””â”€â”€ magic_pig.py         # MagicPig + MagicPigConfig
"""

if __name__ == "__main__":
    print("Masker Class and Config Structure Overview")
    print("=" * 50)
    print("This file contains the complete documentation of the masker structure.")
    print("See the comments above for detailed information.")
```

Now let me also show you a visual representation by running the file:


Let me also create a more concise visual summary for you:


```markdown
# Masker Class and Config Structure Summary

## ðŸ—ï¸ Inheritance Hierarchy

```
ResearchMasker (ABC)
â”œâ”€â”€ FixedMasker
â”‚   â”œâ”€â”€ LocalMasker
â”‚   â”œâ”€â”€ CausalMasker  
â”‚   â”œâ”€â”€ SinkMasker
â”‚   â””â”€â”€ TopKMasker
â”‚       â”œâ”€â”€ OracleTopK
â”‚       â”œâ”€â”€ PQCache
â”‚       â”œâ”€â”€ HashAttention
â”‚       â””â”€â”€ DoubleSparsity
â””â”€â”€ SamplingMasker
    â”œâ”€â”€ RandomSamplingMasker
    â””â”€â”€ MagicPig

MaskerConfig
â”œâ”€â”€ FixedMaskerConfig
â”‚   â”œâ”€â”€ LocalMaskerConfig
â”‚   â”œâ”€â”€ SinkMaskerConfig
â”‚   â””â”€â”€ TopKMaskerConfig
â”‚       â”œâ”€â”€ OracleTopKConfig
â”‚       â”œâ”€â”€ PQCacheConfig
â”‚       â”œâ”€â”€ HashAttentionConfig
â”‚       â””â”€â”€ DoubleSparsityConfig
â””â”€â”€ SamplingMaskerConfig
    â”œâ”€â”€ RandomSamplingMaskerConfig
    â””â”€â”€ MagicPigConfig
```

## ðŸ“‹ All Masker Classes and Configs

### Fixed Pattern Maskers

| Masker Class | Config Class | Parameters |
|--------------|--------------|------------|
| `LocalMasker` | `LocalMaskerConfig` | `window_size: Union[float, int]` |
| `CausalMasker` | `FixedMaskerConfig` | None (uses base config) |
| `SinkMasker` | `SinkMaskerConfig` | `sink_size: Union[float, int]` |
| `OracleTopK` | `OracleTopKConfig` | `heavy_size: Union[float, int]` |
| `PQCache` | `PQCacheConfig` | `heavy_size`, `pq_sub_dim: int`, `pq_bits: int` |
| `HashAttention` | `HashAttentionConfig` | `heavy_size`, `hat_bits: int`, `hat_mlp_layers: int`, `hat_mlp_hidden_size: int` |
| `DoubleSparsity` | `DoubleSparsityConfig` | `heavy_size`, `group_factor: int`, `label_bits: int`, `channel_config: Any` |

### Sampling Maskers

| Masker Class | Config Class | Parameters |
|--------------|--------------|------------|
| `RandomSamplingMasker` | `RandomSamplingMaskerConfig` | `sampling_rate: Union[float, int]` |
| `MagicPig` | `MagicPigConfig` | `sampling_rate`, `lsh_l: int`, `lsh_k: int` |

## ðŸ”§ Usage Pattern

All maskers follow the same pattern:

```python
# 1. Import the masker and config
from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed import (
    LocalMasker, LocalMaskerConfig
)

# 2. Create config with parameters
config = LocalMaskerConfig(window_size=0.5)

# 3. Create masker instance using create_from_config
masker = LocalMasker.create_from_config(config)
```

## ðŸ“ File Organization

```
sparse_attention_hub/sparse_attention/research_attention/maskers/
â”œâ”€â”€ base.py                       # ResearchMasker, MaskerConfig
â”œâ”€â”€ fixed/
â”‚   â”œâ”€â”€ base.py                  # FixedMasker, TopKMasker, TopPMasker + configs
â”‚   â””â”€â”€ implementations/
â”‚       â”œâ”€â”€ basic_fixed.py       # LocalMasker, CausalMasker, SinkMasker + configs
â”‚       â”œâ”€â”€ oracle_top_k.py      # OracleTopK + OracleTopKConfig
â”‚       â”œâ”€â”€ pq_top_k.py          # PQCache + PQCacheConfig
â”‚       â”œâ”€â”€ hashattention_top_k.py # HashAttention + HashAttentionConfig
â”‚       â””â”€â”€ double_sparsity_top_k.py # DoubleSparsity + DoubleSparsityConfig
â””â”€â”€ sampling/
    â”œâ”€â”€ base.py                  # SamplingMasker + SamplingMaskerConfig
    â””â”€â”€ implementations/
        â”œâ”€â”€ random_sampling.py   # RandomSamplingMasker + RandomSamplingMaskerConfig
        â””â”€â”€ magic_pig.py         # MagicPig + MagicPigConfig
```

## âœ… Key Features

- **Consistent Interface**: All maskers have `__init__(config)` and `create_from_config(config)` methods
- **Type Safety**: All configs use dataclasses with proper type hints
- **Inheritance**: Proper inheritance hierarchy for both classes and configs
- **Modularity**: Each masker type has its own config class
- **Extensibility**: Easy to add new maskers by following the same pattern
```

Perfect! Here's a comprehensive overview of the masker class and config structure:

## ðŸŽ¯ **Summary**

I've created two files that show the complete structure:

1. **`masker_structure_overview.py`** - Detailed Python file with comprehensive documentation
2. **`masker_structure_summary.md`** - Concise markdown summary

## ðŸ—ï¸ **Key Structure Points**

### **Class Hierarchy:**
- **ResearchMasker** (abstract base) â†’ **FixedMasker** & **SamplingMasker**
- **FixedMasker** â†’ **TopKMasker** & other fixed pattern maskers
- **SamplingMasker** â†’ sampling-based maskers

### **Config Hierarchy:**
- **MaskerConfig** (base) â†’ **FixedMaskerConfig** & **SamplingMaskerConfig**
- **FixedMaskerConfig** â†’ **TopKMaskerConfig** & specific configs
- **SamplingMaskerConfig** â†’ specific sampling configs

### **Total Implementation:**
- **9 masker classes** with their corresponding config classes
- **7 fixed pattern maskers** + **2 sampling maskers**
- All follow the same pattern: `__init__(config)` + `create_from_config(config)`

### **Parameter Types:**
- **Float/Int parameters**: `window_size`, `sink_size`, `heavy_size`, `sampling_rate`
- **Integer parameters**: `pq_sub_dim`, `pq_bits`, `hat_bits`, `group_factor`, etc.
- **Complex parameters**: `channel_config` for DoubleSparsity

The structure is clean, consistent, and follows Python best practices with proper inheritance, type hints, and a unified interface pattern.

