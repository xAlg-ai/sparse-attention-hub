# Create a mock benchmark structure
_Exported on 7/15/2025 at 13:58:50 PDT from Cursor (1.2.4)_

---

**User**

understand the benchmark structure.
Look at the  longbench benchmark and create a similar small mock benchmark which has only 5 samples , context field has text which are less than 250 words  and questions just ask a simple question about it. make sure that 2 of the 5 samples share the same context.

---

**Cursor**

I'll help you understand the benchmark structure and create a mock benchmark. Let me first examine the longbench benchmark implementation and then create a similar small mock benchmark according to your specifications.

```python
# SPDX-FileCopyrightText: Copyright (c) 1993-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

"""
Mock benchmark module for testing and demonstration purposes.
"""

from .mock_benchmark import MockBenchmark

__all__ = ["MockBenchmark"]
```

```python
# SPDX-FileCopyrightText: Copyright (c) 1993-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

"""Mock benchmark implementation for testing and demonstration purposes."""

from typing import Dict, Any, List
import pandas as pd

from ..base import Benchmark


class MockBenchmark(Benchmark):
    """Mock benchmark for testing and demonstration purposes.

    This benchmark contains 5 simple samples with short contexts (<250 words)
    and basic comprehension questions. Two samples share the same context to
    test context grouping functionality.

    Example:
        >>> mock_benchmark = MockBenchmark()
        >>> results = mock_benchmark.run_benchmark(adapter, result_dir="/path/to/results")
        >>> print(f"Accuracy: {results['accuracy']}")
    """

    # Class attributes required by base Benchmark class
    all_datasets: List[str] = ["reading_comprehension"]
    benchmark_name: str = "mock_benchmark"
    huggingface_dataset_id: str = "mock/dataset"  # Not actually used since we override _load_datasets

    def _load_datasets(self) -> pd.DataFrame:
        """Load mock dataset with hardcoded samples.
        
        Returns:
            DataFrame containing 5 mock samples with context, question, and answers.
        """
        # Sample contexts - all under 250 words
        contexts: Dict[str, str] = {
            "science_context": """
                Photosynthesis is the process by which green plants and some other organisms 
                use sunlight to synthesize foods from carbon dioxide and water. Photosynthesis 
                in plants generally involves the green pigment chlorophyll and generates oxygen 
                as a byproduct. The process occurs in two main stages: the light-dependent 
                reactions and the light-independent reactions (Calvin cycle). During the 
                light-dependent reactions, chlorophyll absorbs light energy and converts it 
                into chemical energy in the form of ATP and NADPH. These energy carriers are 
                then used in the Calvin cycle to convert carbon dioxide into glucose. This 
                process is essential for life on Earth as it produces the oxygen we breathe 
                and forms the base of most food chains. Plants typically perform photosynthesis 
                in their leaves, where specialized organelles called chloroplasts contain the 
                chlorophyll necessary for the process.
            """.strip(),
            
            "history_context": """
                The Renaissance was a period of cultural, artistic, and intellectual revival 
                that began in Italy during the 14th century and spread throughout Europe. 
                It marked the transition from medieval to modern times and was characterized 
                by renewed interest in classical Greek and Roman culture. Key figures of the 
                Renaissance included Leonardo da Vinci, who was both an artist and inventor; 
                Michelangelo, famous for his sculptures and paintings; and Galileo Galilei, 
                who made significant contributions to astronomy and physics. The period saw 
                major developments in art, literature, science, and philosophy. The invention 
                of the printing press by Johannes Gutenberg around 1440 revolutionized the 
                spread of knowledge and ideas. Renaissance art was characterized by realistic 
                portrayals, perspective, and detailed studies of human anatomy. This period 
                laid the foundation for the Scientific Revolution and the Enlightenment that 
                would follow.
            """.strip(),
            
            "geography_context": """
                The Amazon rainforest, often called the "lungs of the Earth," is the world's 
                largest tropical rainforest. Located primarily in Brazil, it also extends 
                into Peru, Colombia, Venezuela, Ecuador, Bolivia, Guyana, Suriname, and 
                French Guiana. The Amazon covers approximately 5.5 million square kilometers 
                and contains about 10% of the world's known biodiversity. The forest plays 
                a crucial role in regulating the global climate by absorbing large amounts 
                of carbon dioxide and producing oxygen. It is home to thousands of species 
                of plants, animals, and indigenous communities. The Amazon River, which flows 
                through the rainforest, is the longest river in the world and has the largest 
                drainage basin. Unfortunately, the rainforest faces threats from deforestation, 
                mining, and climate change, making its conservation critical for the health 
                of our planet.
            """.strip()
        }

        # Create sample data - 5 samples total, 2 sharing the same context
        sample_data: List[Dict[str, Any]] = [
            {
                "context": contexts["science_context"],
                "question": "What are the two main stages of photosynthesis?",
                "answers": ["light-dependent reactions and light-independent reactions", 
                          "light-dependent reactions and Calvin cycle"],
                "task": "reading_comprehension"
            },
            {
                "context": contexts["science_context"],  # Same context as sample 1
                "question": "What gas is produced as a byproduct of photosynthesis?",
                "answers": ["oxygen"],
                "task": "reading_comprehension"
            },
            {
                "context": contexts["history_context"],
                "question": "In which century did the Renaissance begin?",
                "answers": ["14th century", "14th"],
                "task": "reading_comprehension"
            },
            {
                "context": contexts["geography_context"],
                "question": "Why is the Amazon rainforest called the 'lungs of the Earth'?",
                "answers": ["because it absorbs carbon dioxide and produces oxygen", 
                          "it regulates global climate by absorbing CO2 and producing oxygen"],
                "task": "reading_comprehension"
            },
            {
                "context": contexts["geography_context"],
                "question": "Which river flows through the Amazon rainforest?",
                "answers": ["Amazon River", "the Amazon River"],
                "task": "reading_comprehension"
            }
        ]

        # Convert to DataFrame
        df: pd.DataFrame = pd.DataFrame(sample_data)
        
        # Add sample IDs for tracking
        df["sample_id"] = range(1, len(df) + 1)
        
        print(f"Loaded {len(df)} mock samples")
        print(f"Unique contexts: {df['context'].nunique()}")
        
        return df

    def post_run_evaluate(self, results_df: pd.DataFrame) -> Dict[str, Any]:
        """Compute evaluation metrics for mock benchmark results.

        Uses simple exact match and substring matching for evaluation.

        Args:
            results_df: DataFrame containing benchmark results with columns:
                - context: The input context
                - question: The input question
                - predicted_answer: Model's predicted answer
                - answers: Ground truth answers (list)
                - task: Task name
                - sample_id: Sample identifier

        Returns:
            Dictionary containing computed metrics including accuracy and per-sample scores.
        """
        if len(results_df) == 0:
            return {"error": "No results to evaluate"}

        total_samples: int = len(results_df)
        correct_predictions: int = 0
        sample_scores: List[Dict[str, Any]] = []

        for _, row in results_df.iterrows():
            predicted_answer: str = str(row["predicted_answer"]).strip().lower()
            ground_truth_answers: List[str] = row["answers"]
            sample_id: int = row["sample_id"]
            
            # Check if prediction matches any ground truth answer
            is_correct: bool = False
            for gt_answer in ground_truth_answers:
                gt_answer_normalized: str = str(gt_answer).strip().lower()
                
                # Check exact match or substring match
                if (predicted_answer == gt_answer_normalized or 
                    gt_answer_normalized in predicted_answer):
                    is_correct = True
                    break
            
            if is_correct:
                correct_predictions += 1
            
            sample_scores.append({
                "sample_id": sample_id,
                "question": row["question"],
                "predicted_answer": row["predicted_answer"],
                "ground_truth": ground_truth_answers,
                "correct": is_correct
            })

        # Calculate metrics
        accuracy: float = correct_predictions / total_samples if total_samples > 0 else 0.0
        
        metrics: Dict[str, Any] = {
            "accuracy": round(accuracy, 3),
            "correct_predictions": correct_predictions,
            "total_samples": total_samples,
            "sample_scores": sample_scores,
            "summary": {
                "benchmark": self.benchmark_name,
                "task": "reading_comprehension",
                "unique_contexts": results_df["context"].nunique(),
                "evaluation_method": "exact_match_and_substring"
            }
        }

        return metrics
```

```python
# Import concrete benchmark implementations
from .longbench import LongBench

# Import all benchmark submodules to make them available for import
from . import AIME2024
from . import AIME2025
from . import infinite_bench
from . import longbench
from . import longbenchv2
from . import loogle
from . import ruler
from . import zero_scrolls

__all__ = [
    "Benchmark",
    "LongBench",
    "AIME2024",
    "AIME2025", 
    "infinite_bench",
    "longbench",
    "longbenchv2",
    "loogle",
    "ruler",
    "zero_scrolls"
]

->

# Import concrete benchmark implementations
from .longbench import LongBench
from .mock_benchmark import MockBenchmark

# Import all benchmark submodules to make them available for import
from . import AIME2024
from . import AIME2025
from . import infinite_bench
from . import longbench
from . import longbenchv2
from . import loogle
from . import mock_benchmark
from . import ruler
from . import zero_scrolls

__all__ = [
    "Benchmark",
    "LongBench",
    "MockBenchmark",
    "AIME2024",
    "AIME2025", 
    "infinite_bench",
    "longbench",
    "longbenchv2",
    "loogle",
    "mock_benchmark",
    "ruler",
    "zero_scrolls"
]
```

```markdown
# Mock Benchmark

A simple benchmark for testing and demonstration purposes, designed to validate the benchmark framework with minimal data.

## Overview

The Mock Benchmark contains 5 carefully crafted samples with:
- **Short contexts**: All contexts are under 250 words (127-136 words each)
- **Simple questions**: Basic reading comprehension questions
- **Context sharing**: Samples 1&2 share one context, samples 4&5 share another context
- **Clear answers**: Straightforward, factual answers for easy evaluation

## Data Structure

The benchmark includes 3 unique contexts covering different domains:

1. **Science Context** (132 words): About photosynthesis
   - Sample 1: "What are the two main stages of photosynthesis?"
   - Sample 2: "What gas is produced as a byproduct of photosynthesis?"

2. **History Context** (136 words): About the Renaissance
   - Sample 3: "In which century did the Renaissance begin?"

3. **Geography Context** (127 words): About the Amazon rainforest
   - Sample 4: "Why is the Amazon rainforest called the 'lungs of the Earth'?"
   - Sample 5: "Which river flows through the Amazon rainforest?"

## Usage

```python
from benchmark.mock_benchmark import MockBenchmark

# Create benchmark instance
mock_benchmark = MockBenchmark()

# Run with a model adapter
results = mock_benchmark.run_benchmark(adapter, result_dir="/path/to/results")

# View results
print(f"Accuracy: {results['accuracy']}")
print(f"Correct predictions: {results['correct_predictions']}/{results['total_samples']}")
```

## Evaluation

The benchmark uses simple exact match and substring matching for evaluation:
- Exact match with ground truth answers
- Case-insensitive substring matching
- Multiple acceptable answers per question

## Purpose

This mock benchmark is ideal for:
- Testing the benchmark framework
- Validating model adapters
- Demonstrating benchmark usage
- Quick evaluation during development
```

```python
#!/usr/bin/env python3
"""Demo script showing how to use the MockBenchmark."""

import pandas as pd
from typing import Dict, Any

from benchmark.mock_benchmark import MockBenchmark


def demo_mock_benchmark() -> None:
    """Demonstrate the MockBenchmark usage and structure."""
    
    print("=== MockBenchmark Demo ===\n")
    
    # Create benchmark instance
    print("1. Creating MockBenchmark instance...")
    mock_benchmark = MockBenchmark()
    
    print(f"   Benchmark name: {mock_benchmark.benchmark_name}")
    print(f"   Available datasets: {mock_benchmark.all_datasets}")
    print(f"   Subsets to run: {mock_benchmark.subsets_to_run}")
    
    # Load and examine data
    print("\n2. Loading benchmark data...")
    df: pd.DataFrame = mock_benchmark._load_datasets()
    
    print(f"   Total samples: {len(df)}")
    print(f"   Unique contexts: {df['context'].nunique()}")
    print(f"   Columns: {list(df.columns)}")
    
    # Show sample data
    print("\n3. Sample data overview:")
    for i, (_, row) in enumerate(df.iterrows(), 1):
        context_words = len(row['context'].split())
        print(f"   Sample {i}: {row['question']}")
        print(f"     Context length: {context_words} words")
        print(f"     Expected answers: {row['answers']}")
    
    # Demonstrate context sharing
    print("\n4. Context sharing analysis:")
    context_counts = df['context'].value_counts()
    for i, (context_hash, count) in enumerate(context_counts.items(), 1):
        context_preview = context_hash[:50]
```

---

**User**

Uset mock_benchmark in our tutorial examples in py_example/04* files

---

**Cursor**

I'll help you integrate the mock_benchmark into the tutorial examples in the `py_example/04*` files. Let me first explore what tutorial files exist and understand their current structure.

Now I'll update both tutorial files to use the mock_benchmark for faster and simpler demonstrations. Let me start with the simple benchmark example:

```python
#!/usr/bin/env python3
"""
Simple Benchmark Example

A beginner-friendly example showing how to run a basic benchmark comparison
between dense and sparse attention using the sparse-attention-hub framework.

This example runs a small subset of LongBench tasks and compares:
- Dense (full) attention performance
- Sparse (StreamingLLM) attention performance

Usage:
    python 04_simple_benchmark_example.py
"""

->

#!/usr/bin/env python3
"""
Simple Benchmark Example

A beginner-friendly example showing how to run a basic benchmark comparison
between dense and sparse attention using the sparse-attention-hub framework.

This example uses the MockBenchmark (5 simple samples) for quick demonstration:
- Easy-to-understand reading comprehension questions
- Short contexts (<250 words each)
- Fast execution for testing and learning

Usage:
    python 04_simple_benchmark_example.py
"""
```

```python
from sparse_attention_hub.sparse_attention.research_attention import ResearchAttentionConfig
from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed.implementations import (
    LocalMaskerConfig, SinkMaskerConfig
)
from sparse_attention_hub.adapters.huggingface import ModelAdapterHF
from benchmark import LongBench

->

from sparse_attention_hub.sparse_attention.research_attention import ResearchAttentionConfig
from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed.implementations import (
    LocalMaskerConfig, SinkMaskerConfig
)
from sparse_attention_hub.adapters.huggingface import ModelAdapterHF
from benchmark import MockBenchmark
```

```python
    # Create benchmark with a small subset for quick testing
    benchmark_subsets = ["samsum"]  # Just one task for simplicity
    longbench = LongBench(subsets_to_run=benchmark_subsets)
    
    print(f"\n📊 Running benchmark on: {benchmark_subsets}")
    
    # Create result directories
    result_dir = Path("./simple_benchmark_results")
    result_dir.mkdir(exist_ok=True)
    
    sparse_dir = result_dir / "sparse"
    sparse_dir.mkdir(exist_ok=True)
    
    print("\n🧪 Running Dense Attention Benchmark...")
    start_time = time.time()
    
    # Load and limit dataset for quick testing
    dataset_df = longbench._load_datasets()
    dataset_df = dataset_df.head(3)  # Just 3 samples for quick demo
    print(f"  Processing {len(dataset_df)} samples...")
    
    # Run sparse benchmark
    with sparse_adapter.enable_sparse_mode():
        sparse_metrics = longbench.run_benchmark(
            adapter=sparse_adapter,
            result_dir=str(sparse_dir)
        )
    
    sparse_time = time.time() - start_time
    print(f"  ✅ Sparse completed in {sparse_time:.2f}s")
    print(f"     Average score: {sparse_metrics.get('average_score', 'N/A')}")
    


if __name__ == "__main__":
    main()

->

    # Create mock benchmark - fast and simple
    mock_benchmark = MockBenchmark()
    
    print(f"\n📊 Running MockBenchmark:")
    print(f"  - 5 simple reading comprehension samples")
    print(f"  - 3 different contexts (science, history, geography)")
    print(f"  - Context sharing to test grouping efficiency")
    
    # Create result directories
    result_dir = Path("./simple_benchmark_results")
    result_dir.mkdir(exist_ok=True)
    
    sparse_dir = result_dir / "sparse"
    sparse_dir.mkdir(exist_ok=True)
    
    print("\n🧪 Running Sparse Attention Benchmark...")
    start_time = time.time()
    
    # Show dataset overview
    dataset_df = mock_benchmark._load_datasets()
    print(f"  Processing {len(dataset_df)} samples across {dataset_df['context'].nunique()} contexts...")
    
    # Run sparse benchmark
    with sparse_adapter.enable_sparse_mode():
        sparse_metrics = mock_benchmark.run_benchmark(
            adapter=sparse_adapter,
            result_dir=str(sparse_dir)
        )
    
    sparse_time = time.time() - start_time
    print(f"  ✅ Sparse completed in {sparse_time:.2f}s")
    print(f"     Accuracy: {sparse_metrics.get('accuracy', 'N/A')}")
    print(f"     Correct predictions: {sparse_metrics.get('correct_predictions', 'N/A')}/{sparse_metrics.get('total_samples', 'N/A')}")
    
    print("\n📋 Results Summary:")
    print(f"  • Benchmark: {sparse_metrics['summary']['benchmark']}")
    print(f"  • Task: {sparse_metrics['summary']['task']}")
    print(f"  • Unique contexts: {sparse_metrics['summary']['unique_contexts']}")
    print(f"  • Evaluation method: {sparse_metrics['summary']['evaluation_method']}")
    
    print(f"\n💾 Results saved to: {sparse_dir}")
    print("   - raw_results.csv: Detailed predictions for each sample")
    print("   - metrics.json: Evaluation metrics and summary")


if __name__ == "__main__":
    main()
```

Now let me update the streaming LLM benchmark demo to include MockBenchmark as an option:

```python
#!/usr/bin/env python3
"""
Streaming LLM Benchmark Demo

This example demonstrates how to run benchmarks using the sparse-attention-hub framework.
It compares streaming attention (sparse) vs full attention performance on LongBench tasks.

Features:
- StreamingLLM configuration with sink + local attention
- LongBench benchmark execution
- Performance comparison between sparse and dense attention
- Comprehensive result analysis and visualization
- Memory usage tracking
- Timing analysis

Usage:
    python 04_streaming_llm_benchmark_demo.py [--model MODEL_NAME] [--subsets SUBSET1,SUBSET2]
"""

->

#!/usr/bin/env python3
"""
Streaming LLM Benchmark Demo

This example demonstrates how to run benchmarks using the sparse-attention-hub framework.
It compares streaming attention (sparse) vs full attention performance on benchmark tasks.

Features:
- StreamingLLM configuration with sink + local attention
- MockBenchmark (default) for quick testing or LongBench for comprehensive evaluation
- Performance comparison between sparse and dense attention
- Comprehensive result analysis and visualization
- Memory usage tracking
- Timing analysis

Usage:
    python 04_streaming_llm_benchmark_demo.py [--model MODEL_NAME] [--benchmark mock|longbench] [--subsets SUBSET1,SUBSET2]
"""
```

```python
from sparse_attention_hub.sparse_attention.research_attention import ResearchAttentionConfig
from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed.implementations import (
    LocalMaskerConfig, SinkMaskerConfig
)
from sparse_attention_hub.adapters.huggingface import ModelAdapterHF
from benchmark import LongBench

->

from sparse_attention_hub.sparse_attention.research_attention import ResearchAttentionConfig
from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed.implementations import (
    LocalMaskerConfig, SinkMaskerConfig
)
from sparse_attention_hub.adapters.huggingface import ModelAdapterHF
from benchmark import LongBench, MockBenchmark
```

```python
    def __init__(
        self, 
        model_name: str = "microsoft/Phi-4-mini-instruct",
        device: str = "cuda",
        sink_size: int = 4,
        local_window: int = 64,
        result_dir: str = "./benchmark_results"
    ):

->

    def __init__(
        self, 
        model_name: str = "microsoft/Phi-4-mini-instruct",
        device: str = "cuda",
        sink_size: int = 4,
        local_window: int = 64,
        result_dir: str = "./benchmark_results",
        benchmark_type: str = "mock"
    ):
```

```python
        """Initialize the benchmark runner.
        
        Args:
            model_name: HuggingFace model identifier
            device: Device to run on ('cuda' or 'cpu')
            sink_size: Number of sink tokens for StreamingLLM
            local_window: Local attention window size
            result_dir: Directory to save results
        """

->

        """Initialize the benchmark runner.
        
        Args:
            model_name: HuggingFace model identifier
            device: Device to run on ('cuda' or 'cpu')
            sink_size: Number of sink tokens for StreamingLLM
            local_window: Local attention window size
            result_dir: Directory to save results
            benchmark_type: Type of benchmark to use ('mock' or 'longbench')
        """
```

```python
        self.model_name = model_name
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.sink_size = sink_size
        self.local_window = local_window
        self.result_dir = Path(result_dir)
        self.result_dir.mkdir(exist_ok=True)
        
        print(f"🚀 Initializing Benchmark Runner")
        print(f"   Model: {model_name}")
        print(f"   Device: {self.device}")
        print(f"   StreamingLLM Config: Sink({sink_size}) + Local({local_window})")
        print(f"   Results will be saved to: {self.result_dir}")

->

        self.model_name = model_name
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.sink_size = sink_size
        self.local_window = local_window
        self.result_dir = Path(result_dir)
        self.result_dir.mkdir(exist_ok=True)
        self.benchmark_type = benchmark_type
        
        print(f"🚀 Initializing Benchmark Runner")
        print(f"   Model: {model_name}")
        print(f"   Device: {self.device}")
        print(f"   Benchmark: {benchmark_type.upper()}")
        print(f"   StreamingLLM Config: Sink({sink_size}) + Local({local_window})")
        print(f"   Results will be saved to: {self.result_dir}")
```

```python
    def run_benchmark_comparison(
        self, 
        benchmark_subsets: Optional[List[str]] = None,
        max_samples: Optional[int] = 10
    ) -> Dict[str, Dict]:
        """Run benchmark comparison between dense and sparse attention.
        
        Args:
            benchmark_subsets: List of LongBench subsets to run. If None, uses a small default set.
            max_samples: Maximum number of samples per subset for quick testing
            
        Returns:
            Dictionary containing results for both dense and sparse runs
        """
        if benchmark_subsets is None:
            # Use a small subset for demonstration
            benchmark_subsets = ["narrativeqa", "qasper", "samsum"]
        
        print(f"🧪 Running benchmark comparison on subsets: {benchmark_subsets}")
        
        # Create benchmark instances
        longbench = LongBench(subsets_to_run=benchmark_subsets)

->

    def run_benchmark_comparison(
        self, 
        benchmark_subsets: Optional[List[str]] = None,
        max_samples: Optional[int] = 10
    ) -> Dict[str, Dict]:
        """Run benchmark comparison between dense and sparse attention.
        
        Args:
            benchmark_subsets: List of benchmark subsets to run. For MockBenchmark, this parameter is ignored.
                               For LongBench, if None, uses a small default set.
            max_samples: Maximum number of samples per subset for quick testing (ignored for MockBenchmark)
            
        Returns:
            Dictionary containing results for both dense and sparse runs
        """
        # Create benchmark instance based on type
        if self.benchmark_type == "mock":
            benchmark = MockBenchmark()
            print(f"🧪 Running MockBenchmark comparison:")
            print(f"   - 5 simple reading comprehension samples")
            print(f"   - 3 different contexts (science, history, geography)")
            print(f"   - Fast execution for testing and learning")
        else:  # longbench
            if benchmark_subsets is None:
                # Use a small subset for demonstration
                benchmark_subsets = ["narrativeqa", "qasper", "samsum"]
            
            print(f"🧪 Running LongBench comparison on subsets: {benchmark_subsets}")
            benchmark = LongBench(subsets_to_run=benchmark_subsets)
```

```python
        results['dense'] = self._run_single_benchmark(
            adapter=self.dense_adapter,
            benchmark=longbench,
            mode_name="dense",
            max_samples=max_samples
        )
        
        # Run sparse attention benchmark
        print("\n⚡ Running Sparse Attention Benchmark...")
        if self.sparse_adapter is None:
            raise RuntimeError("Sparse adapter not initialized. Call initialize_adapters() first.")
        results['sparse'] = self._run_single_benchmark(
            adapter=self.sparse_adapter,
            benchmark=longbench,
            mode_name="sparse",
            max_samples=max_samples
        )

->

        results['dense'] = self._run_single_benchmark(
            adapter=self.dense_adapter,
            benchmark=benchmark,
            mode_name="dense",
            max_samples=max_samples
        )
        
        # Run sparse attention benchmark
        print("\n⚡ Running Sparse Attention Benchmark...")
        if self.sparse_adapter is None:
            raise RuntimeError("Sparse adapter not initialized. Call initialize_adapters() first.")
        results['sparse'] = self._run_single_benchmark(
            adapter=self.sparse_adapter,
            benchmark=benchmark,
            mode_name="sparse",
            max_samples=max_samples
        )
```

```python
from benchmark import LongBench, MockBenchmark

->

from benchmark import Benchmark, LongBench, MockBenchmark
```

```python
    def _run_single_benchmark(
        self, 
        adapter: ModelAdapterHF, 
        benchmark: LongBench, 
        mode_name: str,
        max_samples: Optional[int] = None
    ) -> Dict:

->

    def _run_single_benchmark(
        self, 
        adapter: ModelAdapterHF, 
        benchmark: Benchmark, 
        mode_name: str,
        max_samples: Optional[int] = None
    ) -> Dict:
```

```python
    def _run_single_benchmark(
        self, 
        adapter: ModelAdapterHF, 
        benchmark: Benchmark, 
        mode_name: str,
        max_samples: Optional[int] = None
    ) -> Dict:
```

```python
    parser = argparse.ArgumentParser(description="Streaming LLM Benchmark Demo")
    parser.add_argument("--model", default="microsoft/Phi-4-mini-instruct", 
                       help="HuggingFace model name")
    parser.add_argument("--benchmark", default="mock", choices=["mock", "longbench"],
                       help="Benchmark type: 'mock' for quick testing (5 samples) or 'longbench' for comprehensive evaluation")
    parser.add_argument("--subsets", default="narrativeqa,qasper,samsum",
                       help="Comma-separated list of LongBench subsets (ignored for MockBenchmark)")
    parser.add_argument("--max-samples", type=int, default=5,
                       help="Maximum samples per subset for quick testing (ignored for MockBenchmark)")
```

```python
    # Initialize benchmark runner
    runner = BenchmarkRunner(
        model_name=args.model,
        device=args.device,
        sink_size=args.sink_size,
        local_window=args.local_window,
        result_dir=args.result_dir,
        benchmark_type=args.benchmark
    )
```

