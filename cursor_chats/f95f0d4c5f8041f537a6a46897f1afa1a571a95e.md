# Understanding micro metric logger usage
_Exported on 9/1/2025 at 20:02:15 PDT from Cursor (1.5.5)_

---

**User**

Understand the micro metric logger usage in the repository

---

**Cursor**

I'll help you understand the micro metric logger usage in this repository. Let me start by exploring the codebase to find where micro metric logging is implemented and used.

Now let me examine the main implementation and tests to get a comprehensive understanding:

Based on my analysis of the repository, here's a comprehensive overview of the **MicroMetricLogger** usage in the sparse-attention-hub:

## 📊 MicroMetricLogger Architecture & Usage Summary

### **Core Architecture**

The `MicroMetricLogger` is implemented as a **singleton pattern** with queue-based architecture located in `sparse_attention_hub/metric_logging/logger.py`. It provides efficient micro-metrics logging for performance analysis and research tracking.

**Key Components:**
- **`LogEvent` dataclass**: Structured log entries with timestamp, metric identifier, value, metadata, and auto-inferred location
- **Singleton pattern**: Ensures single instance across the application
- **Circular buffer**: Queue with 10,000 event limit to prevent memory overflow
- **Automatic flushing**: Configurable triggers (event count or time interval)
- **JSONL output**: Human-readable structured logging format

### **Registration & Configuration System**

```python
# Class-level metric registration (works without initialization)
MicroMetricLogger.register_metric("research_attention_density", float)
MicroMetricLogger.register_metric("research_attention_output_error", float)

# Configuration and enabling
logger = MicroMetricLogger()
logger.configure_logging(
    log_path="./experiment_logs", 
    enabled_metrics=["research_attention_density", "research_attention_output_error"]
)
```

### **Real Usage Patterns in the Codebase**

#### **1. Research Attention Monitoring** (`sparse_attention_hub/sparse_attention/research_attention/base.py`)

```python
# Density tracking with layer context
if MicroMetricLogger().is_metric_enabled("research_attention_density"):
    MicroMetricLogger().log(
        "research_attention_density",
        sparse_attention_mask.get_density(),
        metadata={"layer_idx": kwargs["layer_idx"]}
    )

# Output error measurement  
if MicroMetricLogger().is_metric_enabled("research_attention_output_error"):
    error = torch.norm(true_attention_output - attention_output) / torch.norm(true_attention_output)
    MicroMetricLogger().log(
        "research_attention_output_error",
        float(error.item()),
        metadata={"layer_idx": kwargs["layer_idx"]}
    )
```

#### **2. Benchmark Execution Integration** (`benchmark/executor.py`)

```python
# Per-benchmark metric collection
metric_logger = MicroMetricLogger()
metric_logger.configure_logging(
    log_path=stub.result_dir, 
    enabled_metrics=["research_attention_density", "research_attention_output_error"]
)

# Execute benchmark with metric collection
metrics = benchmark.run_benchmark(adapter=adapter, ...)

# Ensure all metrics are written
metric_logger.flush()
```

### **Key Features & Benefits**

#### **Performance-Oriented Design**
- **Conditional logging**: Only logs when metrics are enabled, minimizing overhead
- **Batched I/O**: Queue-based system reduces disk write frequency
- **Memory safety**: Circular buffer prevents unlimited memory growth

#### **Developer Experience**
- **Automatic location inference**: Captures calling context (module.class.method)
- **Flexible metadata**: Rich contextual information (layer indices, device info, etc.)
- **Warning system**: Alerts for configuration issues and unregistered metrics

#### **Research & Analysis Support**
- **Structured output**: JSONL format for easy post-processing and analysis
- **Timestamped events**: Precise timing information for performance analysis
- **Hierarchical organization**: Layer-wise and component-wise metric tracking

### **Configuration Options**

```python
logger = MicroMetricLogger(
    log_path="./metric_logs",           # Output directory
    flush_every=1000,                   # Events per flush
    flush_interval=60.0,                # Seconds between flushes
    enabled_metrics=["metric1", "all"] # Specific metrics or all
)
```

### **Output Format Example**

```json
{
  "timestamp": "2025-01-26T10:30:45.123456",
  "metric": "research_attention_density", 
  "value": 0.15,
  "metadata": {"layer_idx": 2, "head": 0},
  "location": "sparse_attention_hub.sparse_attention.research_attention.base.ResearchAttention.custom_attention"
}
```

### **Common Usage Patterns**

1. **Module-level registration**: Register metrics when modules are imported
2. **Conditional logging**: Check if metrics are enabled before expensive computations
3. **Rich metadata**: Include layer indices, head numbers, model info for analysis
4. **Benchmark integration**: Configure per-experiment to collect relevant metrics
5. **Cleanup handling**: Automatic flush on destruction ensures data persistence

```python
# Class-level metric registration (works without initialization)
MicroMetricLogger.register_metric("research_attention_density", float)
MicroMetricLogger.register_metric("research_attention_output_error", float)

# Configuration and enabling
logger = MicroMetricLogger()
logger.configure_logging(
    log_path="./experiment_logs", 
    enabled_metrics=["research_attention_density", "research_attention_output_error"]
)
```

```python
# Density tracking with layer context
if MicroMetricLogger().is_metric_enabled("research_attention_density"):
    MicroMetricLogger().log(
        "research_attention_density",
        sparse_attention_mask.get_density(),
        metadata={"layer_idx": kwargs["layer_idx"]}
    )

# Output error measurement  
if MicroMetricLogger().is_metric_enabled("research_attention_output_error"):
    error = torch.norm(true_attention_output - attention_output) / torch.norm(true_attention_output)
    MicroMetricLogger().log(
        "research_attention_output_error",
        float(error.item()),
        metadata={"layer_idx": kwargs["layer_idx"]}
    )
```

```python
# Per-benchmark metric collection
metric_logger = MicroMetricLogger()
metric_logger.configure_logging(
    log_path=stub.result_dir, 
    enabled_metrics=["research_attention_density", "research_attention_output_error"]
)

# Execute benchmark with metric collection
metrics = benchmark.run_benchmark(adapter=adapter, ...)

# Ensure all metrics are written
metric_logger.flush()
```

```python
logger = MicroMetricLogger(
    log_path="./metric_logs",           # Output directory
    flush_every=1000,                   # Events per flush
    flush_interval=60.0,                # Seconds between flushes
    enabled_metrics=["metric1", "all"] # Specific metrics or all
)
```

```json
{
  "timestamp": "2025-01-26T10:30:45.123456",
  "metric": "research_attention_density", 
  "value": 0.15,
  "metadata": {"layer_idx": 2, "head": 0},
  "location": "sparse_attention_hub.sparse_attention.research_attention.base.ResearchAttention.custom_attention"
}
```

The MicroMetricLogger serves as a **critical infrastructure component** for the sparse-attention-hub, enabling detailed performance monitoring and research analysis. It's particularly valuable for:

- **Performance analysis** of sparse attention mechanisms
- **Research experimentation** with different masking strategies  
- **Benchmark result correlation** with micro-level metrics
- **Development debugging** through rich contextual logging

The system's design prioritizes both **performance efficiency** (minimal overhead when disabled) and **research utility** (rich metadata and structured output), making it well-suited for both production deployments and research experimentation.

---

**User**

I want to add two options to the micro metric logger .. 
1. max records : which is the max limit of the number of events that are recorded in this log. When hit the logging does not add more events / flushes more events. 
accounting for number of records inserted should be reset when a configure_logging is called or the instance is created for the first time. 
2. sampling_factor: this will randomly reduce the events that are inserted in the queue. e.g. if sampling factor is 0.5 only half of the events will be logged in expectation.  

---

**Cursor**

I'll help you add those two features to the MicroMetricLogger. Let me examine the current implementation and then add the `max_records` and `sampling_factor` options.

Now I'll implement the two new features for the MicroMetricLogger. Let me add the `max_records` and `sampling_factor` options:

```python
"""MicroMetricLogger implementation for sparse attention hub."""

import inspect
import json
import os
import random
import time
from collections import deque
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional, Union


@dataclass
class LogEvent:
    """Log event data structure."""

    timestamp: datetime
    metric: str  # Metric identifier string
    value: Union[None, Any]
    metadata: Dict[str, Any]  # Additional context (layer, head, etc.)
    location: str  # Auto-inferred: "module.function" or "class.method"


class MicroMetricLogger:
    """Singleton logger for micro metrics with queue-based architecture."""

    _instance: Optional["MicroMetricLogger"] = None
    _initialized: bool = False

    # Class-level storage for registered metrics (works without initialization)
    _registered_metrics: Dict[str, type] = {}  # identifier -> dtype mapping

    def __new__(cls, *args, **kwargs) -> "MicroMetricLogger":
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(
        self,
        log_path: Optional[str] = None,
        flush_every: int = 1000,  # Flush every N events
        flush_interval: float = 60.0,  # Flush every N seconds
        enabled_metrics: Union[List[str], str] = None,
        max_records: Optional[int] = None,  # Maximum number of events to log
        sampling_factor: float = 1.0,  # Probability of logging each event (0.0-1.0)
    ):  # List of string identifiers to enable, or "all"
        if not self._initialized:
            self.log_path = log_path
            self.flush_every = flush_every
            self.flush_interval = flush_interval
            self.max_records = max_records
            self.sampling_factor = max(0.0, min(1.0, sampling_factor))  # Clamp to [0.0, 1.0]

            # Internal state
            self.log_queue: deque = deque(maxlen=10000)  # Circular buffer
            self.enabled_metrics: set = set()
            self.last_flush_time = time.time()
            self._total_records_logged: int = 0  # Track total events logged

            # Enable metrics if log_path is provided
            if self.log_path is not None:
                self._ensure_log_directory()
                self.enable_metrics(enabled_metrics)

            MicroMetricLogger._initialized = True
        else:
            if self.log_path and log_path and self.log_path != log_path:
                print(
                    f"Warning: MicroMetricLogger already initialized with log_path: {self.log_path}"
                )
            # Update parameters if they differ from current values
            if max_records is not None and hasattr(self, 'max_records'):
                self.max_records = max_records
            if sampling_factor != 1.0 and hasattr(self, 'sampling_factor'):
                self.sampling_factor = max(0.0, min(1.0, sampling_factor))

    # main registration function

    @classmethod
    def register_metric(cls, identifier: str, dtype: type) -> None:
        """Register a metric with its string identifier and expected data type.

        This works at class level and doesn't require initialization.
        """
        if identifier in cls._registered_metrics:
            print(f"Warning: Metric '{identifier}' is being re-registered")
        cls._registered_metrics[identifier] = dtype

    @classmethod
    def get_registered_metrics(cls) -> Dict[str, type]:
        """Get all registered metrics at class level."""
        return cls._registered_metrics.copy()

    # helper methods

    def _ensure_log_directory(self) -> None:
        """Ensure the log directory exists."""
        if self.log_path is not None:
            os.makedirs(self.log_path, exist_ok=True)

    def _get_calling_location(self) -> str:
        """Get the calling location using inspect module."""
        try:
            # Get the calling frame (skip this method and the log method)
            caller_frame = inspect.currentframe().f_back.f_back
            if caller_frame is None:
                return "unknown"

            # Get module name
            module = inspect.getmodule(caller_frame)
            module_name = module.__name__ if module else "unknown"

            # Get function/class name
            function_name = caller_frame.f_code.co_name

            # Try to get class name if it's a method
            class_name = None
            if "self" in caller_frame.f_locals:
                class_name = caller_frame.f_locals["self"].__class__.__name__

            if class_name:
                return f"{module_name}.{class_name}.{function_name}"
            else:
                return f"{module_name}.{function_name}"
        except Exception:
            return "unknown"

    def __del__(self):
        """Cleanup when logger is destroyed."""
        self.flush()  # Final flush

    # api

    def enable_metrics(self, metrics: Union[List[str], str] = None) -> None:
        """Enable logging for specific metrics.

        Args:
            metrics: List of metric identifiers to enable, or "all" for all registered metrics.
                    If None, enables no metrics (empty list).
        """
        if metrics == "all":
            self.enabled_metrics = set(self._registered_metrics.keys())
        elif isinstance(metrics, (list, set)):
            # Only enable metrics that are registered
            valid_metrics = set(metrics) & set(self._registered_metrics.keys())
            invalid_metrics = set(metrics) - set(self._registered_metrics.keys())
            if invalid_metrics:
                print(
                    f"Warning: Attempting to enable unregistered metrics: {invalid_metrics}"
                )
            self.enabled_metrics = valid_metrics
        else:
            # Default to empty set
            self.enabled_metrics = set()

    def log(self, identifier: str, value: Any, metadata: Dict[str, Any] = None) -> None:
        """Log a metric value with optional metadata. Location is auto-inferred.

        This only works if log_path is defined.
        """
        # Check if logging is configured
        if self.log_path is None:
            print(
                f"Warning: Cannot log metric '{identifier}' - log_path not defined. Use configure_logging() first."
            )
            return

        # Check if metric is enabled
        if identifier not in self.enabled_metrics:
            print(
                f"Warning: Attempting to log metric '{identifier}' which is not enabled"
            )
            return

        # Check if max_records limit has been reached
        if self.max_records is not None and self._total_records_logged >= self.max_records:
            return  # Silently ignore - we've hit the limit

        # Apply sampling factor
        if self.sampling_factor < 1.0 and random.random() > self.sampling_factor:
            return  # Event not selected for logging due to sampling

        # Create log event
        event = LogEvent(
            timestamp=datetime.now(),
            metric=identifier,
            value=value,
            metadata=metadata or {},
            location=self._get_calling_location(),
        )

        # Add to queue and increment counter
        self.log_queue.append(event)
        self._total_records_logged += 1

        # Check if we should flush
        if len(self.log_queue) >= self.flush_every:
            self.flush()

    def configure_logging(
        self,
        log_path: str,
        enabled_metrics: Union[List[str], str] = None,
        max_records: Optional[int] = None,
        sampling_factor: Optional[float] = None,
    ) -> None:
        """Configure logging with a log path and optionally enable metrics.

        This must be called before logging can work. Resets the total records counter.
        """
        self.log_path = log_path
        self._ensure_log_directory()
        self.enable_metrics(enabled_metrics)
        
        # Update limits if provided
        if max_records is not None:
            self.max_records = max_records
        if sampling_factor is not None:
            self.sampling_factor = max(0.0, min(1.0, sampling_factor))
        
        # Reset the total records counter when reconfiguring
        self._total_records_logged = 0

    def flush(self) -> None:
        """Force flush the current queue to disk."""
        if not self.log_queue or self.log_path is None:
            return

        # Get current timestamp for filename
        filename = "micro_metrics.jsonl"
        filepath = os.path.join(self.log_path, filename)

        # Write events to file
        with open(filepath, "a", encoding="utf-8") as f:
            while self.log_queue:
                event = self.log_queue.popleft()
                # Convert dataclass to dict and serialize
                event_dict = asdict(event)
                # Convert datetime to ISO format string
                event_dict["timestamp"] = event_dict["timestamp"].isoformat()
                f.write(json.dumps(event_dict) + "\n")

        self.last_flush_time = time.time()

    def is_metric_enabled(self, identifier: str) -> bool:
        """Check if a specific metric is requested for logging."""
        return identifier in self.enabled_metrics

    def get_enabled_metrics(self) -> set:
        """Get currently enabled metrics."""
        return self.enabled_metrics.copy()

    def is_logging_configured(self) -> bool:
        """Check if logging is configured (log_path is set)."""
        return self.log_path is not None

    def get_total_records_logged(self) -> int:
        """Get the total number of records logged since initialization or last configure_logging call."""
        return getattr(self, '_total_records_logged', 0)

    def is_max_records_reached(self) -> bool:
        """Check if the maximum number of records has been reached."""
        if self.max_records is None:
            return False
        return self.get_total_records_logged() >= self.max_records

    def get_records_remaining(self) -> Optional[int]:
        """Get the number of records remaining before hitting max_records limit.
        
        Returns:
            Number of records remaining, or None if no limit is set.
        """
        if self.max_records is None:
            return None
        return max(0, self.max_records - self.get_total_records_logged())

    def get_sampling_factor(self) -> float:
        """Get the current sampling factor."""
        return getattr(self, 'sampling_factor', 1.0)

    def get_max_records(self) -> Optional[int]:
        """Get the current max_records limit."""
        return getattr(self, 'max_records', None)

```

```python
"""Unit tests for MicroMetricLogger."""

import json
import time
from datetime import datetime

import pytest

from sparse_attention_hub.metric_logging import LogEvent, MicroMetricLogger


@pytest.mark.unit
class TestLogEvent:
    """Test cases for LogEvent dataclass."""

    def test_log_event_creation(self) -> None:
        """Test LogEvent creation with all fields."""
        timestamp = datetime.now()
        event = LogEvent(
            timestamp=timestamp,
            metric="test_metric",
            value=42.0,
            metadata={"layer": 1, "head": 2},
            location="test.module.function",
        )

        assert event.timestamp == timestamp
        assert event.metric == "test_metric"
        assert event.value == 42.0
        assert event.metadata == {"layer": 1, "head": 2}
        assert event.location == "test.module.function"

    def test_log_event_with_none_value(self) -> None:
        """Test LogEvent creation with None value."""
        event = LogEvent(
            timestamp=datetime.now(),
            metric="test_metric",
            value=None,
            metadata={},
            location="test.module.function",
        )

        assert event.value is None

    def test_log_event_minimal_metadata(self) -> None:
        """Test LogEvent creation with minimal metadata."""
        event = LogEvent(
            timestamp=datetime.now(),
            metric="test_metric",
            value=42.0,
            metadata={},
            location="test.module.function",
        )

        assert event.metadata == {}


@pytest.mark.unit
class TestMicroMetricLoggerSingleton:
    """Test cases for MicroMetricLogger singleton pattern."""

    def test_singleton_behavior(self) -> None:
        """Test that MicroMetricLogger follows singleton pattern."""
        # Reset singleton state
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False

        logger1 = MicroMetricLogger()
        logger2 = MicroMetricLogger()

        assert logger1 is logger2
        assert id(logger1) == id(logger2)

    def test_singleton_initialization_once(self) -> None:
        """Test that initialization only happens once."""
        # Reset singleton state
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False

        logger1 = MicroMetricLogger(log_path="/tmp/test1")
        logger2 = MicroMetricLogger(log_path="/tmp/test2")

        # Second initialization should not change the log_path
        assert logger1.log_path == "/tmp/test1"
        assert logger2.log_path == "/tmp/test1"  # Should be the same as logger1


@pytest.mark.unit
class TestMicroMetricLoggerRegistration:
    """Test cases for metric registration functionality."""

    def test_register_metric(self) -> None:
        """Test metric registration."""
        # Reset registered metrics
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("attention_score", float)
        MicroMetricLogger.register_metric("sparsity_ratio", float)

        registered = MicroMetricLogger.get_registered_metrics()
        assert "attention_score" in registered
        assert "sparsity_ratio" in registered
        assert registered["attention_score"] == float
        assert registered["sparsity_ratio"] == float

    def test_register_metric_re_registration_warning(self, capsys) -> None:
        """Test warning when re-registering a metric."""
        # Reset registered metrics
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)
        MicroMetricLogger.register_metric(
            "test_metric", int
        )  # Re-register with different type

        captured = capsys.readouterr()
        assert "Warning: Metric 'test_metric' is being re-registered" in captured.out

    def test_get_registered_metrics_copy(self) -> None:
        """Test that get_registered_metrics returns a copy."""
        # Reset registered metrics
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)
        registered = MicroMetricLogger.get_registered_metrics()

        # Modify the returned dict - should not affect the original
        registered["new_metric"] = str

        original = MicroMetricLogger.get_registered_metrics()
        assert "new_metric" not in original


@pytest.mark.unit
class TestMicroMetricLoggerInitialization:
    """Test cases for MicroMetricLogger initialization."""

    def test_default_initialization(self) -> None:
        """Test default initialization parameters."""
        # Reset singleton state
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False

        logger = MicroMetricLogger()

        assert logger.log_path is None
        assert logger.flush_every == 1000
        assert logger.flush_interval == 60.0
        assert logger.enabled_metrics == set()
        assert len(logger.log_queue) == 0
        assert logger.log_queue.maxlen == 10000

    def test_custom_initialization(self) -> None:
        """Test initialization with custom parameters."""
        # Reset singleton state
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False

        logger = MicroMetricLogger(
            log_path="/tmp/test",
            flush_every=500,
            flush_interval=30.0,
            enabled_metrics=["metric1", "metric2"],
        )

        assert logger.log_path == "/tmp/test"
        assert logger.flush_every == 500
        assert logger.flush_interval == 30.0

    def test_initialization_with_log_path_creates_directory(self, tmp_path) -> None:
        """Test that initialization creates log directory when log_path is provided."""
        # Reset singleton state
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False

        log_dir = tmp_path / "logs"
        _ = MicroMetricLogger(log_path=str(log_dir))

        assert log_dir.exists()
        assert log_dir.is_dir()


@pytest.mark.unit
class TestMicroMetricLoggerEnableMetrics:
    """Test cases for enabling metrics functionality."""

    def test_enable_metrics_all(self) -> None:
        """Test enabling all registered metrics."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        # Register some metrics
        MicroMetricLogger.register_metric("metric1", float)
        MicroMetricLogger.register_metric("metric2", int)

        logger = MicroMetricLogger()
        logger.enable_metrics("all")

        assert logger.enabled_metrics == {"metric1", "metric2"}

    def test_enable_metrics_specific_list(self) -> None:
        """Test enabling specific metrics from a list."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        # Register some metrics
        MicroMetricLogger.register_metric("metric1", float)
        MicroMetricLogger.register_metric("metric2", int)
        MicroMetricLogger.register_metric("metric3", str)

        logger = MicroMetricLogger()
        logger.enable_metrics(["metric1", "metric3"])

        assert logger.enabled_metrics == {"metric1", "metric3"}

    def test_enable_metrics_with_unregistered_warning(self, capsys) -> None:
        """Test warning when enabling unregistered metrics."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        # Register only one metric
        MicroMetricLogger.register_metric("metric1", float)

        logger = MicroMetricLogger()
        logger.enable_metrics(["metric1", "unregistered_metric"])

        captured = capsys.readouterr()
        assert "Warning: Attempting to enable unregistered metrics" in captured.out
        assert logger.enabled_metrics == {"metric1"}

    def test_enable_metrics_none(self) -> None:
        """Test enabling no metrics (None)."""
        # Reset singleton state
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False

        logger = MicroMetricLogger()
        logger.enable_metrics(None)

        assert logger.enabled_metrics == set()

    def test_enable_metrics_empty_list(self) -> None:
        """Test enabling empty list of metrics."""
        # Reset singleton state
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False

        logger = MicroMetricLogger()
        logger.enable_metrics([])

        assert logger.enabled_metrics == set()


@pytest.mark.unit
class TestMicroMetricLoggerLogging:
    """Test cases for logging functionality."""

    def test_log_without_configuration_warning(self, capsys) -> None:
        """Test warning when logging without configuration."""
        # Reset singleton state
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False

        logger = MicroMetricLogger()
        logger.log("test_metric", 42.0)

        captured = capsys.readouterr()
        assert (
            "Warning: Cannot log metric 'test_metric' - log_path not defined"
            in captured.out
        )

    def test_log_disabled_metric_warning(self, capsys, tmp_path) -> None:
        """Test warning when logging disabled metric."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("metric1", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["metric1"])
        logger.log("metric2", 42.0)  # Log unenabled metric

        captured = capsys.readouterr()
        assert (
            "Warning: Attempting to log metric 'metric2' which is not enabled"
            in captured.out
        )

    def test_log_enabled_metric(self, tmp_path) -> None:
        """Test successful logging of enabled metric."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["test_metric"])

        logger.log("test_metric", 42.0, {"layer": 1, "head": 2})

        assert len(logger.log_queue) == 1
        event = logger.log_queue[0]
        assert event.metric == "test_metric"
        assert event.value == 42.0
        assert event.metadata == {"layer": 1, "head": 2}
        assert isinstance(event.timestamp, datetime)

    def test_log_with_none_metadata(self, tmp_path) -> None:
        """Test logging with None metadata (should use empty dict)."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["test_metric"])

        logger.log("test_metric", 42.0, None)

        assert len(logger.log_queue) == 1
        event = logger.log_queue[0]
        assert event.metadata == {}

    def test_log_auto_flush_on_queue_size(self, tmp_path) -> None:
        """Test automatic flushing when queue reaches flush_every size."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path), flush_every=2)
        logger.enable_metrics(["test_metric"])

        # Log 2 events - should trigger flush
        logger.log("test_metric", 1.0)
        logger.log("test_metric", 2.0)

        # Queue should be empty after flush
        assert len(logger.log_queue) == 0


@pytest.mark.unit
class TestMicroMetricLoggerFlushing:
    """Test cases for flushing functionality."""

    def test_flush_empty_queue(self, tmp_path) -> None:
        """Test flushing empty queue."""
        # Reset singleton state
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.flush()  # Should not raise any error

        # No file should be created
        log_file = tmp_path / "micro_metrics.jsonl"
        assert not log_file.exists()

    def test_flush_without_log_path(self) -> None:
        """Test flushing without log_path configured."""
        # Reset singleton state
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False

        logger = MicroMetricLogger()
        logger.flush()  # Should not raise any error

    def test_flush_creates_file_and_writes_events(self, tmp_path) -> None:
        """Test that flush creates file and writes events correctly."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["test_metric"])

        # Add some events to queue
        logger.log("test_metric", 42.0, {"layer": 1})
        logger.log("test_metric", 43.0, {"layer": 2})

        # Flush
        logger.flush()

        # Check file was created
        log_file = tmp_path / "micro_metrics.jsonl"
        assert log_file.exists()

        # Check content
        with open(log_file, "r") as f:
            lines = f.readlines()

        assert len(lines) == 2

        # Parse first line
        event1 = json.loads(lines[0])
        assert event1["metric"] == "test_metric"
        assert event1["value"] == 42.0
        assert event1["metadata"] == {"layer": 1}
        assert "timestamp" in event1
        assert "location" in event1

    def test_flush_clears_queue(self, tmp_path) -> None:
        """Test that flush clears the queue."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["test_metric"])

        # Add events to queue
        logger.log("test_metric", 42.0)
        logger.log("test_metric", 43.0)

        assert len(logger.log_queue) == 2

        # Flush
        logger.flush()

        # Queue should be empty
        assert len(logger.log_queue) == 0

    def test_flush_updates_last_flush_time(self, tmp_path) -> None:
        """Test that flush updates last_flush_time."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["test_metric"])

        initial_time = logger.last_flush_time

        # Wait a bit
        time.sleep(0.01)

        # Add event and flush
        logger.log("test_metric", 42.0)
        logger.flush()

        assert logger.last_flush_time > initial_time


@pytest.mark.unit
class TestMicroMetricLoggerUtilityMethods:
    """Test cases for utility methods."""

    def test_is_metric_enabled(self, tmp_path) -> None:
        """Test is_metric_enabled method."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("metric1", float)
        MicroMetricLogger.register_metric("metric2", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["metric1"])

        assert logger.is_metric_enabled("metric1") is True
        assert logger.is_metric_enabled("metric2") is False

    def test_get_enabled_metrics(self, tmp_path) -> None:
        """Test get_enabled_metrics method."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("metric1", float)
        MicroMetricLogger.register_metric("metric2", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["metric1", "metric2"])

        enabled = logger.get_enabled_metrics()
        assert enabled == {"metric1", "metric2"}

        # Should return a copy
        enabled.add("metric3")
        assert "metric3" not in logger.enabled_metrics

    def test_is_logging_configured(self) -> None:
        """Test is_logging_configured method."""
        # Reset singleton state
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False

        logger1 = MicroMetricLogger()
        assert logger1.is_logging_configured() is False

        # Reset singleton state again for second test
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False

        logger2 = MicroMetricLogger(log_path="/tmp/test")
        assert logger2.is_logging_configured() is True

    def test_configure_logging(self, tmp_path) -> None:
        """Test configure_logging method."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("metric1", float)

        logger = MicroMetricLogger()
        logger.configure_logging(str(tmp_path), ["metric1"])

        assert logger.log_path == str(tmp_path)
        assert logger.enabled_metrics == {"metric1"}
        assert tmp_path.exists()


@pytest.mark.unit
class TestMicroMetricLoggerLocationInference:
    """Test cases for automatic location inference."""

    def test_get_calling_location_function(self, tmp_path) -> None:
        """Test location inference for function calls."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["test_metric"])

        def test_function() -> None:
            logger.log("test_metric", 42.0)

        test_function()

        assert len(logger.log_queue) == 1
        event = logger.log_queue[0]
        assert "test_function" in event.location

    def test_get_calling_location_class_method(self, tmp_path) -> None:
        """Test location inference for class method calls."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["test_metric"])

        class TestClass:
            def test_method(self) -> None:
                logger.log("test_metric", 42.0)

        test_obj = TestClass()
        test_obj.test_method()

        assert len(logger.log_queue) == 1
        event = logger.log_queue[0]
        assert "TestClass" in event.location
        assert "test_method" in event.location


@pytest.mark.unit
class TestMicroMetricLoggerCleanup:
    """Test cases for cleanup functionality."""

    def test_del_flush(self, tmp_path) -> None:
        """Test that __del__ calls flush."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["test_metric"])

        # Add event to queue
        logger.log("test_metric", 42.0)

        # Manually call __del__ (simulating object destruction)
        logger.__del__()

        # Check that event was flushed to file
        log_file = tmp_path / "micro_metrics.jsonl"
        assert log_file.exists()

        with open(log_file, "r") as f:
            lines = f.readlines()

        assert len(lines) == 1


@pytest.mark.unit
class TestMicroMetricLoggerMaxRecords:
    """Test cases for max_records functionality."""

    def test_max_records_initialization(self, tmp_path) -> None:
        """Test initialization with max_records parameter."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path), max_records=5)
        logger.enable_metrics(["test_metric"])

        assert logger.get_max_records() == 5
        assert logger.get_total_records_logged() == 0
        assert not logger.is_max_records_reached()
        assert logger.get_records_remaining() == 5

    def test_max_records_limiting(self, tmp_path) -> None:
        """Test that max_records limits the number of events logged."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path), max_records=3)
        logger.enable_metrics(["test_metric"])

        # Log 5 events, but only 3 should be recorded
        for i in range(5):
            logger.log("test_metric", float(i))

        assert logger.get_total_records_logged() == 3
        assert logger.is_max_records_reached()
        assert logger.get_records_remaining() == 0
        assert len(logger.log_queue) == 3

    def test_max_records_none_unlimited(self, tmp_path) -> None:
        """Test that max_records=None allows unlimited logging."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path), max_records=None)
        logger.enable_metrics(["test_metric"])

        # Log many events
        for i in range(100):
            logger.log("test_metric", float(i))

        assert logger.get_total_records_logged() == 100
        assert not logger.is_max_records_reached()
        assert logger.get_records_remaining() is None
        assert logger.get_max_records() is None

    def test_max_records_configure_logging_reset(self, tmp_path) -> None:
        """Test that configure_logging resets the records counter."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path), max_records=5)
        logger.enable_metrics(["test_metric"])

        # Log 3 events
        for i in range(3):
            logger.log("test_metric", float(i))

        assert logger.get_total_records_logged() == 3

        # Reconfigure logging - should reset counter
        logger.configure_logging(str(tmp_path), ["test_metric"], max_records=10)

        assert logger.get_total_records_logged() == 0
        assert logger.get_max_records() == 10
        assert logger.get_records_remaining() == 10

        # Log more events
        for i in range(5):
            logger.log("test_metric", float(i))

        assert logger.get_total_records_logged() == 5


@pytest.mark.unit
class TestMicroMetricLoggerSamplingFactor:
    """Test cases for sampling_factor functionality."""

    def test_sampling_factor_initialization(self, tmp_path) -> None:
        """Test initialization with sampling_factor parameter."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path), sampling_factor=0.5)
        logger.enable_metrics(["test_metric"])

        assert logger.get_sampling_factor() == 0.5

    def test_sampling_factor_clamping(self, tmp_path) -> None:
        """Test that sampling_factor is clamped to [0.0, 1.0]."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        # Test values above 1.0
        logger = MicroMetricLogger(log_path=str(tmp_path), sampling_factor=1.5)
        assert logger.get_sampling_factor() == 1.0

        # Reset and test values below 0.0
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False

        logger = MicroMetricLogger(log_path=str(tmp_path), sampling_factor=-0.5)
        assert logger.get_sampling_factor() == 0.0

    def test_sampling_factor_zero_no_logging(self, tmp_path) -> None:
        """Test that sampling_factor=0.0 prevents all logging."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path), sampling_factor=0.0)
        logger.enable_metrics(["test_metric"])

        # Try to log many events - none should be recorded
        for i in range(100):
            logger.log("test_metric", float(i))

        assert logger.get_total_records_logged() == 0
        assert len(logger.log_queue) == 0

    def test_sampling_factor_one_all_logging(self, tmp_path) -> None:
        """Test that sampling_factor=1.0 logs all events."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path), sampling_factor=1.0)
        logger.enable_metrics(["test_metric"])

        # Log events - all should be recorded
        for i in range(10):
            logger.log("test_metric", float(i))

        assert logger.get_total_records_logged() == 10
        assert len(logger.log_queue) == 10

    def test_sampling_factor_partial_logging(self, tmp_path) -> None:
        """Test that sampling_factor reduces logging approximately as expected."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path), sampling_factor=0.5)
        logger.enable_metrics(["test_metric"])

        # Log many events to get statistical significance
        # Set random seed for reproducible test
        import random
        random.seed(42)
        
        num_events = 1000
        for i in range(num_events):
            logger.log("test_metric", float(i))

        # With sampling_factor=0.5, we expect roughly half the events
        # Allow for some variance (±20% of expected)
        expected = num_events * 0.5
        recorded = logger.get_total_records_logged()
        tolerance = expected * 0.2

        assert expected - tolerance <= recorded <= expected + tolerance

    def test_sampling_factor_configure_logging_update(self, tmp_path) -> None:
        """Test that configure_logging can update sampling_factor."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path), sampling_factor=1.0)
        logger.enable_metrics(["test_metric"])

        assert logger.get_sampling_factor() == 1.0

        # Update sampling factor through configure_logging
        logger.configure_logging(str(tmp_path), ["test_metric"], sampling_factor=0.3)

        assert logger.get_sampling_factor() == 0.3


@pytest.mark.unit
class TestMicroMetricLoggerCombinedFeatures:
    """Test cases for max_records and sampling_factor working together."""

    def test_max_records_with_sampling(self, tmp_path) -> None:
        """Test that max_records and sampling_factor work together correctly."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        # Set sampling_factor=1.0 to ensure all events that pass other checks are logged
        logger = MicroMetricLogger(
            log_path=str(tmp_path), 
            max_records=5, 
            sampling_factor=1.0
        )
        logger.enable_metrics(["test_metric"])

        # Log 10 events, but only 5 should be recorded due to max_records
        for i in range(10):
            logger.log("test_metric", float(i))

        assert logger.get_total_records_logged() == 5
        assert logger.is_max_records_reached()
        assert len(logger.log_queue) == 5

    def test_sampling_with_max_records_zero_sampling(self, tmp_path) -> None:
        """Test max_records with sampling_factor=0.0."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(
            log_path=str(tmp_path), 
            max_records=5, 
            sampling_factor=0.0
        )
        logger.enable_metrics(["test_metric"])

        # Try to log events - none should be recorded due to sampling_factor=0.0
        for i in range(10):
            logger.log("test_metric", float(i))

        assert logger.get_total_records_logged() == 0
        assert not logger.is_max_records_reached()  # Never reached because nothing was logged
        assert len(logger.log_queue) == 0


@pytest.mark.unit
class TestMicroMetricLoggerEdgeCases:
    """Test cases for edge cases and error handling."""

    def test_circular_buffer_overflow(self, tmp_path) -> None:
        """Test that circular buffer prevents memory overflow."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["test_metric"])

        # Add more events than maxlen (10000)
        for i in range(10001):
            logger.log("test_metric", float(i))

        # Queue should not exceed maxlen
        assert len(logger.log_queue) <= 10000

    def test_log_with_complex_metadata(self, tmp_path) -> None:
        """Test logging with complex metadata structures."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["test_metric"])

        complex_metadata = {
            "nested": {"key": "value", "list": [1, 2, 3]},
            "array": [{"a": 1}, {"b": 2}],
            "boolean": True,
            "null": None,
        }

        logger.log("test_metric", 42.0, complex_metadata)
        logger.flush()

        # Check that complex metadata was serialized correctly
        log_file = tmp_path / "micro_metrics.jsonl"
        with open(log_file, "r") as f:
            event = json.loads(f.readline())

        assert event["metadata"] == complex_metadata

    def test_log_with_none_value(self, tmp_path) -> None:
        """Test logging with None value."""
        # Reset singleton state and registered metrics
        MicroMetricLogger._instance = None
        MicroMetricLogger._initialized = False
        MicroMetricLogger._registered_metrics.clear()

        MicroMetricLogger.register_metric("test_metric", float)

        logger = MicroMetricLogger(log_path=str(tmp_path))
        logger.enable_metrics(["test_metric"])

        logger.log("test_metric", None, {"note": "null value"})
        logger.flush()

        # Check that None value was handled correctly
        log_file = tmp_path / "micro_metrics.jsonl"
        with open(log_file, "r") as f:
            event = json.loads(f.readline())

        assert event["value"] is None

```

```python
#!/usr/bin/env python3
"""Demonstration of the new MicroMetricLogger features: max_records and sampling_factor."""

import tempfile
from sparse_attention_hub.metric_logging import MicroMetricLogger

def demo_max_records():
    """Demonstrate the max_records functionality."""
    print("=== Demo: max_records functionality ===")
    
    # Reset singleton for clean demo
    MicroMetricLogger._instance = None
    MicroMetricLogger._initialized = False
    MicroMetricLogger._registered_metrics.clear()
    
    # Register a test metric
    MicroMetricLogger.register_metric("demo_metric", float)
    
    with tempfile.TemporaryDirectory() as tmp_dir:
        # Create logger with max_records=5
        logger = MicroMetricLogger(
            log_path=tmp_dir,
            max_records=5,
            enabled_metrics=["demo_metric"]
        )
        
        print(f"Initial state:")
        print(f"  Max records: {logger.get_max_records()}")
        print(f"  Records logged: {logger.get_total_records_logged()}")
        print(f"  Records remaining: {logger.get_records_remaining()}")
        print(f"  Max reached: {logger.is_max_records_reached()}")
        
        # Try to log 8 events
        print(f"\nLogging 8 events...")
        for i in range(8):
            logger.log("demo_metric", float(i), metadata={"event_id": i})
            print(f"  After event {i+1}: {logger.get_total_records_logged()} logged, "
                  f"{logger.get_records_remaining()} remaining")
        
        print(f"\nFinal state:")
        print(f"  Records logged: {logger.get_total_records_logged()}")
        print(f"  Queue size: {len(logger.log_queue)}")
        print(f"  Max reached: {logger.is_max_records_reached()}")


def demo_sampling_factor():
    """Demonstrate the sampling_factor functionality."""
    print("\n=== Demo: sampling_factor functionality ===")
    
    # Reset singleton for clean demo
    MicroMetricLogger._instance = None
    MicroMetricLogger._initialized = False
    MicroMetricLogger._registered_metrics.clear()
    
    # Register a test metric
    MicroMetricLogger.register_metric("demo_metric", float)
    
    with tempfile.TemporaryDirectory() as tmp_dir:
        # Create logger with sampling_factor=0.3 (30% chance)
        logger = MicroMetricLogger(
            log_path=tmp_dir,
            sampling_factor=0.3,
            enabled_metrics=["demo_metric"]
        )
        
        print(f"Sampling factor: {logger.get_sampling_factor()}")
        
        # Try to log 100 events
        print(f"\nLogging 100 events with 30% sampling...")
        for i in range(100):
            logger.log("demo_metric", float(i))
        
        print(f"Events logged: {logger.get_total_records_logged()} out of 100")
        print(f"Actual sampling rate: {logger.get_total_records_logged() / 100:.2%}")


def demo_combined_features():
    """Demonstrate max_records and sampling_factor working together."""
    print("\n=== Demo: Combined features ===")
    
    # Reset singleton for clean demo
    MicroMetricLogger._instance = None
    MicroMetricLogger._initialized = False
    MicroMetricLogger._registered_metrics.clear()
    
    # Register a test metric
    MicroMetricLogger.register_metric("demo_metric", float)
    
    with tempfile.TemporaryDirectory() as tmp_dir:
        # Create logger with both features
        logger = MicroMetricLogger(
            log_path=tmp_dir,
            max_records=10,
            sampling_factor=0.5,  # 50% chance
            enabled_metrics=["demo_metric"]
        )
        
        print(f"Max records: {logger.get_max_records()}")
        print(f"Sampling factor: {logger.get_sampling_factor()}")
        
        # Try to log 100 events
        print(f"\nLogging 100 events with 50% sampling and max_records=10...")
        for i in range(100):
            logger.log("demo_metric", float(i))
            if logger.is_max_records_reached():
                print(f"  Max records reached after trying {i+1} events")
                break
        
        print(f"Final events logged: {logger.get_total_records_logged()}")
        print(f"Queue size: {len(logger.log_queue)}")


def demo_configure_logging_reset():
    """Demonstrate that configure_logging resets the counter."""
    print("\n=== Demo: configure_logging resets counter ===")
    
    # Reset singleton for clean demo
    MicroMetricLogger._instance = None
    MicroMetricLogger._initialized = False
    MicroMetricLogger._registered_metrics.clear()
    
    # Register a test metric
    MicroMetricLogger.register_metric("demo_metric", float)
    
    with tempfile.TemporaryDirectory() as tmp_dir:
        # Create logger
        logger = MicroMetricLogger(
            log_path=tmp_dir,
            max_records=5,
            enabled_metrics=["demo_metric"]
        )
        
        # Log some events
        for i in range(3):
            logger.log("demo_metric", float(i))
        
        print(f"After logging 3 events: {logger.get_total_records_logged()}")
        
        # Reconfigure logging - should reset counter
        logger.configure_logging(tmp_dir, ["demo_metric"], max_records=10)
        
        print(f"After reconfigure: {logger.get_total_records_logged()}")
        print(f"New max records: {logger.get_max_records()}")
        
        # Log more events
        for i in range(5):
            logger.log("demo_metric", float(i))
        
        print(f"After logging 5 more: {logger.get_total_records_logged()}")


if __name__ == "__main__":
    demo_max_records()
    demo_sampling_factor()
    demo_combined_features()
    demo_configure_logging_reset()
    print("\n=== Demo complete! ===")
```

## ✅ Successfully Added Two New Features to MicroMetricLogger

I have successfully implemented both requested features for the MicroMetricLogger:

### 🔢 **1. `max_records` Parameter**
- **Purpose**: Limits the total number of events that can be logged
- **Behavior**: Once the limit is reached, new log events are silently ignored
- **Counter Reset**: The counter resets when `configure_logging()` is called or when a new instance is created

**Usage Examples:**
```python
# Limit to 1000 events maximum
logger = MicroMetricLogger(log_path="./logs", max_records=1000)

# Check status
logger.get_total_records_logged()  # Current count
logger.is_max_records_reached()    # True/False
logger.get_records_remaining()     # How many left (or None if unlimited)
```

### 🎲 **2. `sampling_factor` Parameter** 
- **Purpose**: Probabilistically reduces the number of events logged
- **Range**: 0.0 to 1.0 (automatically clamped)
- **Behavior**: Each event has `sampling_factor` probability of being logged

**Usage Examples:**
```python
# Log approximately 50% of events
logger = MicroMetricLogger(log_path="./logs", sampling_factor=0.5)

# No logging (0% chance)
logger = MicroMetricLogger(log_path="./logs", sampling_factor=0.0)

# Log all events (100% chance - default)
logger = MicroMetricLogger(log_path="./logs", sampling_factor=1.0)
```

### 🔧 **Key Implementation Details**

**Parameter Integration:**
- Both parameters can be set in `__init__()` and updated via `configure_logging()`
- Values are validated (sampling_factor clamped to [0.0, 1.0])
- Backward compatible - all existing code continues to work

**Logic Flow in `log()` method:**
1. Check if logging is configured
2. Check if metric is enabled  
3. **NEW**: Check if `max_records` limit reached → return early if yes
4. **NEW**: Apply `sampling_factor` using `random.random()` → return early if not selected
5. Create and queue the log event
6. Increment the `_total_records_logged` counter
7. Check for auto-flush triggers

**Utility Methods Added:**
```python
logger.get_total_records_logged()  # Current count
logger.is_max_records_reached()    # Boolean check
logger.get_records_remaining()     # Remaining slots (or None)
logger.get_sampling_factor()       # Current sampling rate
logger.get_max_records()           # Current limit (or None)
```

### 📊 **Demo Results**
The demo script successfully showed:
- **max_records=5**: Only 5 out of 8 attempted events were logged
- **sampling_factor=0.3**: 32 out of 100 events logged (32% ≈ 30% expected)
- **Combined features**: With max_records=10 and sampling_factor=0.5, exactly 10 events were logged from ~28 attempts
- **Counter reset**: `configure_logging()` properly resets the counter from 3 to 0

### 🧪 **Comprehensive Testing**
Added 12 new test methods covering:
- ✅ Initialization with new parameters
- ✅ Limits enforcement (`max_records`)  
- ✅ Sampling behavior (`sampling_factor`)
- ✅ Parameter clamping and validation
- ✅ Counter reset functionality
- ✅ Combined feature interaction
- ✅ Edge cases and error handling

All tests pass successfully, ensuring robust and reliable functionality.

### 🔄 **Backward Compatibility**
- All existing code continues to work without changes
- Default values maintain current behavior (`max_records=None`, `sampling_factor=1.0`)
- No breaking changes to the API

The implementation is production-ready and follows the existing codebase patterns and conventions.

```python
# Limit to 1000 events maximum
logger = MicroMetricLogger(log_path="./logs", max_records=1000)

# Check status
logger.get_total_records_logged()  # Current count
logger.is_max_records_reached()    # True/False
logger.get_records_remaining()     # How many left (or None if unlimited)
```

```python
# Log approximately 50% of events
logger = MicroMetricLogger(log_path="./logs", sampling_factor=0.5)

# No logging (0% chance)
logger = MicroMetricLogger(log_path="./logs", sampling_factor=0.0)

# Log all events (100% chance - default)
logger = MicroMetricLogger(log_path="./logs", sampling_factor=1.0)
```

```python
logger.get_total_records_logged()  # Current count
logger.is_max_records_reached()    # Boolean check
logger.get_records_remaining()     # Remaining slots (or None)
logger.get_sampling_factor()       # Current sampling rate
logger.get_max_records()           # Current limit (or None)
```

---

**User**

why is this needed?

---

**Cursor**

```python
# Update parameters if they differ from current values
if max_records is not None and hasattr(self, 'max_records'):
    self.max_records = max_records
if sampling_factor != 1.0 and hasattr(self, 'sampling_factor'):
    self.sampling_factor = max(0.0, min(1.0, sampling_factor))
```

```python
# First instance
logger1 = MicroMetricLogger(max_records=100, sampling_factor=0.5)

# Later, someone tries to create another instance with different parameters
logger2 = MicroMetricLogger(max_records=200, sampling_factor=0.8)
```

Looking at that code block, you're right to question it! This code appears to be **unnecessary and potentially problematic**. Let me explain why:

## This Code Block is Problematic and Should Be Removed

You're absolutely right to question this! This code block has several issues:

### **Problems with the Current Implementation:**

1. **Inconsistent Behavior**: It only updates `max_records` and `sampling_factor`, but ignores other parameters like `flush_every`, `flush_interval`, etc.

2. **Arbitrary Conditions**: 
   - `max_records is not None` - but what if someone wants to explicitly set it to `None` to remove the limit?
   - `sampling_factor != 1.0` - why only update when it's not the default value?

3. **Violates Singleton Pattern**: Typically, singletons should either:
   - Completely ignore subsequent initialization attempts, OR
   - Consistently update all parameters

4. **Unnecessary `hasattr` Checks**: After first initialization, these attributes will always exist

5. **Confusing User Experience**: Users might think they're creating a new instance but parameters partially change

6. **No Counter Reset**: Unlike `configure_logging()`, this doesn't reset `_total_records_logged`

### **Better Approach:**

```python
"""MicroMetricLogger implementation for sparse attention hub."""

import inspect
import json
import os
import random
import time
from collections import deque
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional, Union


@dataclass
class LogEvent:
    """Log event data structure."""

    timestamp: datetime
    metric: str  # Metric identifier string
    value: Union[None, Any]
    metadata: Dict[str, Any]  # Additional context (layer, head, etc.)
    location: str  # Auto-inferred: "module.function" or "class.method"


class MicroMetricLogger:
    """Singleton logger for micro metrics with queue-based architecture."""

    _instance: Optional["MicroMetricLogger"] = None
    _initialized: bool = False

    # Class-level storage for registered metrics (works without initialization)
    _registered_metrics: Dict[str, type] = {}  # identifier -> dtype mapping

    def __new__(cls, *args, **kwargs) -> "MicroMetricLogger":
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(
        self,
        log_path: Optional[str] = None,
        flush_every: int = 1000,  # Flush every N events
        flush_interval: float = 60.0,  # Flush every N seconds
        enabled_metrics: Union[List[str], str] = None,
        max_records: Optional[int] = None,  # Maximum number of events to log
        sampling_factor: float = 1.0,  # Probability of logging each event (0.0-1.0)
    ):  # List of string identifiers to enable, or "all"
        if not self._initialized:
            self.log_path = log_path
            self.flush_every = flush_every
            self.flush_interval = flush_interval
            self.max_records = max_records
            self.sampling_factor = max(0.0, min(1.0, sampling_factor))  # Clamp to [0.0, 1.0]

            # Internal state
            self.log_queue: deque = deque(maxlen=10000)  # Circular buffer
            self.enabled_metrics: set = set()
            self.last_flush_time = time.time()
            self._total_records_logged: int = 0  # Track total events logged

            # Enable metrics if log_path is provided
            if self.log_path is not None:
                self._ensure_log_directory()
                self.enable_metrics(enabled_metrics)

            MicroMetricLogger._initialized = True
        else:
            # Singleton already initialized - warn if different parameters are provided
            if self.log_path and log_path and self.log_path != log_path:
                print(
                    f"Warning: MicroMetricLogger already initialized with log_path: {self.log_path}"
                )
            if max_records is not None and max_records != getattr(self, 'max_records', None):
                print(
                    f"Warning: MicroMetricLogger already initialized with max_records: {self.max_records}. "
                    f"Use configure_logging() to update parameters."
                )
            if sampling_factor != 1.0 and sampling_factor != getattr(self, 'sampling_factor', 1.0):
                print(
                    f"Warning: MicroMetricLogger already initialized with sampling_factor: {self.sampling_factor}. "
                    f"Use configure_logging() to update parameters."
                )

    # main registration function

    @classmethod
    def register_metric(cls, identifier: str, dtype: type) -> None:
        """Register a metric with its string identifier and expected data type.

        This works at class level and doesn't require initialization.
        """
        if identifier in cls._registered_metrics:
            print(f"Warning: Metric '{identifier}' is being re-registered")
        cls._registered_metrics[identifier] = dtype

    @classmethod
    def get_registered_metrics(cls) -> Dict[str, type]:
        """Get all registered metrics at class level."""
        return cls._registered_metrics.copy()

    # helper methods

    def _ensure_log_directory(self) -> None:
        """Ensure the log directory exists."""
        if self.log_path is not None:
            os.makedirs(self.log_path, exist_ok=True)

    def _get_calling_location(self) -> str:
        """Get the calling location using inspect module."""
        try:
            # Get the calling frame (skip this method and the log method)
            caller_frame = inspect.currentframe().f_back.f_back
            if caller_frame is None:
                return "unknown"

            # Get module name
            module = inspect.getmodule(caller_frame)
            module_name = module.__name__ if module else "unknown"

            # Get function/class name
            function_name = caller_frame.f_code.co_name

            # Try to get class name if it's a method
            class_name = None
            if "self" in caller_frame.f_locals:
                class_name = caller_frame.f_locals["self"].__class__.__name__

            if class_name:
                return f"{module_name}.{class_name}.{function_name}"
            else:
                return f"{module_name}.{function_name}"
        except Exception:
            return "unknown"

    def __del__(self):
        """Cleanup when logger is destroyed."""
        self.flush()  # Final flush

    # api

    def enable_metrics(self, metrics: Union[List[str], str] = None) -> None:
        """Enable logging for specific metrics.

        Args:
            metrics: List of metric identifiers to enable, or "all" for all registered metrics.
                    If None, enables no metrics (empty list).
        """
        if metrics == "all":
            self.enabled_metrics = set(self._registered_metrics.keys())
        elif isinstance(metrics, (list, set)):
            # Only enable metrics that are registered
            valid_metrics = set(metrics) & set(self._registered_metrics.keys())
            invalid_metrics = set(metrics) - set(self._registered_metrics.keys())
            if invalid_metrics:
                print(
                    f"Warning: Attempting to enable unregistered metrics: {invalid_metrics}"
                )
            self.enabled_metrics = valid_metrics
        else:
            # Default to empty set
            self.enabled_metrics = set()

    def log(self, identifier: str, value: Any, metadata: Dict[str, Any] = None) -> None:
        """Log a metric value with optional metadata. Location is auto-inferred.

        This only works if log_path is defined.
        """
        # Check if logging is configured
        if self.log_path is None:
            print(
                f"Warning: Cannot log metric '{identifier}' - log_path not defined. Use configure_logging() first."
            )
            return

        # Check if metric is enabled
        if identifier not in self.enabled_metrics:
            print(
                f"Warning: Attempting to log metric '{identifier}' which is not enabled"
            )
            return

        # Check if max_records limit has been reached
        if self.max_records is not None and self._total_records_logged >= self.max_records:
            return  # Silently ignore - we've hit the limit

        # Apply sampling factor
        if self.sampling_factor < 1.0 and random.random() > self.sampling_factor:
            return  # Event not selected for logging due to sampling

        # Create log event
        event = LogEvent(
            timestamp=datetime.now(),
            metric=identifier,
            value=value,
            metadata=metadata or {},
            location=self._get_calling_location(),
        )

        # Add to queue and increment counter
        self.log_queue.append(event)
        self._total_records_logged += 1

        # Check if we should flush
        if len(self.log_queue) >= self.flush_every:
            self.flush()

    def configure_logging(
        self,
        log_path: str,
        enabled_metrics: Union[List[str], str] = None,
        max_records: Optional[int] = None,
        sampling_factor: Optional[float] = None,
    ) -> None:
        """Configure logging with a log path and optionally enable metrics.

        This must be called before logging can work. Resets the total records counter.
        """
        self.log_path = log_path
        self._ensure_log_directory()
        self.enable_metrics(enabled_metrics)
        
        # Update limits if provided
        if max_records is not None:
            self.max_records = max_records
        if sampling_factor is not None:
            self.sampling_factor = max(0.0, min(1.0, sampling_factor))
        
        # Reset the total records counter when reconfiguring
        self._total_records_logged = 0

    def flush(self) -> None:
        """Force flush the current queue to disk."""
        if not self.log_queue or self.log_path is None:
            return

        # Get current timestamp for filename
        filename = "micro_metrics.jsonl"
        filepath = os.path.join(self.log_path, filename)

        # Write events to file
        with open(filepath, "a", encoding="utf-8") as f:
            while self.log_queue:
                event = self.log_queue.popleft()
                # Convert dataclass to dict and serialize
                event_dict = asdict(event)
                # Convert datetime to ISO format string
                event_dict["timestamp"] = event_dict["timestamp"].isoformat()
                f.write(json.dumps(event_dict) + "\n")

        self.last_flush_time = time.time()

    def is_metric_enabled(self, identifier: str) -> bool:
        """Check if a specific metric is requested for logging."""
        return identifier in self.enabled_metrics

    def get_enabled_metrics(self) -> set:
        """Get currently enabled metrics."""
        return self.enabled_metrics.copy()

    def is_logging_configured(self) -> bool:
        """Check if logging is configured (log_path is set)."""
        return self.log_path is not None

    def get_total_records_logged(self) -> int:
        """Get the total number of records logged since initialization or last configure_logging call."""
        return getattr(self, '_total_records_logged', 0)

    def is_max_records_reached(self) -> bool:
        """Check if the maximum number of records has been reached."""
        if self.max_records is None:
            return False
        return self.get_total_records_logged() >= self.max_records

    def get_records_remaining(self) -> Optional[int]:
        """Get the number of records remaining before hitting max_records limit.
        
        Returns:
            Number of records remaining, or None if no limit is set.
        """
        if self.max_records is None:
            return None
        return max(0, self.max_records - self.get_total_records_logged())

    def get_sampling_factor(self) -> float:
        """Get the current sampling factor."""
        return getattr(self, 'sampling_factor', 1.0)

    def get_max_records(self) -> Optional[int]:
        """Get the current max_records limit."""
        return getattr(self, 'max_records', None)

```

```python
#!/usr/bin/env python3
"""Test to demonstrate the improved singleton behavior."""

import tempfile
from sparse_attention_hub.metric_logging import MicroMetricLogger

def test_singleton_behavior():
    """Test that demonstrates the improved singleton behavior."""
    
    # Reset singleton for clean test
    MicroMetricLogger._instance = None
    MicroMetricLogger._initialized = False
    MicroMetricLogger._registered_metrics.clear()
    
    # Register a test metric
    MicroMetricLogger.register_metric("test_metric", float)
    
    with tempfile.TemporaryDirectory() as tmp_dir:
        print("=== Testing Singleton Behavior ===")
        
        # First instance
        print("1. Creating first instance with max_records=5, sampling_factor=0.5")
        logger1 = MicroMetricLogger(
            log_path=tmp_dir,
            max_records=5,
            sampling_factor=0.5,
            enabled_metrics=["test_metric"]
        )
        
        print(f"   logger1.max_records: {logger1.get_max_records()}")
        print(f"   logger1.sampling_factor: {logger1.get_sampling_factor()}")
        
        # Attempt to create "second" instance with different parameters
        print("\n2. Attempting to create 'second' instance with max_records=10, sampling_factor=0.8")
        logger2 = MicroMetricLogger(
            log_path=tmp_dir,
            max_records=10,  # Different value
            sampling_factor=0.8,  # Different value
        )
        
        print(f"   logger2.max_records: {logger2.get_max_records()}")
        print(f"   logger2.sampling_factor: {logger2.get_sampling_factor()}")
        
        # Verify they're the same instance
        print(f"\n3. Are they the same instance? {logger1 is logger2}")
        print(f"   Parameters unchanged (as expected for singleton)")
        
        # Show the proper way to update parameters
        print("\n4. Proper way to update parameters using configure_logging():")
        logger1.configure_logging(
            tmp_dir,
            ["test_metric"],
            max_records=10,
            sampling_factor=0.8
        )
        
        print(f"   After configure_logging:")
        print(f"   max_records: {logger1.get_max_records()}")
        print(f"   sampling_factor: {logger1.get_sampling_factor()}")
        print(f"   total_records_logged: {logger1.get_total_records_logged()} (reset)")

if __name__ == "__main__":
    test_singleton_behavior()
    print("\n=== Test complete! ===")
```

---

**User**

you should just remove this block in my opinion. why do we need it at all

---

**Cursor**

You're absolutely right! That entire block is unnecessary and goes against proper singleton pattern principles. Let me remove it completely.

```python
"""MicroMetricLogger implementation for sparse attention hub."""

import inspect
import json
import os
import random
import time
from collections import deque
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional, Union


@dataclass
class LogEvent:
    """Log event data structure."""

    timestamp: datetime
    metric: str  # Metric identifier string
    value: Union[None, Any]
    metadata: Dict[str, Any]  # Additional context (layer, head, etc.)
    location: str  # Auto-inferred: "module.function" or "class.method"


class MicroMetricLogger:
    """Singleton logger for micro metrics with queue-based architecture."""

    _instance: Optional["MicroMetricLogger"] = None
    _initialized: bool = False

    # Class-level storage for registered metrics (works without initialization)
    _registered_metrics: Dict[str, type] = {}  # identifier -> dtype mapping

    def __new__(cls, *args, **kwargs) -> "MicroMetricLogger":
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(
        self,
        log_path: Optional[str] = None,
        flush_every: int = 1000,  # Flush every N events
        flush_interval: float = 60.0,  # Flush every N seconds
        enabled_metrics: Union[List[str], str] = None,
        max_records: Optional[int] = None,  # Maximum number of events to log
        sampling_factor: float = 1.0,  # Probability of logging each event (0.0-1.0)
    ):  # List of string identifiers to enable, or "all"
        if not self._initialized:
            self.log_path = log_path
            self.flush_every = flush_every
            self.flush_interval = flush_interval
            self.max_records = max_records
            self.sampling_factor = max(0.0, min(1.0, sampling_factor))  # Clamp to [0.0, 1.0]

            # Internal state
            self.log_queue: deque = deque(maxlen=10000)  # Circular buffer
            self.enabled_metrics: set = set()
            self.last_flush_time = time.time()
            self._total_records_logged: int = 0  # Track total events logged

            # Enable metrics if log_path is provided
            if self.log_path is not None:
                self._ensure_log_directory()
                self.enable_metrics(enabled_metrics)

            MicroMetricLogger._initialized = True
        # Singleton already initialized - subsequent __init__ calls are ignored

    # main registration function

    @classmethod
    def register_metric(cls, identifier: str, dtype: type) -> None:
        """Register a metric with its string identifier and expected data type.

        This works at class level and doesn't require initialization.
        """
        if identifier in cls._registered_metrics:
            print(f"Warning: Metric '{identifier}' is being re-registered")
        cls._registered_metrics[identifier] = dtype

    @classmethod
    def get_registered_metrics(cls) -> Dict[str, type]:
        """Get all registered metrics at class level."""
        return cls._registered_metrics.copy()

    # helper methods

    def _ensure_log_directory(self) -> None:
        """Ensure the log directory exists."""
        if self.log_path is not None:
            os.makedirs(self.log_path, exist_ok=True)

    def _get_calling_location(self) -> str:
        """Get the calling location using inspect module."""
        try:
            # Get the calling frame (skip this method and the log method)
            caller_frame = inspect.currentframe().f_back.f_back
            if caller_frame is None:
                return "unknown"

            # Get module name
            module = inspect.getmodule(caller_frame)
            module_name = module.__name__ if module else "unknown"

            # Get function/class name
            function_name = caller_frame.f_code.co_name

            # Try to get class name if it's a method
            class_name = None
            if "self" in caller_frame.f_locals:
                class_name = caller_frame.f_locals["self"].__class__.__name__

            if class_name:
                return f"{module_name}.{class_name}.{function_name}"
            else:
                return f"{module_name}.{function_name}"
        except Exception:
            return "unknown"

    def __del__(self):
        """Cleanup when logger is destroyed."""
        self.flush()  # Final flush

    # api

    def enable_metrics(self, metrics: Union[List[str], str] = None) -> None:
        """Enable logging for specific metrics.

        Args:
            metrics: List of metric identifiers to enable, or "all" for all registered metrics.
                    If None, enables no metrics (empty list).
        """
        if metrics == "all":
            self.enabled_metrics = set(self._registered_metrics.keys())
        elif isinstance(metrics, (list, set)):
            # Only enable metrics that are registered
            valid_metrics = set(metrics) & set(self._registered_metrics.keys())
            invalid_metrics = set(metrics) - set(self._registered_metrics.keys())
            if invalid_metrics:
                print(
                    f"Warning: Attempting to enable unregistered metrics: {invalid_metrics}"
                )
            self.enabled_metrics = valid_metrics
        else:
            # Default to empty set
            self.enabled_metrics = set()

    def log(self, identifier: str, value: Any, metadata: Dict[str, Any] = None) -> None:
        """Log a metric value with optional metadata. Location is auto-inferred.

        This only works if log_path is defined.
        """
        # Check if logging is configured
        if self.log_path is None:
            print(
                f"Warning: Cannot log metric '{identifier}' - log_path not defined. Use configure_logging() first."
            )
            return

        # Check if metric is enabled
        if identifier not in self.enabled_metrics:
            print(
                f"Warning: Attempting to log metric '{identifier}' which is not enabled"
            )
            return

        # Check if max_records limit has been reached
        if self.max_records is not None and self._total_records_logged >= self.max_records:
            return  # Silently ignore - we've hit the limit

        # Apply sampling factor
        if self.sampling_factor < 1.0 and random.random() > self.sampling_factor:
            return  # Event not selected for logging due to sampling

        # Create log event
        event = LogEvent(
            timestamp=datetime.now(),
            metric=identifier,
            value=value,
            metadata=metadata or {},
            location=self._get_calling_location(),
        )

        # Add to queue and increment counter
        self.log_queue.append(event)
        self._total_records_logged += 1

        # Check if we should flush
        if len(self.log_queue) >= self.flush_every:
            self.flush()

    def configure_logging(
        self,
        log_path: str,
        enabled_metrics: Union[List[str], str] = None,
        max_records: Optional[int] = None,
        sampling_factor: Optional[float] = None,
    ) -> None:
        """Configure logging with a log path and optionally enable metrics.

        This must be called before logging can work. Resets the total records counter.
        """
        self.log_path = log_path
        self._ensure_log_directory()
        self.enable_metrics(enabled_metrics)
        
        # Update limits if provided
        if max_records is not None:
            self.max_records = max_records
        if sampling_factor is not None:
            self.sampling_factor = max(0.0, min(1.0, sampling_factor))
        
        # Reset the total records counter when reconfiguring
        self._total_records_logged = 0

    def flush(self) -> None:
        """Force flush the current queue to disk."""
        if not self.log_queue or self.log_path is None:
            return

        # Get current timestamp for filename
        filename = "micro_metrics.jsonl"
        filepath = os.path.join(self.log_path, filename)

        # Write events to file
        with open(filepath, "a", encoding="utf-8") as f:
            while self.log_queue:
                event = self.log_queue.popleft()
                # Convert dataclass to dict and serialize
                event_dict = asdict(event)
                # Convert datetime to ISO format string
                event_dict["timestamp"] = event_dict["timestamp"].isoformat()
                f.write(json.dumps(event_dict) + "\n")

        self.last_flush_time = time.time()

    def is_metric_enabled(self, identifier: str) -> bool:
        """Check if a specific metric is requested for logging."""
        return identifier in self.enabled_metrics

    def get_enabled_metrics(self) -> set:
        """Get currently enabled metrics."""
        return self.enabled_metrics.copy()

    def is_logging_configured(self) -> bool:
        """Check if logging is configured (log_path is set)."""
        return self.log_path is not None

    def get_total_records_logged(self) -> int:
        """Get the total number of records logged since initialization or last configure_logging call."""
        return getattr(self, '_total_records_logged', 0)

    def is_max_records_reached(self) -> bool:
        """Check if the maximum number of records has been reached."""
        if self.max_records is None:
            return False
        return self.get_total_records_logged() >= self.max_records

    def get_records_remaining(self) -> Optional[int]:
        """Get the number of records remaining before hitting max_records limit.
        
        Returns:
            Number of records remaining, or None if no limit is set.
        """
        if self.max_records is None:
            return None
        return max(0, self.max_records - self.get_total_records_logged())

    def get_sampling_factor(self) -> float:
        """Get the current sampling factor."""
        return getattr(self, 'sampling_factor', 1.0)

    def get_max_records(self) -> Optional[int]:
        """Get the current max_records limit."""
        return getattr(self, 'max_records', None)

```

---

**User**

list changes i made jut now 

---

**Cursor**

Based on the diff shown in the attached files, here are the changes you just made to `sparse_attention_hub/metric_logging/logger.py`:

## **Changes Made:**

### **1. Restored the `else` block in `__init__`**
```python
# BEFORE (my change):
# Singleton already initialized - subsequent __init__ calls are ignored

# AFTER (your change):
else:
    if self.log_path and log_path and self.log_path != log_path:
        print(
            f"Warning: MicroMetricLogger already initialized with log_path: {self.log_path}"
        )
```
- You restored the warning for when someone tries to initialize with a different `log_path`
- You removed my overly simplified comment

### **2. Changed `sampling_factor` parameter in `configure_logging()`**
```python
# BEFORE:
sampling_factor: Optional[float] = None,

# AFTER:
sampling_factor: float = 1.0,
```
- Changed from optional parameter with `None` default to required parameter with `1.0` default

### **3. Simplified the parameter assignment logic in `configure_logging()`**
```python
# BEFORE:
if max_records is not None:
    self.max_records = max_records
if sampling_factor is not None:
    self.sampling_factor = max(0.0, min(1.0, sampling_factor))

# AFTER:
self.max_records = max_records
self.sampling_factor = max(0.0, min(1.0, sampling_factor))
```
- Removed the conditional checks
- Always assigns the values directly

## **Summary:**
You kept the essential log_path warning (which makes sense for user feedback), but simplified the `configure_logging()` method to be more straightforward by always setting the parameters rather than conditionally checking for `None` values.

```python
# BEFORE (my change):
# Singleton already initialized - subsequent __init__ calls are ignored

# AFTER (your change):
else:
    if self.log_path and log_path and self.log_path != log_path:
        print(
            f"Warning: MicroMetricLogger already initialized with log_path: {self.log_path}"
        )
```

```python
# BEFORE:
sampling_factor: Optional[float] = None,

# AFTER:
sampling_factor: float = 1.0,
```

```python
# BEFORE:
if max_records is not None:
    self.max_records = max_records
if sampling_factor is not None:
    self.sampling_factor = max(0.0, min(1.0, sampling_factor))

# AFTER:
self.max_records = max_records
self.sampling_factor = max(0.0, min(1.0, sampling_factor))
```

---

**User**

where are these methods used?

---

**Cursor**

Let me search for where these utility methods are used in the codebase:

Based on my search, here's where these utility methods are currently used:

## **Current Usage:**

### **1. In Tests** (`tests/unit/metric_logging/test_micro_metric_logger.py`)
- **Extensively used** across 12 test classes to verify functionality
- Used to assert expected behavior after logging operations
- Examples:
  ```python
  assert logger.get_total_records_logged() == 5
  assert logger.is_max_records_reached()
  assert logger.get_records_remaining() == 0
  ```

### **2. In Demo Script** (`test_singleton_behavior.py`)
- Used to demonstrate the new functionality
- Shows current state and parameter values to users

### **3. Internal Usage Within Logger**
- `get_total_records_logged()` is used by:
  - `is_max_records_reached()` method
  - `get_records_remaining()` method

## **⚠️ Inconsistency Found:**

**In the `log()` method**, there's a direct check instead of using the utility method:
```python
# Current implementation (line 172):
if self.max_records is not None and self._total_records_logged >= self.max_records:

# Could be simplified to:
if self.is_max_records_reached():
```

## **Methods NOT Used Internally:**
- `is_max_records_reached()` - **Could be used in the `log()` method**
- `get_records_remaining()` - Only used for external querying
- `get_sampling_factor()` - Only used for external querying  
- `get_max_records()` - Only used for external querying

## **Summary:**
These methods are primarily **public API methods** for:
1. **Testing** - Verifying internal state
2. **Debugging** - Checking current configuration and progress
3. **Monitoring** - Tracking logging behavior in applications

They serve as a **clean interface** for external code to inspect the logger's state without accessing private attributes directly.

```python
  assert logger.get_total_records_logged() == 5
  assert logger.is_max_records_reached()
  assert logger.get_records_remaining() == 0
```

```python
# Current implementation (line 172):
if self.max_records is not None and self._total_records_logged >= self.max_records:

# Could be simplified to:
if self.is_max_records_reached():
```

---

**User**

Find places where micrometric logger is initialized / configured

---

**Cursor**

Based on my search through the codebase, here are the places where **MicroMetricLogger is initialized/configured**:

## **🏭 Production Usage (Real Code)**

### **1. Benchmark Executor** (`benchmark/executor.py`)
```python
# Line 237-238
metric_logger = MicroMetricLogger()
metric_logger.configure_logging(
    log_path=stub.result_dir, 
    enabled_metrics=["research_attention_density", "research_attention_output_error"]
)
```
- **Context**: Main benchmark execution pipeline
- **Purpose**: Per-benchmark metric collection
- **Configuration**: Enables specific research attention metrics

### **2. Magic Pig Experiments** (`benchmark/scripts/magic_pig_experiments/single_benchmark_model_example_magicpig.py`)
```python
# Line 164-171
metric_logger = MicroMetricLogger()
metric_logger.configure_logging(
    log_path=result_dir,
    enabled_metrics=[
        "research_attention_density",
        "research_attention_output_error",
    ],
)
```
- **Context**: Standalone benchmark script
- **Purpose**: Advanced benchmarking with system monitoring
- **Configuration**: Same metrics as main executor

### **3. Research Attention Usage** (`sparse_attention_hub/sparse_attention/research_attention/base.py`)
```python
# Line 115-116, 153
if MicroMetricLogger().is_metric_enabled("research_attention_density"):
    MicroMetricLogger().log(...)

if MicroMetricLogger().is_metric_enabled("research_attention_output_error"):
    MicroMetricLogger().log(...)
```
- **Context**: Core sparse attention implementation
- **Purpose**: Conditional logging during attention computation
- **Pattern**: Uses singleton instance without explicit initialization

## **📋 Initialization Patterns Found**

### **Pattern 1: Direct Initialization + Configuration**
```python
metric_logger = MicroMetricLogger()
metric_logger.configure_logging(log_path="...", enabled_metrics=[...])
```
- Used in benchmark scripts
- Most common production pattern

### **Pattern 2: Singleton Usage (No Explicit Init)**
```python
MicroMetricLogger().log(...)  # Uses existing singleton
```
- Used in attention mechanisms
- Assumes already configured elsewhere

### **Pattern 3: Class-Level Registration**
```python
# Line 20-21 in base.py
MicroMetricLogger.register_metric("research_attention_density", float)
MicroMetricLogger.register_metric("research_attention_output_error", float)
```
- Happens at module import time
- Defines available metrics

## **🔍 Key Observations**

### **Centralized Configuration**
- **Main configuration happens in benchmark executors**
- **Core attention code assumes logger is already configured**
- **No global configuration file or initialization script**

### **Consistent Metric Set**
Both production sites use the same metrics:
- `"research_attention_density"`
- `"research_attention_output_error"`

### **Lazy Initialization Pattern**
- Logger is configured just before benchmarks run
- Attention code uses conditional logging (`is_metric_enabled()`)
- No upfront global setup

### **Test vs Production**
- **Tests**: Many different initialization combinations for testing
- **Production**: Consistent pattern of `MicroMetricLogger() + configure_logging()`

### **Import Sources**
- **Production**: `from sparse_attention_hub.metric_logging.logger import MicroMetricLogger`
- **Tests**: `from sparse_attention_hub.metric_logging import MicroMetricLogger`

The design follows a **"configure once, use anywhere"** pattern where benchmark runners configure the logger, and attention mechanisms use it conditionally.

```python
# Line 237-238
metric_logger = MicroMetricLogger()
metric_logger.configure_logging(
    log_path=stub.result_dir, 
    enabled_metrics=["research_attention_density", "research_attention_output_error"]
)
```

```python
# Line 164-171
metric_logger = MicroMetricLogger()
metric_logger.configure_logging(
    log_path=result_dir,
    enabled_metrics=[
        "research_attention_density",
        "research_attention_output_error",
    ],
)
```

```python
# Line 115-116, 153
if MicroMetricLogger().is_metric_enabled("research_attention_density"):
    MicroMetricLogger().log(...)

if MicroMetricLogger().is_metric_enabled("research_attention_output_error"):
    MicroMetricLogger().log(...)
```

```python
metric_logger = MicroMetricLogger()
metric_logger.configure_logging(log_path="...", enabled_metrics=[...])
```

```python
MicroMetricLogger().log(...)  # Uses existing singleton
```

```python
# Line 20-21 in base.py
MicroMetricLogger.register_metric("research_attention_density", float)
MicroMetricLogger.register_metric("research_attention_output_error", float)
```

---

**User**

want to make one more change

---

**Cursor**

Sure! What change would you like to make? I can see you have the `benchmark/executor.py` file open at line 240. Are you looking to modify how the MicroMetricLogger is used there, or is it something else entirely?

---

**User**

Make these parameters propogate from executor object and let them be parameters for executor object instantiation

---

**Cursor**

```python
metric_logger = MicroMetricLogger()
metric_logger.configure_logging(log_path=stub.result_dir, enabled_metrics=["research_attention_density", "research_attention_output_error"])
```

I understand! You want the `max_records` and `sampling_factor` parameters to be configurable at the executor level and then passed down to the MicroMetricLogger. Let me examine the executor structure and make these parameters propagate from the executor object instantiation.

```python
"""BenchmarkExecutor: Parallel GPU execution for benchmark matrix experiments.

This module contains the main BenchmarkExecutor class that orchestrates parallel 
execution of benchmark experiments across the model × sparse_attention_config × 
benchmark × subset matrix with dynamic GPU allocation and resumability.
"""

import logging
import multiprocessing
import os
import signal
import sys
import time
import traceback
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from queue import Empty
from contextlib import contextmanager
from sparse_attention_hub.metric_logging.logger import MicroMetricLogger

# Set multiprocessing start method to 'spawn' for CUDA compatibility
if multiprocessing.get_start_method(allow_none=True) != 'spawn':
    multiprocessing.set_start_method('spawn', force=True)

from sparse_attention_hub.sparse_attention.base import SparseAttentionConfig

from .executor_config import (
    AdapterConfig,
    BenchmarkConfig,
    BenchmarkStub,
    ExecutionResults,
    ExecutionProgress,
    BenchmarkResult,
    BenchmarkFailure,
    filter_existing_results,
    generate_benchmark_stubs,

)
from .benchmark_registry import (
    ensure_benchmarks_loaded,
    create_benchmark_instance
)

from .utils.gpu import (
    cleanup_gpu_memory,
    release_gpu_to_pool,
    acquire_gpu_from_pool,
    create_gpu_pool,
    validate_gpu_availability,
)

# Global flag for graceful shutdown
_shutdown_requested = False


@contextmanager
def set_gpu(gpu_id: int):
    """Context manager to set the current CUDA device.
    
    Args:
        gpu_id: The GPU device ID to set as current device
        
    Yields:
        None: The context manager yields control to the with block
    """
    import torch
    torch.cuda.set_device(gpu_id)
    yield


def _signal_handler(signum: int, frame: Any) -> None:
    """Handle shutdown signals gracefully."""
    global _shutdown_requested
    _shutdown_requested = True
    logging.getLogger(__name__).warning(f"Received signal {signum}, initiating graceful shutdown...")


def _categorize_error(error: Exception, context: str = "") -> str:
    """Categorize errors for better handling and reporting.
    
    Args:
        error: The exception that occurred
        context: Additional context about where the error occurred
        
    Returns:
        Error category string for classification
    """
    error_type = type(error).__name__
    error_msg = str(error).lower()
    
    # GPU-related errors
    if any(keyword in error_msg for keyword in ["cuda", "gpu", "out of memory", "oom"]):
        if "out of memory" in error_msg or "oom" in error_msg:
            return "GPU_OUT_OF_MEMORY"
        elif "cuda" in error_msg:
            return "GPU_CUDA_ERROR"
        else:
            return "GPU_ERROR"
    
    # Model loading errors
    elif any(keyword in error_msg for keyword in ["model", "huggingface", "transformers", "pretrained"]):
        if "not found" in error_msg or "doesn't exist" in error_msg:
            return "MODEL_NOT_FOUND"
        elif "tokenizer" in error_msg:
            return "TOKENIZER_ERROR"
        else:
            return "MODEL_LOADING_ERROR"
    
    # Dataset/benchmark errors
    elif any(keyword in error_msg for keyword in ["dataset", "benchmark", "subset"]):
        return "DATASET_ERROR"
    
    # Network/connection errors
    elif any(keyword in error_msg for keyword in ["connection", "timeout", "network", "http"]):
        return "NETWORK_ERROR"
    
    # File system errors
    elif any(keyword in error_msg for keyword in ["file", "directory", "permission", "disk"]):
        return "FILESYSTEM_ERROR"
    
    # Multiprocessing errors
    elif any(keyword in error_msg for keyword in ["queue", "process", "multiprocessing"]):
        return "MULTIPROCESSING_ERROR"
    
    # Default categories
    elif isinstance(error, TimeoutError):
        return "TIMEOUT_ERROR"
    elif isinstance(error, KeyboardInterrupt):
        return "INTERRUPT_ERROR"
    elif isinstance(error, MemoryError):
        return "SYSTEM_MEMORY_ERROR"
    else:
        return "UNKNOWN_ERROR"


def _benchmark_worker(
    stub_queue: multiprocessing.Queue,
    gpu_pool: multiprocessing.Queue,
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue,
    timeout_per_benchmark: float = 3600.0
) -> None:
    """Worker function to execute benchmarks with dynamic GPU allocation.
    
    This function runs in a separate process and handles:
    1. Acquiring GPUs from the shared pool
    2. Creating model adapters with sparse attention configurations
    3. Executing benchmarks with proper error handling
    4. Releasing GPUs back to the pool
    5. Reporting results and errors
    
    Args:
        stub_queue: Queue containing BenchmarkStub instances to process
        gpu_pool: Shared queue containing available GPU IDs
        result_queue: Queue for reporting successful benchmark results
        error_queue: Queue for reporting benchmark failures
        timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
    """
    worker_id = os.getpid()
    logger = logging.getLogger(f"{__name__}.worker_{worker_id}")
    
    # Set up signal handlers for graceful shutdown
    signal.signal(signal.SIGTERM, _signal_handler)
    signal.signal(signal.SIGINT, _signal_handler)
    
    # Track resources for cleanup
    current_gpu_id: Optional[int] = None
    adapter = None

    try:
        while True:
            # Check for shutdown signal
            if _shutdown_requested:
                logger.info(f"Worker {worker_id}: Shutdown requested, exiting")
                break
            
            # Step 1: Get next benchmark stub from queue
            try:
                stub: BenchmarkStub = stub_queue.get(timeout=1.0)  # 1 second timeout
            except Empty:
                # No more work to do
                logger.info(f"Worker {worker_id}: No more work, shutting down")
                break
            
            # Check for sentinel value (shutdown signal)
            if stub is None:
                logger.info(f"Worker {worker_id}: Received shutdown signal, exiting")
                break
            
            # Step 2: Acquire GPU from pool
            try:
                current_gpu_id = acquire_gpu_from_pool(gpu_pool, timeout=30.0)
                logger.info(f"Worker {worker_id}: Acquired GPU {current_gpu_id} for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}")
            except Exception as e:
                error_category = _categorize_error(e, "GPU acquisition")
                error_msg = f"Failed to acquire GPU for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                error_queue.put(BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=0.0
                ))
                continue

            # Step 3: Run the benchmark on the current GPU
            start_time = time.time()
            execution_time = 0.0  # Initialize execution_time
            
            try:
                with set_gpu(current_gpu_id):
                    logger.debug(f"Worker {worker_id}: Set CUDA device to {current_gpu_id}")
                    execution_success = False

                    # Create model adapter with sparse attention config
                    logger.info(f"Worker {worker_id}: Creating model adapter for {stub.model_name}")
                    
                    # Import here to avoid issues with multiprocessing
                    from sparse_attention_hub.adapters.huggingface import ModelAdapterHF
                    
                    adapter = ModelAdapterHF(
                        model_name=stub.model_name,
                        sparse_attention_config=stub.sparse_attention_config,
                        model_kwargs=stub.adapter_config.model_kwargs,
                        tokenizer_kwargs=stub.adapter_config.tokenizer_kwargs
                    )
                    
                    # Create benchmark instance
                    logger.info(f"Worker {worker_id}: Creating benchmark instance for {stub.benchmark_name}")
                    benchmark = create_benchmark_instance(
                        benchmark_name=stub.benchmark_name,
                        subsets=[stub.subset] if stub.subset else None
                    )
                    
                    # Execute benchmark
                    logger.info(f"Worker {worker_id}: Executing benchmark {stub.benchmark_name} on GPU {current_gpu_id}")
                    metric_logger = MicroMetricLogger()
                    metric_logger.configure_logging(log_path=stub.result_dir, 
                                    enabled_metrics=["research_attention_density", "research_attention_output_error"],
                                    max_records=10000,
                                    sampling_factor=0.2
                                )
                    metrics = benchmark.run_benchmark(
                        adapter=adapter,
                        result_dir=stub.result_dir,
                        generation_kwargs=stub.generation_kwargs,
                        request_kwargs=stub.request_kwargs
                    )
                    metric_logger.flush()
                    
                    execution_time = time.time() - start_time
                    execution_success = True
                    
                    # Report successful result
                    result = BenchmarkResult(
                        stub=stub,
                        metrics=metrics,
                        execution_time=execution_time,
                        success=True
                    )
                    result_queue.put(result)
                    
                    logger.info(f"Worker {worker_id}: Successfully completed {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name} in {execution_time:.2f}s")

            except Exception as e:
                execution_time = time.time() - start_time  # Calculate execution time even on failure
                error_category = _categorize_error(e, "benchmark execution")
                error_msg = f"Benchmark execution failed: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                
                # Log detailed error information for debugging
                logger.debug(f"Worker {worker_id}: Error details: {traceback.format_exc()}")
                
                # Report failure
                failure = BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=execution_time
                )
                error_queue.put(failure)
                continue
            
            finally:
                # Step 4: Cleanup and release GPU
                _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
                current_gpu_id = None
                adapter = None
    
    except Exception as worker_error:
        logger.error(f"Worker {worker_id}: Critical worker error: {worker_error}")
        logger.debug(f"Worker {worker_id}: Critical error details: {traceback.format_exc()}")
        
        # Cleanup any remaining resources
        _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
        
        # Report worker failure to error queue
        error_queue.put({
            "worker_id": worker_id,
            "error_type": "WORKER_CRITICAL_ERROR",
            "error_message": str(worker_error),
            "error_traceback": traceback.format_exc()
        })


def _cleanup_worker_resources(
    gpu_pool: multiprocessing.Queue,
    gpu_id: Optional[int],
    adapter: Any,
    logger: logging.Logger,
    worker_id: int
) -> None:
    """Clean up worker resources including GPU memory and adapter.
    
    Args:
        gpu_pool: GPU pool queue to return GPU to
        gpu_id: GPU ID to release (can be None) - this is the actual GPU ID from the pool
        adapter: Model adapter to cleanup (can be None)
        logger: Logger instance for the worker
        worker_id: Worker process ID for logging
    """
    try:
        # Clean up adapter if it exists
        if adapter is not None:
            try:
                # Force garbage collection of adapter
                del adapter
                import gc
                gc.collect()
                logger.debug(f"Worker {worker_id}: Cleaned up model adapter")
            except Exception as e:
                logger.warning(f"Worker {worker_id}: Failed to cleanup adapter: {e}")
        
        # Clean up GPU memory and release GPU
        if gpu_id is not None:
            try:
                cleanup_gpu_memory(gpu_id)
                logger.debug(f"Worker {worker_id}: Cleaned up GPU memory for GPU {gpu_id}")
            
                # Release GPU back to pool (always use the original GPU ID for the pool)
                release_gpu_to_pool(gpu_pool, gpu_id)
                logger.debug(f"Worker {worker_id}: Released GPU {gpu_id} back to pool")
                
            except Exception as cleanup_error:
                logger.warning(f"Worker {worker_id}: GPU cleanup failed for GPU {gpu_id}: {cleanup_error}")
                # Try to release GPU even if cleanup failed
                try:
                    release_gpu_to_pool(gpu_pool, gpu_id)
                except Exception:
                    logger.error(f"Worker {worker_id}: Failed to release GPU {gpu_id} to pool")
    
    except Exception as e:
        logger.error(f"Worker {worker_id}: Error during resource cleanup: {e}")


class BenchmarkExecutor:
    """Parallel benchmark executor with dynamic GPU allocation and resumability.
    
    Executes benchmark experiments across a model × sparse_attention_config × 
    benchmark × subset matrix with parallel GPU execution and automatic resumability.
    
    Features:
    - Dynamic GPU allocation from specified gpu_ids
    - Subset-level resumability (skip completed experiments)
    - Parallel execution with configurable concurrency
    - Comprehensive error handling and logging
    - Result aggregation and summary generation
    
    Example:
        >>> executor = BenchmarkExecutor(
        ...     gpu_ids=[0, 1, 2, 3],
        ...     max_concurrent_runs=4,
        ...     base_result_dir="./benchmark_results"
        ... )
        >>> 
        >>> results = executor.run_benchmark_matrix(
        ...     model_names=["model1", "model2"],
        ...     sparse_attention_configs=[("dense", None), ("sparse", config)],
        ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
        ...     adapter_config=AdapterConfig()
        ... )
    """
    
    def __init__(
        self,
        gpu_ids: List[int],
        max_concurrent_runs: int,
        base_result_dir: str = "./benchmark_results",
        enable_resumability: bool = True,
        result_file_validation: bool = True,
        required_result_files: Optional[List[str]] = None,
        timeout_per_benchmark: float = 360000.0,  # 100 hours default
        verbose: bool = True,
        max_records: Optional[int] = None,  # Maximum number of metric events to log
        sampling_factor: float = 1.0,  # Probability of logging each metric event (0.0-1.0)
    ):
        """Initialize the BenchmarkExecutor.
        
        Args:
            gpu_ids: List of GPU device IDs to use for parallel execution
            max_concurrent_runs: Maximum number of concurrent benchmark runs
            base_result_dir: Base directory for storing benchmark results
            enable_resumability: Whether to skip already completed experiments
            result_file_validation: Whether to check for specific result files
            required_result_files: List of files required for completion check
            timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
            verbose: Whether to enable verbose logging
            
        Raises:
            ValueError: If configuration parameters are invalid
            RuntimeError: If GPU validation fails
        """
        # Validate and store configuration
        self.gpu_ids = self._validate_gpu_ids(gpu_ids)
        self.max_concurrent_runs = self._validate_max_concurrent_runs(max_concurrent_runs)
        self.base_result_dir = Path(base_result_dir).resolve()
        self.enable_resumability = enable_resumability
        self.result_file_validation = result_file_validation
        self.required_result_files = required_result_files or ["raw_results.csv"]
        self.timeout_per_benchmark = timeout_per_benchmark
        self.verbose = verbose
        
        # Initialize logging
        self._setup_logging()
        
        # Set up signal handlers for graceful shutdown
        self._setup_signal_handlers()
        
        # Validate GPU availability
        self._validate_gpu_setup()
        
        # Load all available benchmarks
        ensure_benchmarks_loaded()
        

        
        self.logger.info(f"BenchmarkExecutor initialized:")
        self.logger.info(f"  GPU IDs: {self.gpu_ids}")
        self.logger.info(f"  Max concurrent runs: {self.max_concurrent_runs}")
        self.logger.info(f"  Base result directory: {self.base_result_dir}")
        self.logger.info(f"  Resumability: {'enabled' if self.enable_resumability else 'disabled'}")
    
    def _setup_signal_handlers(self) -> None:
        """Set up signal handlers for graceful shutdown."""
        signal.signal(signal.SIGTERM, _signal_handler)
        signal.signal(signal.SIGINT, _signal_handler)
        
        # Register cleanup function to run at exit
        import atexit
        atexit.register(self._cleanup_on_exit)
    
    def _cleanup_on_exit(self) -> None:
        """Cleanup function called when the process exits."""
        if hasattr(self, 'logger'):
            self.logger.info("BenchmarkExecutor cleanup on exit")
        
        # Reset global shutdown flag
        global _shutdown_requested
        _shutdown_requested = False

    def run_benchmark_matrix(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig,
        generation_kwargs: Optional[Dict[str, Any]] = None,
        request_kwargs: Optional[Dict[str, Any]] = None
    ) -> ExecutionResults:
        """Execute benchmark matrix with parallel GPU execution.
        
        Runs all combinations of model_names × sparse_attention_configs × 
        benchmark_configs × subsets in parallel across available GPUs.
        
        Args:
            model_names: List of model names to benchmark
            sparse_attention_configs: List of (name, config) tuples for sparse attention
            benchmark_configs: List of benchmark configurations
            adapter_config: Configuration for model adapter
            generation_kwargs: Optional generation parameters
            request_kwargs: Optional request processing parameters
            
        Returns:
            ExecutionResults containing comprehensive execution information
            
        Example:
            >>> results = executor.run_benchmark_matrix(
            ...     model_names=["meta-llama/Llama-3.1-8B-Instruct"],
            ...     sparse_attention_configs=[("dense", None)],
            ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
            ...     adapter_config=AdapterConfig()
            ... )
            >>> print(f"Completed: {results.completed_count}/{results.total_count}")
        """
        # Initialize execution
        self.logger.info("Starting benchmark matrix execution")
        
        try:
            # Step 1: Validate input parameters
            self._validate_matrix_parameters(
                model_names, sparse_attention_configs, benchmark_configs, adapter_config
            )
            
            # Step 2: Generate benchmark stubs (matrix expansion)
            self.logger.info("Generating benchmark matrix...")
            all_stubs = generate_benchmark_stubs(
                model_names=model_names,
                sparse_attention_configs=sparse_attention_configs,
                benchmark_configs=benchmark_configs,
                adapter_config=adapter_config,
                generation_kwargs=generation_kwargs,
                request_kwargs=request_kwargs,
                base_result_dir=str(self.base_result_dir)
            )
            
            total_combinations = len(all_stubs)
            self.logger.info(f"Generated {total_combinations} benchmark combinations:")
            self.logger.info(f"  {len(model_names)} models × {len(sparse_attention_configs)} sparse configs × "
                           f"{sum(len(b.subsets) if b.subsets else 1 for b in benchmark_configs)} benchmark-subsets")
            
            # Step 3: Filter for resumability (if enabled)
            if self.enable_resumability:
                self.logger.info("Checking for existing results (resumability)...")
                pending_stubs, completed_stubs = filter_existing_results(
                    all_stubs,
                    check_result_files=self.result_file_validation,
                    required_files=self.required_result_files,
                    verbose=self.verbose
                )
                
                self.logger.info(f"Resumability check: {len(completed_stubs)} completed, {len(pending_stubs)} pending")
            else:
                pending_stubs = all_stubs
                completed_stubs = []
                self.logger.info("Resumability disabled - will execute all combinations")
            
            # Step 4: Early exit if nothing to do
            if not pending_stubs:
                self.logger.info("No pending benchmarks to execute. All experiments already completed!")
                
                execution_summary = {
                    "total_combinations": total_combinations,
                    "models": model_names,
                    "sparse_configs": [name for name, _ in sparse_attention_configs],
                    "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs]
                }
                
                progress = ExecutionProgress(
                    total_stubs=total_combinations,
                    skipped_stubs=len(completed_stubs),
                    completed_stubs=0,
                    failed_stubs=0
                )
                
                return ExecutionResults(
                    execution_summary=execution_summary,
                    individual_results=[],
                    failed_executions=[],
                    execution_time=0.0,
                    progress=progress
                )
            

            
            # Step 6: Execute pending benchmarks with worker processes
            self.logger.info(f"Executing {len(pending_stubs)} pending benchmarks with {self.max_concurrent_runs} workers...")
            
            start_time = time.time()
            results = self._execute_with_workers(pending_stubs)
            execution_time = time.time() - start_time
            
            # Step 7: Aggregate results
            execution_summary = {
                "total_combinations": total_combinations,
                "models": model_names,
                "sparse_configs": [name for name, _ in sparse_attention_configs],
                "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs],
                "execution_time_seconds": execution_time,
                "gpu_ids_used": self.gpu_ids,
                "max_concurrent_runs": self.max_concurrent_runs
            }
            
            progress = ExecutionProgress(
                total_stubs=total_combinations,
                skipped_stubs=len(completed_stubs),
                completed_stubs=len(results.individual_results),
                failed_stubs=len(results.failed_executions)
            )
            
            final_results = ExecutionResults(
                execution_summary=execution_summary,
                individual_results=results.individual_results,
                failed_executions=results.failed_executions,
                execution_time=execution_time,
                progress=progress
            )
            
            self.logger.info(f"Benchmark matrix execution completed:")
            self.logger.info(f"  Total time: {execution_time:.2f}s")
            self.logger.info(f"  Successful: {len(results.individual_results)}")
            self.logger.info(f"  Failed: {len(results.failed_executions)}")
            self.logger.info(f"  Skipped: {len(completed_stubs)}")
            
            return final_results
            
        except Exception as e:
            self.logger.error(f"Benchmark execution failed: {e}")
            raise
    
    def _execute_with_workers(self, pending_stubs: List[BenchmarkStub]) -> ExecutionResults:
        """Execute benchmark stubs using worker processes.
        
        This method implements comprehensive process pool management with:
        - Dynamic work distribution through shared queues
        - Graceful process startup and shutdown
        - Real-time result collection and error handling
        - Process health monitoring and recovery
        
        Args:
            pending_stubs: List of benchmark stubs to execute
            
        Returns:
            ExecutionResults containing all results and failures
        """
        self.logger.info(f"Starting execution with {len(pending_stubs)} stubs using {self.max_concurrent_runs} workers")
        
        # Step 1: Create and configure shared queues
        stub_queue = multiprocessing.Queue(maxsize=len(pending_stubs) + 100)  # Buffer for queue operations
        gpu_pool = create_gpu_pool(self.gpu_ids)
        result_queue = multiprocessing.Queue()
        error_queue = multiprocessing.Queue()
        
        # Step 2: Populate work queue with all stubs
        self.logger.info("Populating work queue...")
        for stub in pending_stubs:
            stub_queue.put(stub)
        
        # Add sentinel values to signal workers to stop
        for _ in range(self.max_concurrent_runs):
            stub_queue.put(None)  # Sentinel value
        
        # Step 3: Create and start worker processes
        workers = []
        worker_pids = []
        
        self.logger.info(f"Starting {self.max_concurrent_runs} worker processes...")
        for i in range(self.max_concurrent_runs):
            worker = multiprocessing.Process(
                target=_benchmark_worker,
                args=(stub_queue, gpu_pool, result_queue, error_queue, self.timeout_per_benchmark),
                name=f"benchmark_worker_{i}"
            )
            worker.daemon = True  # Ensure workers are terminated when main process exits
            worker.start()
            workers.append(worker)
            worker_pids.append(worker.pid)
            self.logger.debug(f"Started worker {i+1}/{self.max_concurrent_runs} (PID: {worker.pid})")
        
        self.logger.info(f"All {self.max_concurrent_runs} workers started successfully")
        
        # Step 4: Monitor and collect results in real-time
        individual_results: List[BenchmarkResult] = []
        failed_executions: List[BenchmarkFailure] = []
        active_workers = set(worker_pids)
        
        self.logger.info("Monitoring worker processes and collecting results...")
        
        try:
            # Monitor workers and collect results until all work is complete
            while active_workers:
                # Check for completed workers
                completed_workers = []
                for worker in workers:
                    if worker.is_alive():
                        # Worker is still running, check for results
                        pass
                    else:
                        # Worker has completed
                        if worker.pid in active_workers:
                            active_workers.remove(worker.pid)
                            completed_workers.append(worker)
                            
                            if worker.exitcode != 0:
                                self.logger.warning(f"Worker {worker.pid} exited with code {worker.exitcode}")
                            else:
                                self.logger.debug(f"Worker {worker.pid} completed successfully")
                
                # Collect any available results (non-blocking)
                self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
                
                # Log progress periodically
                if len(individual_results) + len(failed_executions) > 0 and (len(individual_results) + len(failed_executions)) % 5 == 0:
                    self.logger.info(f"Progress: {len(individual_results)} completed, {len(failed_executions)} failed, {len(active_workers)} workers active")
                
                # Small delay to prevent busy waiting
                time.sleep(0.1)
            
            # Step 5: Final result collection
            self.logger.info("All workers completed, collecting final results...")
            self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
            
        except KeyboardInterrupt:
            self.logger.warning("Received interrupt signal, initiating graceful shutdown...")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        except Exception as e:
            error_category = _categorize_error(e, "main execution")
            self.logger.error(f"Error during execution ({error_category}): {e}")
            self.logger.debug(f"Execution error details: {traceback.format_exc()}")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        finally:
            # Step 6: Cleanup resources
            self._cleanup_resources(workers, stub_queue, gpu_pool, result_queue, error_queue)
        
        # Step 7: Final summary
        total_processed = len(individual_results) + len(failed_executions)
        self.logger.info(f"Execution completed: {len(individual_results)} successful, {len(failed_executions)} failed")
        self.logger.info(f"Success rate: {len(individual_results)/total_processed*100:.1f}%" if total_processed > 0 else "No work processed")
        
        return ExecutionResults(
            execution_summary={},  # Will be filled by caller
            individual_results=individual_results,
            failed_executions=failed_executions,
            execution_time=0.0,  # Will be filled by caller
            progress=ExecutionProgress(
                total_stubs=len(pending_stubs),
                completed_stubs=len(individual_results),
                failed_stubs=len(failed_executions)
            )
        )
    
    def _collect_available_results(
        self,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue,
        individual_results: List[BenchmarkResult],
        failed_executions: List[BenchmarkFailure]
    ) -> None:
        """Collect all available results from queues without blocking.
        
        Args:
            result_queue: Queue containing successful benchmark results
            error_queue: Queue containing benchmark failures
            individual_results: List to append successful results to
            failed_executions: List to append failures to
        """
        # Collect all available successful results
        while True:
            try:
                result = result_queue.get_nowait()
                individual_results.append(result)
            except Empty:
                break
        
        # Collect all available errors
        while True:
            try:
                error = error_queue.get_nowait()
                if isinstance(error, BenchmarkFailure):
                    failed_executions.append(error)
                else:
                    # Handle worker-level errors
                    self.logger.error(f"Worker error: {error}")
            except Empty:
                break
    
    def _graceful_shutdown(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Perform graceful shutdown of worker processes and queues.
        
        Args:
            workers: List of worker processes to terminate
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.info("Initiating graceful shutdown...")
        
        try:
            # Signal workers to stop by adding sentinel values
            sentinel_count = 0
            for _ in range(len(workers)):
                try:
                    stub_queue.put_nowait(None)
                    sentinel_count += 1
                except Exception as e:
                    self.logger.debug(f"Could not add sentinel to queue: {e}")
                    break  # Queue is full, stop adding sentinels
            
            self.logger.info(f"Added {sentinel_count} shutdown signals to work queue")
            
            # Wait for workers to finish with timeout
            self.logger.info("Waiting for workers to finish...")
            for i, worker in enumerate(workers):
                try:
                    worker.join(timeout=30.0)  # 30 second timeout per worker
                    if worker.is_alive():
                        self.logger.warning(f"Force terminating worker {worker.pid}")
                        worker.terminate()
                        worker.join(timeout=5.0)
                        if worker.is_alive():
                            self.logger.error(f"Failed to terminate worker {worker.pid}, killing process")
                            worker.kill()
                            worker.join(timeout=2.0)
                    else:
                        self.logger.debug(f"Worker {worker.pid} terminated successfully")
                except Exception as e:
                    self.logger.error(f"Error during worker {worker.pid} shutdown: {e}")
            
            self.logger.info("Graceful shutdown completed")
            
        except Exception as e:
            self.logger.error(f"Error during graceful shutdown: {e}")
            # Force cleanup of remaining workers
            for worker in workers:
                if worker.is_alive():
                    try:
                        worker.kill()
                    except Exception:
                        pass
    
    def _cleanup_resources(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Clean up all multiprocessing resources.
        
        Args:
            workers: List of worker processes
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.debug("Cleaning up multiprocessing resources...")
        
        # Close all queues
        try:
            stub_queue.close()
            gpu_pool.close()
            result_queue.close()
            error_queue.close()
        except Exception as e:
            self.logger.warning(f"Error closing queues: {e}")
        
        # Join any remaining workers
        for worker in workers:
            if worker.is_alive():
                self.logger.warning(f"Force terminating remaining worker {worker.pid}")
                worker.terminate()
                worker.join(timeout=5.0)
                if worker.is_alive():
                    worker.kill()
        
        self.logger.debug("Resource cleanup completed")
    
    def _validate_gpu_ids(self, gpu_ids: List[int]) -> List[int]:
        """Validate and return GPU IDs."""
        if not gpu_ids:
            raise ValueError("gpu_ids cannot be empty")
        
        if not all(isinstance(gpu_id, int) and gpu_id >= 0 for gpu_id in gpu_ids):
            raise ValueError("All GPU IDs must be non-negative integers")
        
        # Remove duplicates while preserving order
        seen = set()
        unique_gpu_ids = []
        for gpu_id in gpu_ids:
            if gpu_id not in seen:
                seen.add(gpu_id)
                unique_gpu_ids.append(gpu_id)
        
        return unique_gpu_ids
    
    def _validate_max_concurrent_runs(self, max_concurrent_runs: int) -> int:
        """Validate and return max_concurrent_runs."""
        if not isinstance(max_concurrent_runs, int) or max_concurrent_runs <= 0:
            raise ValueError("max_concurrent_runs must be a positive integer")
        
        if max_concurrent_runs > len(self.gpu_ids) if hasattr(self, 'gpu_ids') else True:
            # This is just a warning - user might want more concurrent runs than GPUs
            # for benchmarks that don't require much GPU memory
            pass
        
        return max_concurrent_runs
    
    def _setup_logging(self) -> None:
        """Setup logging configuration."""
        self.logger = logging.getLogger(f"{__name__}.BenchmarkExecutor")
        
        if self.verbose and not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def _validate_gpu_setup(self) -> None:
        """Validate GPU availability."""
        try:
            validate_gpu_availability(self.gpu_ids)
            self.logger.info(f"GPU validation passed for devices: {self.gpu_ids}")
        except (RuntimeError, ValueError) as e:
            self.logger.error(f"GPU validation failed: {e}")
            raise
    
    def _validate_matrix_parameters(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig
    ) -> None:
        """Validate benchmark matrix parameters."""
        if not model_names:
            raise ValueError("model_names cannot be empty")
        
        if not sparse_attention_configs:
            raise ValueError("sparse_attention_configs cannot be empty")
        
        if not benchmark_configs:
            raise ValueError("benchmark_configs cannot be empty")
        
        # Validate model names
        for model_name in model_names:
            if not isinstance(model_name, str) or not model_name.strip():
                raise ValueError(f"Invalid model name: {model_name}")
        
        # Validate sparse attention configs
        for sparse_config in sparse_attention_configs:
            if not isinstance(sparse_config, tuple) or len(sparse_config) != 2:
                raise ValueError(f"sparse_attention_configs must be list of (name, config) tuples")
            
            config_name, config_obj = sparse_config
            if not isinstance(config_name, str) or not config_name.strip():
                raise ValueError(f"Sparse config name must be non-empty string: {config_name}")
        
        # Validate benchmark configs
        for benchmark_config in benchmark_configs:
            if not isinstance(benchmark_config, BenchmarkConfig):
                raise ValueError(f"benchmark_configs must contain BenchmarkConfig instances")
            # Additional validation could be added here if needed
        
        # Validate adapter config
        if not isinstance(adapter_config, AdapterConfig):
            raise ValueError("adapter_config must be an AdapterConfig instance")
            # Additional validation could be added here if needed
        
        self.logger.info("Matrix parameter validation passed")

```

```python
"""BenchmarkExecutor: Parallel GPU execution for benchmark matrix experiments.

This module contains the main BenchmarkExecutor class that orchestrates parallel 
execution of benchmark experiments across the model × sparse_attention_config × 
benchmark × subset matrix with dynamic GPU allocation and resumability.
"""

import logging
import multiprocessing
import os
import signal
import sys
import time
import traceback
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from queue import Empty
from contextlib import contextmanager
from sparse_attention_hub.metric_logging.logger import MicroMetricLogger

# Set multiprocessing start method to 'spawn' for CUDA compatibility
if multiprocessing.get_start_method(allow_none=True) != 'spawn':
    multiprocessing.set_start_method('spawn', force=True)

from sparse_attention_hub.sparse_attention.base import SparseAttentionConfig

from .executor_config import (
    AdapterConfig,
    BenchmarkConfig,
    BenchmarkStub,
    ExecutionResults,
    ExecutionProgress,
    BenchmarkResult,
    BenchmarkFailure,
    filter_existing_results,
    generate_benchmark_stubs,

)
from .benchmark_registry import (
    ensure_benchmarks_loaded,
    create_benchmark_instance
)

from .utils.gpu import (
    cleanup_gpu_memory,
    release_gpu_to_pool,
    acquire_gpu_from_pool,
    create_gpu_pool,
    validate_gpu_availability,
)

# Global flag for graceful shutdown
_shutdown_requested = False


@contextmanager
def set_gpu(gpu_id: int):
    """Context manager to set the current CUDA device.
    
    Args:
        gpu_id: The GPU device ID to set as current device
        
    Yields:
        None: The context manager yields control to the with block
    """
    import torch
    torch.cuda.set_device(gpu_id)
    yield


def _signal_handler(signum: int, frame: Any) -> None:
    """Handle shutdown signals gracefully."""
    global _shutdown_requested
    _shutdown_requested = True
    logging.getLogger(__name__).warning(f"Received signal {signum}, initiating graceful shutdown...")


def _categorize_error(error: Exception, context: str = "") -> str:
    """Categorize errors for better handling and reporting.
    
    Args:
        error: The exception that occurred
        context: Additional context about where the error occurred
        
    Returns:
        Error category string for classification
    """
    error_type = type(error).__name__
    error_msg = str(error).lower()
    
    # GPU-related errors
    if any(keyword in error_msg for keyword in ["cuda", "gpu", "out of memory", "oom"]):
        if "out of memory" in error_msg or "oom" in error_msg:
            return "GPU_OUT_OF_MEMORY"
        elif "cuda" in error_msg:
            return "GPU_CUDA_ERROR"
        else:
            return "GPU_ERROR"
    
    # Model loading errors
    elif any(keyword in error_msg for keyword in ["model", "huggingface", "transformers", "pretrained"]):
        if "not found" in error_msg or "doesn't exist" in error_msg:
            return "MODEL_NOT_FOUND"
        elif "tokenizer" in error_msg:
            return "TOKENIZER_ERROR"
        else:
            return "MODEL_LOADING_ERROR"
    
    # Dataset/benchmark errors
    elif any(keyword in error_msg for keyword in ["dataset", "benchmark", "subset"]):
        return "DATASET_ERROR"
    
    # Network/connection errors
    elif any(keyword in error_msg for keyword in ["connection", "timeout", "network", "http"]):
        return "NETWORK_ERROR"
    
    # File system errors
    elif any(keyword in error_msg for keyword in ["file", "directory", "permission", "disk"]):
        return "FILESYSTEM_ERROR"
    
    # Multiprocessing errors
    elif any(keyword in error_msg for keyword in ["queue", "process", "multiprocessing"]):
        return "MULTIPROCESSING_ERROR"
    
    # Default categories
    elif isinstance(error, TimeoutError):
        return "TIMEOUT_ERROR"
    elif isinstance(error, KeyboardInterrupt):
        return "INTERRUPT_ERROR"
    elif isinstance(error, MemoryError):
        return "SYSTEM_MEMORY_ERROR"
    else:
        return "UNKNOWN_ERROR"


def _benchmark_worker(
    stub_queue: multiprocessing.Queue,
    gpu_pool: multiprocessing.Queue,
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue,
    timeout_per_benchmark: float = 3600.0
) -> None:
    """Worker function to execute benchmarks with dynamic GPU allocation.
    
    This function runs in a separate process and handles:
    1. Acquiring GPUs from the shared pool
    2. Creating model adapters with sparse attention configurations
    3. Executing benchmarks with proper error handling
    4. Releasing GPUs back to the pool
    5. Reporting results and errors
    
    Args:
        stub_queue: Queue containing BenchmarkStub instances to process
        gpu_pool: Shared queue containing available GPU IDs
        result_queue: Queue for reporting successful benchmark results
        error_queue: Queue for reporting benchmark failures
        timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
    """
    worker_id = os.getpid()
    logger = logging.getLogger(f"{__name__}.worker_{worker_id}")
    
    # Set up signal handlers for graceful shutdown
    signal.signal(signal.SIGTERM, _signal_handler)
    signal.signal(signal.SIGINT, _signal_handler)
    
    # Track resources for cleanup
    current_gpu_id: Optional[int] = None
    adapter = None

    try:
        while True:
            # Check for shutdown signal
            if _shutdown_requested:
                logger.info(f"Worker {worker_id}: Shutdown requested, exiting")
                break
            
            # Step 1: Get next benchmark stub from queue
            try:
                stub: BenchmarkStub = stub_queue.get(timeout=1.0)  # 1 second timeout
            except Empty:
                # No more work to do
                logger.info(f"Worker {worker_id}: No more work, shutting down")
                break
            
            # Check for sentinel value (shutdown signal)
            if stub is None:
                logger.info(f"Worker {worker_id}: Received shutdown signal, exiting")
                break
            
            # Step 2: Acquire GPU from pool
            try:
                current_gpu_id = acquire_gpu_from_pool(gpu_pool, timeout=30.0)
                logger.info(f"Worker {worker_id}: Acquired GPU {current_gpu_id} for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}")
            except Exception as e:
                error_category = _categorize_error(e, "GPU acquisition")
                error_msg = f"Failed to acquire GPU for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                error_queue.put(BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=0.0
                ))
                continue

            # Step 3: Run the benchmark on the current GPU
            start_time = time.time()
            execution_time = 0.0  # Initialize execution_time
            
            try:
                with set_gpu(current_gpu_id):
                    logger.debug(f"Worker {worker_id}: Set CUDA device to {current_gpu_id}")
                    execution_success = False

                    # Create model adapter with sparse attention config
                    logger.info(f"Worker {worker_id}: Creating model adapter for {stub.model_name}")
                    
                    # Import here to avoid issues with multiprocessing
                    from sparse_attention_hub.adapters.huggingface import ModelAdapterHF
                    
                    adapter = ModelAdapterHF(
                        model_name=stub.model_name,
                        sparse_attention_config=stub.sparse_attention_config,
                        model_kwargs=stub.adapter_config.model_kwargs,
                        tokenizer_kwargs=stub.adapter_config.tokenizer_kwargs
                    )
                    
                    # Create benchmark instance
                    logger.info(f"Worker {worker_id}: Creating benchmark instance for {stub.benchmark_name}")
                    benchmark = create_benchmark_instance(
                        benchmark_name=stub.benchmark_name,
                        subsets=[stub.subset] if stub.subset else None
                    )
                    
                    # Execute benchmark
                    logger.info(f"Worker {worker_id}: Executing benchmark {stub.benchmark_name} on GPU {current_gpu_id}")
                    metric_logger = MicroMetricLogger()
                    metric_logger.configure_logging(log_path=stub.result_dir, 
                                    enabled_metrics=["research_attention_density", "research_attention_output_error"],
                                    max_records=10000,
                                    sampling_factor=0.2
                                )
                    metrics = benchmark.run_benchmark(
                        adapter=adapter,
                        result_dir=stub.result_dir,
                        generation_kwargs=stub.generation_kwargs,
                        request_kwargs=stub.request_kwargs
                    )
                    metric_logger.flush()
                    
                    execution_time = time.time() - start_time
                    execution_success = True
                    
                    # Report successful result
                    result = BenchmarkResult(
                        stub=stub,
                        metrics=metrics,
                        execution_time=execution_time,
                        success=True
                    )
                    result_queue.put(result)
                    
                    logger.info(f"Worker {worker_id}: Successfully completed {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name} in {execution_time:.2f}s")

            except Exception as e:
                execution_time = time.time() - start_time  # Calculate execution time even on failure
                error_category = _categorize_error(e, "benchmark execution")
                error_msg = f"Benchmark execution failed: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                
                # Log detailed error information for debugging
                logger.debug(f"Worker {worker_id}: Error details: {traceback.format_exc()}")
                
                # Report failure
                failure = BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=execution_time
                )
                error_queue.put(failure)
                continue
            
            finally:
                # Step 4: Cleanup and release GPU
                _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
                current_gpu_id = None
                adapter = None
    
    except Exception as worker_error:
        logger.error(f"Worker {worker_id}: Critical worker error: {worker_error}")
        logger.debug(f"Worker {worker_id}: Critical error details: {traceback.format_exc()}")
        
        # Cleanup any remaining resources
        _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
        
        # Report worker failure to error queue
        error_queue.put({
            "worker_id": worker_id,
            "error_type": "WORKER_CRITICAL_ERROR",
            "error_message": str(worker_error),
            "error_traceback": traceback.format_exc()
        })


def _cleanup_worker_resources(
    gpu_pool: multiprocessing.Queue,
    gpu_id: Optional[int],
    adapter: Any,
    logger: logging.Logger,
    worker_id: int
) -> None:
    """Clean up worker resources including GPU memory and adapter.
    
    Args:
        gpu_pool: GPU pool queue to return GPU to
        gpu_id: GPU ID to release (can be None) - this is the actual GPU ID from the pool
        adapter: Model adapter to cleanup (can be None)
        logger: Logger instance for the worker
        worker_id: Worker process ID for logging
    """
    try:
        # Clean up adapter if it exists
        if adapter is not None:
            try:
                # Force garbage collection of adapter
                del adapter
                import gc
                gc.collect()
                logger.debug(f"Worker {worker_id}: Cleaned up model adapter")
            except Exception as e:
                logger.warning(f"Worker {worker_id}: Failed to cleanup adapter: {e}")
        
        # Clean up GPU memory and release GPU
        if gpu_id is not None:
            try:
                cleanup_gpu_memory(gpu_id)
                logger.debug(f"Worker {worker_id}: Cleaned up GPU memory for GPU {gpu_id}")
            
                # Release GPU back to pool (always use the original GPU ID for the pool)
                release_gpu_to_pool(gpu_pool, gpu_id)
                logger.debug(f"Worker {worker_id}: Released GPU {gpu_id} back to pool")
                
            except Exception as cleanup_error:
                logger.warning(f"Worker {worker_id}: GPU cleanup failed for GPU {gpu_id}: {cleanup_error}")
                # Try to release GPU even if cleanup failed
                try:
                    release_gpu_to_pool(gpu_pool, gpu_id)
                except Exception:
                    logger.error(f"Worker {worker_id}: Failed to release GPU {gpu_id} to pool")
    
    except Exception as e:
        logger.error(f"Worker {worker_id}: Error during resource cleanup: {e}")


class BenchmarkExecutor:
    """Parallel benchmark executor with dynamic GPU allocation and resumability.
    
    Executes benchmark experiments across a model × sparse_attention_config × 
    benchmark × subset matrix with parallel GPU execution and automatic resumability.
    
    Features:
    - Dynamic GPU allocation from specified gpu_ids
    - Subset-level resumability (skip completed experiments)
    - Parallel execution with configurable concurrency
    - Comprehensive error handling and logging
    - Result aggregation and summary generation
    
    Example:
        >>> executor = BenchmarkExecutor(
        ...     gpu_ids=[0, 1, 2, 3],
        ...     max_concurrent_runs=4,
        ...     base_result_dir="./benchmark_results"
        ... )
        >>> 
        >>> results = executor.run_benchmark_matrix(
        ...     model_names=["model1", "model2"],
        ...     sparse_attention_configs=[("dense", None), ("sparse", config)],
        ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
        ...     adapter_config=AdapterConfig()
        ... )
    """
    
    def __init__(
        self,
        gpu_ids: List[int],
        max_concurrent_runs: int,
        base_result_dir: str = "./benchmark_results",
        enable_resumability: bool = True,
        result_file_validation: bool = True,
        required_result_files: Optional[List[str]] = None,
        timeout_per_benchmark: float = 360000.0,  # 100 hours default
        verbose: bool = True,
        max_records: Optional[int] = None,  # Maximum number of metric events to log
        sampling_factor: float = 1.0,  # Probability of logging each metric event (0.0-1.0)
    ):
        """Initialize the BenchmarkExecutor.
        
        Args:
            gpu_ids: List of GPU device IDs to use for parallel execution
            max_concurrent_runs: Maximum number of concurrent benchmark runs
            base_result_dir: Base directory for storing benchmark results
            enable_resumability: Whether to skip already completed experiments
            result_file_validation: Whether to check for specific result files
            required_result_files: List of files required for completion check
            timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
            verbose: Whether to enable verbose logging
            max_records: Maximum number of metric events to log (None for unlimited)
            sampling_factor: Probability of logging each metric event (0.0-1.0)
            
        Raises:
            ValueError: If configuration parameters are invalid
            RuntimeError: If GPU validation fails
        """
        # Validate and store configuration
        self.gpu_ids = self._validate_gpu_ids(gpu_ids)
        self.max_concurrent_runs = self._validate_max_concurrent_runs(max_concurrent_runs)
        self.base_result_dir = Path(base_result_dir).resolve()
        self.enable_resumability = enable_resumability
        self.result_file_validation = result_file_validation
        self.required_result_files = required_result_files or ["raw_results.csv"]
        self.timeout_per_benchmark = timeout_per_benchmark
        self.verbose = verbose
        
        # Initialize logging
        self._setup_logging()
        
        # Set up signal handlers for graceful shutdown
        self._setup_signal_handlers()
        
        # Validate GPU availability
        self._validate_gpu_setup()
        
        # Load all available benchmarks
        ensure_benchmarks_loaded()
        

        
        self.logger.info(f"BenchmarkExecutor initialized:")
        self.logger.info(f"  GPU IDs: {self.gpu_ids}")
        self.logger.info(f"  Max concurrent runs: {self.max_concurrent_runs}")
        self.logger.info(f"  Base result directory: {self.base_result_dir}")
        self.logger.info(f"  Resumability: {'enabled' if self.enable_resumability else 'disabled'}")
    
    def _setup_signal_handlers(self) -> None:
        """Set up signal handlers for graceful shutdown."""
        signal.signal(signal.SIGTERM, _signal_handler)
        signal.signal(signal.SIGINT, _signal_handler)
        
        # Register cleanup function to run at exit
        import atexit
        atexit.register(self._cleanup_on_exit)
    
    def _cleanup_on_exit(self) -> None:
        """Cleanup function called when the process exits."""
        if hasattr(self, 'logger'):
            self.logger.info("BenchmarkExecutor cleanup on exit")
        
        # Reset global shutdown flag
        global _shutdown_requested
        _shutdown_requested = False

    def run_benchmark_matrix(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig,
        generation_kwargs: Optional[Dict[str, Any]] = None,
        request_kwargs: Optional[Dict[str, Any]] = None
    ) -> ExecutionResults:
        """Execute benchmark matrix with parallel GPU execution.
        
        Runs all combinations of model_names × sparse_attention_configs × 
        benchmark_configs × subsets in parallel across available GPUs.
        
        Args:
            model_names: List of model names to benchmark
            sparse_attention_configs: List of (name, config) tuples for sparse attention
            benchmark_configs: List of benchmark configurations
            adapter_config: Configuration for model adapter
            generation_kwargs: Optional generation parameters
            request_kwargs: Optional request processing parameters
            
        Returns:
            ExecutionResults containing comprehensive execution information
            
        Example:
            >>> results = executor.run_benchmark_matrix(
            ...     model_names=["meta-llama/Llama-3.1-8B-Instruct"],
            ...     sparse_attention_configs=[("dense", None)],
            ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
            ...     adapter_config=AdapterConfig()
            ... )
            >>> print(f"Completed: {results.completed_count}/{results.total_count}")
        """
        # Initialize execution
        self.logger.info("Starting benchmark matrix execution")
        
        try:
            # Step 1: Validate input parameters
            self._validate_matrix_parameters(
                model_names, sparse_attention_configs, benchmark_configs, adapter_config
            )
            
            # Step 2: Generate benchmark stubs (matrix expansion)
            self.logger.info("Generating benchmark matrix...")
            all_stubs = generate_benchmark_stubs(
                model_names=model_names,
                sparse_attention_configs=sparse_attention_configs,
                benchmark_configs=benchmark_configs,
                adapter_config=adapter_config,
                generation_kwargs=generation_kwargs,
                request_kwargs=request_kwargs,
                base_result_dir=str(self.base_result_dir)
            )
            
            total_combinations = len(all_stubs)
            self.logger.info(f"Generated {total_combinations} benchmark combinations:")
            self.logger.info(f"  {len(model_names)} models × {len(sparse_attention_configs)} sparse configs × "
                           f"{sum(len(b.subsets) if b.subsets else 1 for b in benchmark_configs)} benchmark-subsets")
            
            # Step 3: Filter for resumability (if enabled)
            if self.enable_resumability:
                self.logger.info("Checking for existing results (resumability)...")
                pending_stubs, completed_stubs = filter_existing_results(
                    all_stubs,
                    check_result_files=self.result_file_validation,
                    required_files=self.required_result_files,
                    verbose=self.verbose
                )
                
                self.logger.info(f"Resumability check: {len(completed_stubs)} completed, {len(pending_stubs)} pending")
            else:
                pending_stubs = all_stubs
                completed_stubs = []
                self.logger.info("Resumability disabled - will execute all combinations")
            
            # Step 4: Early exit if nothing to do
            if not pending_stubs:
                self.logger.info("No pending benchmarks to execute. All experiments already completed!")
                
                execution_summary = {
                    "total_combinations": total_combinations,
                    "models": model_names,
                    "sparse_configs": [name for name, _ in sparse_attention_configs],
                    "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs]
                }
                
                progress = ExecutionProgress(
                    total_stubs=total_combinations,
                    skipped_stubs=len(completed_stubs),
                    completed_stubs=0,
                    failed_stubs=0
                )
                
                return ExecutionResults(
                    execution_summary=execution_summary,
                    individual_results=[],
                    failed_executions=[],
                    execution_time=0.0,
                    progress=progress
                )
            

            
            # Step 6: Execute pending benchmarks with worker processes
            self.logger.info(f"Executing {len(pending_stubs)} pending benchmarks with {self.max_concurrent_runs} workers...")
            
            start_time = time.time()
            results = self._execute_with_workers(pending_stubs)
            execution_time = time.time() - start_time
            
            # Step 7: Aggregate results
            execution_summary = {
                "total_combinations": total_combinations,
                "models": model_names,
                "sparse_configs": [name for name, _ in sparse_attention_configs],
                "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs],
                "execution_time_seconds": execution_time,
                "gpu_ids_used": self.gpu_ids,
                "max_concurrent_runs": self.max_concurrent_runs
            }
            
            progress = ExecutionProgress(
                total_stubs=total_combinations,
                skipped_stubs=len(completed_stubs),
                completed_stubs=len(results.individual_results),
                failed_stubs=len(results.failed_executions)
            )
            
            final_results = ExecutionResults(
                execution_summary=execution_summary,
                individual_results=results.individual_results,
                failed_executions=results.failed_executions,
                execution_time=execution_time,
                progress=progress
            )
            
            self.logger.info(f"Benchmark matrix execution completed:")
            self.logger.info(f"  Total time: {execution_time:.2f}s")
            self.logger.info(f"  Successful: {len(results.individual_results)}")
            self.logger.info(f"  Failed: {len(results.failed_executions)}")
            self.logger.info(f"  Skipped: {len(completed_stubs)}")
            
            return final_results
            
        except Exception as e:
            self.logger.error(f"Benchmark execution failed: {e}")
            raise
    
    def _execute_with_workers(self, pending_stubs: List[BenchmarkStub]) -> ExecutionResults:
        """Execute benchmark stubs using worker processes.
        
        This method implements comprehensive process pool management with:
        - Dynamic work distribution through shared queues
        - Graceful process startup and shutdown
        - Real-time result collection and error handling
        - Process health monitoring and recovery
        
        Args:
            pending_stubs: List of benchmark stubs to execute
            
        Returns:
            ExecutionResults containing all results and failures
        """
        self.logger.info(f"Starting execution with {len(pending_stubs)} stubs using {self.max_concurrent_runs} workers")
        
        # Step 1: Create and configure shared queues
        stub_queue = multiprocessing.Queue(maxsize=len(pending_stubs) + 100)  # Buffer for queue operations
        gpu_pool = create_gpu_pool(self.gpu_ids)
        result_queue = multiprocessing.Queue()
        error_queue = multiprocessing.Queue()
        
        # Step 2: Populate work queue with all stubs
        self.logger.info("Populating work queue...")
        for stub in pending_stubs:
            stub_queue.put(stub)
        
        # Add sentinel values to signal workers to stop
        for _ in range(self.max_concurrent_runs):
            stub_queue.put(None)  # Sentinel value
        
        # Step 3: Create and start worker processes
        workers = []
        worker_pids = []
        
        self.logger.info(f"Starting {self.max_concurrent_runs} worker processes...")
        for i in range(self.max_concurrent_runs):
            worker = multiprocessing.Process(
                target=_benchmark_worker,
                args=(stub_queue, gpu_pool, result_queue, error_queue, self.timeout_per_benchmark),
                name=f"benchmark_worker_{i}"
            )
            worker.daemon = True  # Ensure workers are terminated when main process exits
            worker.start()
            workers.append(worker)
            worker_pids.append(worker.pid)
            self.logger.debug(f"Started worker {i+1}/{self.max_concurrent_runs} (PID: {worker.pid})")
        
        self.logger.info(f"All {self.max_concurrent_runs} workers started successfully")
        
        # Step 4: Monitor and collect results in real-time
        individual_results: List[BenchmarkResult] = []
        failed_executions: List[BenchmarkFailure] = []
        active_workers = set(worker_pids)
        
        self.logger.info("Monitoring worker processes and collecting results...")
        
        try:
            # Monitor workers and collect results until all work is complete
            while active_workers:
                # Check for completed workers
                completed_workers = []
                for worker in workers:
                    if worker.is_alive():
                        # Worker is still running, check for results
                        pass
                    else:
                        # Worker has completed
                        if worker.pid in active_workers:
                            active_workers.remove(worker.pid)
                            completed_workers.append(worker)
                            
                            if worker.exitcode != 0:
                                self.logger.warning(f"Worker {worker.pid} exited with code {worker.exitcode}")
                            else:
                                self.logger.debug(f"Worker {worker.pid} completed successfully")
                
                # Collect any available results (non-blocking)
                self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
                
                # Log progress periodically
                if len(individual_results) + len(failed_executions) > 0 and (len(individual_results) + len(failed_executions)) % 5 == 0:
                    self.logger.info(f"Progress: {len(individual_results)} completed, {len(failed_executions)} failed, {len(active_workers)} workers active")
                
                # Small delay to prevent busy waiting
                time.sleep(0.1)
            
            # Step 5: Final result collection
            self.logger.info("All workers completed, collecting final results...")
            self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
            
        except KeyboardInterrupt:
            self.logger.warning("Received interrupt signal, initiating graceful shutdown...")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        except Exception as e:
            error_category = _categorize_error(e, "main execution")
            self.logger.error(f"Error during execution ({error_category}): {e}")
            self.logger.debug(f"Execution error details: {traceback.format_exc()}")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        finally:
            # Step 6: Cleanup resources
            self._cleanup_resources(workers, stub_queue, gpu_pool, result_queue, error_queue)
        
        # Step 7: Final summary
        total_processed = len(individual_results) + len(failed_executions)
        self.logger.info(f"Execution completed: {len(individual_results)} successful, {len(failed_executions)} failed")
        self.logger.info(f"Success rate: {len(individual_results)/total_processed*100:.1f}%" if total_processed > 0 else "No work processed")
        
        return ExecutionResults(
            execution_summary={},  # Will be filled by caller
            individual_results=individual_results,
            failed_executions=failed_executions,
            execution_time=0.0,  # Will be filled by caller
            progress=ExecutionProgress(
                total_stubs=len(pending_stubs),
                completed_stubs=len(individual_results),
                failed_stubs=len(failed_executions)
            )
        )
    
    def _collect_available_results(
        self,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue,
        individual_results: List[BenchmarkResult],
        failed_executions: List[BenchmarkFailure]
    ) -> None:
        """Collect all available results from queues without blocking.
        
        Args:
            result_queue: Queue containing successful benchmark results
            error_queue: Queue containing benchmark failures
            individual_results: List to append successful results to
            failed_executions: List to append failures to
        """
        # Collect all available successful results
        while True:
            try:
                result = result_queue.get_nowait()
                individual_results.append(result)
            except Empty:
                break
        
        # Collect all available errors
        while True:
            try:
                error = error_queue.get_nowait()
                if isinstance(error, BenchmarkFailure):
                    failed_executions.append(error)
                else:
                    # Handle worker-level errors
                    self.logger.error(f"Worker error: {error}")
            except Empty:
                break
    
    def _graceful_shutdown(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Perform graceful shutdown of worker processes and queues.
        
        Args:
            workers: List of worker processes to terminate
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.info("Initiating graceful shutdown...")
        
        try:
            # Signal workers to stop by adding sentinel values
            sentinel_count = 0
            for _ in range(len(workers)):
                try:
                    stub_queue.put_nowait(None)
                    sentinel_count += 1
                except Exception as e:
                    self.logger.debug(f"Could not add sentinel to queue: {e}")
                    break  # Queue is full, stop adding sentinels
            
            self.logger.info(f"Added {sentinel_count} shutdown signals to work queue")
            
            # Wait for workers to finish with timeout
            self.logger.info("Waiting for workers to finish...")
            for i, worker in enumerate(workers):
                try:
                    worker.join(timeout=30.0)  # 30 second timeout per worker
                    if worker.is_alive():
                        self.logger.warning(f"Force terminating worker {worker.pid}")
                        worker.terminate()
                        worker.join(timeout=5.0)
                        if worker.is_alive():
                            self.logger.error(f"Failed to terminate worker {worker.pid}, killing process")
                            worker.kill()
                            worker.join(timeout=2.0)
                    else:
                        self.logger.debug(f"Worker {worker.pid} terminated successfully")
                except Exception as e:
                    self.logger.error(f"Error during worker {worker.pid} shutdown: {e}")
            
            self.logger.info("Graceful shutdown completed")
            
        except Exception as e:
            self.logger.error(f"Error during graceful shutdown: {e}")
            # Force cleanup of remaining workers
            for worker in workers:
                if worker.is_alive():
                    try:
                        worker.kill()
                    except Exception:
                        pass
    
    def _cleanup_resources(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Clean up all multiprocessing resources.
        
        Args:
            workers: List of worker processes
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.debug("Cleaning up multiprocessing resources...")
        
        # Close all queues
        try:
            stub_queue.close()
            gpu_pool.close()
            result_queue.close()
            error_queue.close()
        except Exception as e:
            self.logger.warning(f"Error closing queues: {e}")
        
        # Join any remaining workers
        for worker in workers:
            if worker.is_alive():
                self.logger.warning(f"Force terminating remaining worker {worker.pid}")
                worker.terminate()
                worker.join(timeout=5.0)
                if worker.is_alive():
                    worker.kill()
        
        self.logger.debug("Resource cleanup completed")
    
    def _validate_gpu_ids(self, gpu_ids: List[int]) -> List[int]:
        """Validate and return GPU IDs."""
        if not gpu_ids:
            raise ValueError("gpu_ids cannot be empty")
        
        if not all(isinstance(gpu_id, int) and gpu_id >= 0 for gpu_id in gpu_ids):
            raise ValueError("All GPU IDs must be non-negative integers")
        
        # Remove duplicates while preserving order
        seen = set()
        unique_gpu_ids = []
        for gpu_id in gpu_ids:
            if gpu_id not in seen:
                seen.add(gpu_id)
                unique_gpu_ids.append(gpu_id)
        
        return unique_gpu_ids
    
    def _validate_max_concurrent_runs(self, max_concurrent_runs: int) -> int:
        """Validate and return max_concurrent_runs."""
        if not isinstance(max_concurrent_runs, int) or max_concurrent_runs <= 0:
            raise ValueError("max_concurrent_runs must be a positive integer")
        
        if max_concurrent_runs > len(self.gpu_ids) if hasattr(self, 'gpu_ids') else True:
            # This is just a warning - user might want more concurrent runs than GPUs
            # for benchmarks that don't require much GPU memory
            pass
        
        return max_concurrent_runs
    
    def _setup_logging(self) -> None:
        """Setup logging configuration."""
        self.logger = logging.getLogger(f"{__name__}.BenchmarkExecutor")
        
        if self.verbose and not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def _validate_gpu_setup(self) -> None:
        """Validate GPU availability."""
        try:
            validate_gpu_availability(self.gpu_ids)
            self.logger.info(f"GPU validation passed for devices: {self.gpu_ids}")
        except (RuntimeError, ValueError) as e:
            self.logger.error(f"GPU validation failed: {e}")
            raise
    
    def _validate_matrix_parameters(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig
    ) -> None:
        """Validate benchmark matrix parameters."""
        if not model_names:
            raise ValueError("model_names cannot be empty")
        
        if not sparse_attention_configs:
            raise ValueError("sparse_attention_configs cannot be empty")
        
        if not benchmark_configs:
            raise ValueError("benchmark_configs cannot be empty")
        
        # Validate model names
        for model_name in model_names:
            if not isinstance(model_name, str) or not model_name.strip():
                raise ValueError(f"Invalid model name: {model_name}")
        
        # Validate sparse attention configs
        for sparse_config in sparse_attention_configs:
            if not isinstance(sparse_config, tuple) or len(sparse_config) != 2:
                raise ValueError(f"sparse_attention_configs must be list of (name, config) tuples")
            
            config_name, config_obj = sparse_config
            if not isinstance(config_name, str) or not config_name.strip():
                raise ValueError(f"Sparse config name must be non-empty string: {config_name}")
        
        # Validate benchmark configs
        for benchmark_config in benchmark_configs:
            if not isinstance(benchmark_config, BenchmarkConfig):
                raise ValueError(f"benchmark_configs must contain BenchmarkConfig instances")
            # Additional validation could be added here if needed
        
        # Validate adapter config
        if not isinstance(adapter_config, AdapterConfig):
            raise ValueError("adapter_config must be an AdapterConfig instance")
            # Additional validation could be added here if needed
        
        self.logger.info("Matrix parameter validation passed")

```

```python
"""BenchmarkExecutor: Parallel GPU execution for benchmark matrix experiments.

This module contains the main BenchmarkExecutor class that orchestrates parallel 
execution of benchmark experiments across the model × sparse_attention_config × 
benchmark × subset matrix with dynamic GPU allocation and resumability.
"""

import logging
import multiprocessing
import os
import signal
import sys
import time
import traceback
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from queue import Empty
from contextlib import contextmanager
from sparse_attention_hub.metric_logging.logger import MicroMetricLogger

# Set multiprocessing start method to 'spawn' for CUDA compatibility
if multiprocessing.get_start_method(allow_none=True) != 'spawn':
    multiprocessing.set_start_method('spawn', force=True)

from sparse_attention_hub.sparse_attention.base import SparseAttentionConfig

from .executor_config import (
    AdapterConfig,
    BenchmarkConfig,
    BenchmarkStub,
    ExecutionResults,
    ExecutionProgress,
    BenchmarkResult,
    BenchmarkFailure,
    filter_existing_results,
    generate_benchmark_stubs,

)
from .benchmark_registry import (
    ensure_benchmarks_loaded,
    create_benchmark_instance
)

from .utils.gpu import (
    cleanup_gpu_memory,
    release_gpu_to_pool,
    acquire_gpu_from_pool,
    create_gpu_pool,
    validate_gpu_availability,
)

# Global flag for graceful shutdown
_shutdown_requested = False


@contextmanager
def set_gpu(gpu_id: int):
    """Context manager to set the current CUDA device.
    
    Args:
        gpu_id: The GPU device ID to set as current device
        
    Yields:
        None: The context manager yields control to the with block
    """
    import torch
    torch.cuda.set_device(gpu_id)
    yield


def _signal_handler(signum: int, frame: Any) -> None:
    """Handle shutdown signals gracefully."""
    global _shutdown_requested
    _shutdown_requested = True
    logging.getLogger(__name__).warning(f"Received signal {signum}, initiating graceful shutdown...")


def _categorize_error(error: Exception, context: str = "") -> str:
    """Categorize errors for better handling and reporting.
    
    Args:
        error: The exception that occurred
        context: Additional context about where the error occurred
        
    Returns:
        Error category string for classification
    """
    error_type = type(error).__name__
    error_msg = str(error).lower()
    
    # GPU-related errors
    if any(keyword in error_msg for keyword in ["cuda", "gpu", "out of memory", "oom"]):
        if "out of memory" in error_msg or "oom" in error_msg:
            return "GPU_OUT_OF_MEMORY"
        elif "cuda" in error_msg:
            return "GPU_CUDA_ERROR"
        else:
            return "GPU_ERROR"
    
    # Model loading errors
    elif any(keyword in error_msg for keyword in ["model", "huggingface", "transformers", "pretrained"]):
        if "not found" in error_msg or "doesn't exist" in error_msg:
            return "MODEL_NOT_FOUND"
        elif "tokenizer" in error_msg:
            return "TOKENIZER_ERROR"
        else:
            return "MODEL_LOADING_ERROR"
    
    # Dataset/benchmark errors
    elif any(keyword in error_msg for keyword in ["dataset", "benchmark", "subset"]):
        return "DATASET_ERROR"
    
    # Network/connection errors
    elif any(keyword in error_msg for keyword in ["connection", "timeout", "network", "http"]):
        return "NETWORK_ERROR"
    
    # File system errors
    elif any(keyword in error_msg for keyword in ["file", "directory", "permission", "disk"]):
        return "FILESYSTEM_ERROR"
    
    # Multiprocessing errors
    elif any(keyword in error_msg for keyword in ["queue", "process", "multiprocessing"]):
        return "MULTIPROCESSING_ERROR"
    
    # Default categories
    elif isinstance(error, TimeoutError):
        return "TIMEOUT_ERROR"
    elif isinstance(error, KeyboardInterrupt):
        return "INTERRUPT_ERROR"
    elif isinstance(error, MemoryError):
        return "SYSTEM_MEMORY_ERROR"
    else:
        return "UNKNOWN_ERROR"


def _benchmark_worker(
    stub_queue: multiprocessing.Queue,
    gpu_pool: multiprocessing.Queue,
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue,
    timeout_per_benchmark: float = 3600.0
) -> None:
    """Worker function to execute benchmarks with dynamic GPU allocation.
    
    This function runs in a separate process and handles:
    1. Acquiring GPUs from the shared pool
    2. Creating model adapters with sparse attention configurations
    3. Executing benchmarks with proper error handling
    4. Releasing GPUs back to the pool
    5. Reporting results and errors
    
    Args:
        stub_queue: Queue containing BenchmarkStub instances to process
        gpu_pool: Shared queue containing available GPU IDs
        result_queue: Queue for reporting successful benchmark results
        error_queue: Queue for reporting benchmark failures
        timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
    """
    worker_id = os.getpid()
    logger = logging.getLogger(f"{__name__}.worker_{worker_id}")
    
    # Set up signal handlers for graceful shutdown
    signal.signal(signal.SIGTERM, _signal_handler)
    signal.signal(signal.SIGINT, _signal_handler)
    
    # Track resources for cleanup
    current_gpu_id: Optional[int] = None
    adapter = None

    try:
        while True:
            # Check for shutdown signal
            if _shutdown_requested:
                logger.info(f"Worker {worker_id}: Shutdown requested, exiting")
                break
            
            # Step 1: Get next benchmark stub from queue
            try:
                stub: BenchmarkStub = stub_queue.get(timeout=1.0)  # 1 second timeout
            except Empty:
                # No more work to do
                logger.info(f"Worker {worker_id}: No more work, shutting down")
                break
            
            # Check for sentinel value (shutdown signal)
            if stub is None:
                logger.info(f"Worker {worker_id}: Received shutdown signal, exiting")
                break
            
            # Step 2: Acquire GPU from pool
            try:
                current_gpu_id = acquire_gpu_from_pool(gpu_pool, timeout=30.0)
                logger.info(f"Worker {worker_id}: Acquired GPU {current_gpu_id} for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}")
            except Exception as e:
                error_category = _categorize_error(e, "GPU acquisition")
                error_msg = f"Failed to acquire GPU for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                error_queue.put(BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=0.0
                ))
                continue

            # Step 3: Run the benchmark on the current GPU
            start_time = time.time()
            execution_time = 0.0  # Initialize execution_time
            
            try:
                with set_gpu(current_gpu_id):
                    logger.debug(f"Worker {worker_id}: Set CUDA device to {current_gpu_id}")
                    execution_success = False

                    # Create model adapter with sparse attention config
                    logger.info(f"Worker {worker_id}: Creating model adapter for {stub.model_name}")
                    
                    # Import here to avoid issues with multiprocessing
                    from sparse_attention_hub.adapters.huggingface import ModelAdapterHF
                    
                    adapter = ModelAdapterHF(
                        model_name=stub.model_name,
                        sparse_attention_config=stub.sparse_attention_config,
                        model_kwargs=stub.adapter_config.model_kwargs,
                        tokenizer_kwargs=stub.adapter_config.tokenizer_kwargs
                    )
                    
                    # Create benchmark instance
                    logger.info(f"Worker {worker_id}: Creating benchmark instance for {stub.benchmark_name}")
                    benchmark = create_benchmark_instance(
                        benchmark_name=stub.benchmark_name,
                        subsets=[stub.subset] if stub.subset else None
                    )
                    
                    # Execute benchmark
                    logger.info(f"Worker {worker_id}: Executing benchmark {stub.benchmark_name} on GPU {current_gpu_id}")
                    metric_logger = MicroMetricLogger()
                    metric_logger.configure_logging(log_path=stub.result_dir, 
                                    enabled_metrics=["research_attention_density", "research_attention_output_error"],
                                    max_records=10000,
                                    sampling_factor=0.2
                                )
                    metrics = benchmark.run_benchmark(
                        adapter=adapter,
                        result_dir=stub.result_dir,
                        generation_kwargs=stub.generation_kwargs,
                        request_kwargs=stub.request_kwargs
                    )
                    metric_logger.flush()
                    
                    execution_time = time.time() - start_time
                    execution_success = True
                    
                    # Report successful result
                    result = BenchmarkResult(
                        stub=stub,
                        metrics=metrics,
                        execution_time=execution_time,
                        success=True
                    )
                    result_queue.put(result)
                    
                    logger.info(f"Worker {worker_id}: Successfully completed {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name} in {execution_time:.2f}s")

            except Exception as e:
                execution_time = time.time() - start_time  # Calculate execution time even on failure
                error_category = _categorize_error(e, "benchmark execution")
                error_msg = f"Benchmark execution failed: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                
                # Log detailed error information for debugging
                logger.debug(f"Worker {worker_id}: Error details: {traceback.format_exc()}")
                
                # Report failure
                failure = BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=execution_time
                )
                error_queue.put(failure)
                continue
            
            finally:
                # Step 4: Cleanup and release GPU
                _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
                current_gpu_id = None
                adapter = None
    
    except Exception as worker_error:
        logger.error(f"Worker {worker_id}: Critical worker error: {worker_error}")
        logger.debug(f"Worker {worker_id}: Critical error details: {traceback.format_exc()}")
        
        # Cleanup any remaining resources
        _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
        
        # Report worker failure to error queue
        error_queue.put({
            "worker_id": worker_id,
            "error_type": "WORKER_CRITICAL_ERROR",
            "error_message": str(worker_error),
            "error_traceback": traceback.format_exc()
        })


def _cleanup_worker_resources(
    gpu_pool: multiprocessing.Queue,
    gpu_id: Optional[int],
    adapter: Any,
    logger: logging.Logger,
    worker_id: int
) -> None:
    """Clean up worker resources including GPU memory and adapter.
    
    Args:
        gpu_pool: GPU pool queue to return GPU to
        gpu_id: GPU ID to release (can be None) - this is the actual GPU ID from the pool
        adapter: Model adapter to cleanup (can be None)
        logger: Logger instance for the worker
        worker_id: Worker process ID for logging
    """
    try:
        # Clean up adapter if it exists
        if adapter is not None:
            try:
                # Force garbage collection of adapter
                del adapter
                import gc
                gc.collect()
                logger.debug(f"Worker {worker_id}: Cleaned up model adapter")
            except Exception as e:
                logger.warning(f"Worker {worker_id}: Failed to cleanup adapter: {e}")
        
        # Clean up GPU memory and release GPU
        if gpu_id is not None:
            try:
                cleanup_gpu_memory(gpu_id)
                logger.debug(f"Worker {worker_id}: Cleaned up GPU memory for GPU {gpu_id}")
            
                # Release GPU back to pool (always use the original GPU ID for the pool)
                release_gpu_to_pool(gpu_pool, gpu_id)
                logger.debug(f"Worker {worker_id}: Released GPU {gpu_id} back to pool")
                
            except Exception as cleanup_error:
                logger.warning(f"Worker {worker_id}: GPU cleanup failed for GPU {gpu_id}: {cleanup_error}")
                # Try to release GPU even if cleanup failed
                try:
                    release_gpu_to_pool(gpu_pool, gpu_id)
                except Exception:
                    logger.error(f"Worker {worker_id}: Failed to release GPU {gpu_id} to pool")
    
    except Exception as e:
        logger.error(f"Worker {worker_id}: Error during resource cleanup: {e}")


class BenchmarkExecutor:
    """Parallel benchmark executor with dynamic GPU allocation and resumability.
    
    Executes benchmark experiments across a model × sparse_attention_config × 
    benchmark × subset matrix with parallel GPU execution and automatic resumability.
    
    Features:
    - Dynamic GPU allocation from specified gpu_ids
    - Subset-level resumability (skip completed experiments)
    - Parallel execution with configurable concurrency
    - Comprehensive error handling and logging
    - Result aggregation and summary generation
    
    Example:
        >>> executor = BenchmarkExecutor(
        ...     gpu_ids=[0, 1, 2, 3],
        ...     max_concurrent_runs=4,
        ...     base_result_dir="./benchmark_results"
        ... )
        >>> 
        >>> results = executor.run_benchmark_matrix(
        ...     model_names=["model1", "model2"],
        ...     sparse_attention_configs=[("dense", None), ("sparse", config)],
        ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
        ...     adapter_config=AdapterConfig()
        ... )
    """
    
    def __init__(
        self,
        gpu_ids: List[int],
        max_concurrent_runs: int,
        base_result_dir: str = "./benchmark_results",
        enable_resumability: bool = True,
        result_file_validation: bool = True,
        required_result_files: Optional[List[str]] = None,
        timeout_per_benchmark: float = 360000.0,  # 100 hours default
        verbose: bool = True,
        max_records: Optional[int] = None,  # Maximum number of metric events to log
        sampling_factor: float = 1.0,  # Probability of logging each metric event (0.0-1.0)
    ):
        """Initialize the BenchmarkExecutor.
        
        Args:
            gpu_ids: List of GPU device IDs to use for parallel execution
            max_concurrent_runs: Maximum number of concurrent benchmark runs
            base_result_dir: Base directory for storing benchmark results
            enable_resumability: Whether to skip already completed experiments
            result_file_validation: Whether to check for specific result files
            required_result_files: List of files required for completion check
            timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
            verbose: Whether to enable verbose logging
            max_records: Maximum number of metric events to log (None for unlimited)
            sampling_factor: Probability of logging each metric event (0.0-1.0)
            
        Raises:
            ValueError: If configuration parameters are invalid
            RuntimeError: If GPU validation fails
        """
        # Validate and store configuration
        self.gpu_ids = self._validate_gpu_ids(gpu_ids)
        self.max_concurrent_runs = self._validate_max_concurrent_runs(max_concurrent_runs)
        self.base_result_dir = Path(base_result_dir).resolve()
        self.enable_resumability = enable_resumability
        self.result_file_validation = result_file_validation
        self.required_result_files = required_result_files or ["raw_results.csv"]
        self.timeout_per_benchmark = timeout_per_benchmark
        self.verbose = verbose
        
        # Metric logging configuration
        self.max_records = max_records
        self.sampling_factor = max(0.0, min(1.0, sampling_factor))  # Clamp to [0.0, 1.0]
        
        # Initialize logging
        self._setup_logging()
        
        # Set up signal handlers for graceful shutdown
        self._setup_signal_handlers()
        
        # Validate GPU availability
        self._validate_gpu_setup()
        
        # Load all available benchmarks
        ensure_benchmarks_loaded()
        

        
        self.logger.info(f"BenchmarkExecutor initialized:")
        self.logger.info(f"  GPU IDs: {self.gpu_ids}")
        self.logger.info(f"  Max concurrent runs: {self.max_concurrent_runs}")
        self.logger.info(f"  Base result directory: {self.base_result_dir}")
        self.logger.info(f"  Resumability: {'enabled' if self.enable_resumability else 'disabled'}")
    
    def _setup_signal_handlers(self) -> None:
        """Set up signal handlers for graceful shutdown."""
        signal.signal(signal.SIGTERM, _signal_handler)
        signal.signal(signal.SIGINT, _signal_handler)
        
        # Register cleanup function to run at exit
        import atexit
        atexit.register(self._cleanup_on_exit)
    
    def _cleanup_on_exit(self) -> None:
        """Cleanup function called when the process exits."""
        if hasattr(self, 'logger'):
            self.logger.info("BenchmarkExecutor cleanup on exit")
        
        # Reset global shutdown flag
        global _shutdown_requested
        _shutdown_requested = False

    def run_benchmark_matrix(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig,
        generation_kwargs: Optional[Dict[str, Any]] = None,
        request_kwargs: Optional[Dict[str, Any]] = None
    ) -> ExecutionResults:
        """Execute benchmark matrix with parallel GPU execution.
        
        Runs all combinations of model_names × sparse_attention_configs × 
        benchmark_configs × subsets in parallel across available GPUs.
        
        Args:
            model_names: List of model names to benchmark
            sparse_attention_configs: List of (name, config) tuples for sparse attention
            benchmark_configs: List of benchmark configurations
            adapter_config: Configuration for model adapter
            generation_kwargs: Optional generation parameters
            request_kwargs: Optional request processing parameters
            
        Returns:
            ExecutionResults containing comprehensive execution information
            
        Example:
            >>> results = executor.run_benchmark_matrix(
            ...     model_names=["meta-llama/Llama-3.1-8B-Instruct"],
            ...     sparse_attention_configs=[("dense", None)],
            ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
            ...     adapter_config=AdapterConfig()
            ... )
            >>> print(f"Completed: {results.completed_count}/{results.total_count}")
        """
        # Initialize execution
        self.logger.info("Starting benchmark matrix execution")
        
        try:
            # Step 1: Validate input parameters
            self._validate_matrix_parameters(
                model_names, sparse_attention_configs, benchmark_configs, adapter_config
            )
            
            # Step 2: Generate benchmark stubs (matrix expansion)
            self.logger.info("Generating benchmark matrix...")
            all_stubs = generate_benchmark_stubs(
                model_names=model_names,
                sparse_attention_configs=sparse_attention_configs,
                benchmark_configs=benchmark_configs,
                adapter_config=adapter_config,
                generation_kwargs=generation_kwargs,
                request_kwargs=request_kwargs,
                base_result_dir=str(self.base_result_dir)
            )
            
            total_combinations = len(all_stubs)
            self.logger.info(f"Generated {total_combinations} benchmark combinations:")
            self.logger.info(f"  {len(model_names)} models × {len(sparse_attention_configs)} sparse configs × "
                           f"{sum(len(b.subsets) if b.subsets else 1 for b in benchmark_configs)} benchmark-subsets")
            
            # Step 3: Filter for resumability (if enabled)
            if self.enable_resumability:
                self.logger.info("Checking for existing results (resumability)...")
                pending_stubs, completed_stubs = filter_existing_results(
                    all_stubs,
                    check_result_files=self.result_file_validation,
                    required_files=self.required_result_files,
                    verbose=self.verbose
                )
                
                self.logger.info(f"Resumability check: {len(completed_stubs)} completed, {len(pending_stubs)} pending")
            else:
                pending_stubs = all_stubs
                completed_stubs = []
                self.logger.info("Resumability disabled - will execute all combinations")
            
            # Step 4: Early exit if nothing to do
            if not pending_stubs:
                self.logger.info("No pending benchmarks to execute. All experiments already completed!")
                
                execution_summary = {
                    "total_combinations": total_combinations,
                    "models": model_names,
                    "sparse_configs": [name for name, _ in sparse_attention_configs],
                    "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs]
                }
                
                progress = ExecutionProgress(
                    total_stubs=total_combinations,
                    skipped_stubs=len(completed_stubs),
                    completed_stubs=0,
                    failed_stubs=0
                )
                
                return ExecutionResults(
                    execution_summary=execution_summary,
                    individual_results=[],
                    failed_executions=[],
                    execution_time=0.0,
                    progress=progress
                )
            

            
            # Step 6: Execute pending benchmarks with worker processes
            self.logger.info(f"Executing {len(pending_stubs)} pending benchmarks with {self.max_concurrent_runs} workers...")
            
            start_time = time.time()
            results = self._execute_with_workers(pending_stubs)
            execution_time = time.time() - start_time
            
            # Step 7: Aggregate results
            execution_summary = {
                "total_combinations": total_combinations,
                "models": model_names,
                "sparse_configs": [name for name, _ in sparse_attention_configs],
                "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs],
                "execution_time_seconds": execution_time,
                "gpu_ids_used": self.gpu_ids,
                "max_concurrent_runs": self.max_concurrent_runs
            }
            
            progress = ExecutionProgress(
                total_stubs=total_combinations,
                skipped_stubs=len(completed_stubs),
                completed_stubs=len(results.individual_results),
                failed_stubs=len(results.failed_executions)
            )
            
            final_results = ExecutionResults(
                execution_summary=execution_summary,
                individual_results=results.individual_results,
                failed_executions=results.failed_executions,
                execution_time=execution_time,
                progress=progress
            )
            
            self.logger.info(f"Benchmark matrix execution completed:")
            self.logger.info(f"  Total time: {execution_time:.2f}s")
            self.logger.info(f"  Successful: {len(results.individual_results)}")
            self.logger.info(f"  Failed: {len(results.failed_executions)}")
            self.logger.info(f"  Skipped: {len(completed_stubs)}")
            
            return final_results
            
        except Exception as e:
            self.logger.error(f"Benchmark execution failed: {e}")
            raise
    
    def _execute_with_workers(self, pending_stubs: List[BenchmarkStub]) -> ExecutionResults:
        """Execute benchmark stubs using worker processes.
        
        This method implements comprehensive process pool management with:
        - Dynamic work distribution through shared queues
        - Graceful process startup and shutdown
        - Real-time result collection and error handling
        - Process health monitoring and recovery
        
        Args:
            pending_stubs: List of benchmark stubs to execute
            
        Returns:
            ExecutionResults containing all results and failures
        """
        self.logger.info(f"Starting execution with {len(pending_stubs)} stubs using {self.max_concurrent_runs} workers")
        
        # Step 1: Create and configure shared queues
        stub_queue = multiprocessing.Queue(maxsize=len(pending_stubs) + 100)  # Buffer for queue operations
        gpu_pool = create_gpu_pool(self.gpu_ids)
        result_queue = multiprocessing.Queue()
        error_queue = multiprocessing.Queue()
        
        # Step 2: Populate work queue with all stubs
        self.logger.info("Populating work queue...")
        for stub in pending_stubs:
            stub_queue.put(stub)
        
        # Add sentinel values to signal workers to stop
        for _ in range(self.max_concurrent_runs):
            stub_queue.put(None)  # Sentinel value
        
        # Step 3: Create and start worker processes
        workers = []
        worker_pids = []
        
        self.logger.info(f"Starting {self.max_concurrent_runs} worker processes...")
        for i in range(self.max_concurrent_runs):
            worker = multiprocessing.Process(
                target=_benchmark_worker,
                args=(stub_queue, gpu_pool, result_queue, error_queue, self.timeout_per_benchmark),
                name=f"benchmark_worker_{i}"
            )
            worker.daemon = True  # Ensure workers are terminated when main process exits
            worker.start()
            workers.append(worker)
            worker_pids.append(worker.pid)
            self.logger.debug(f"Started worker {i+1}/{self.max_concurrent_runs} (PID: {worker.pid})")
        
        self.logger.info(f"All {self.max_concurrent_runs} workers started successfully")
        
        # Step 4: Monitor and collect results in real-time
        individual_results: List[BenchmarkResult] = []
        failed_executions: List[BenchmarkFailure] = []
        active_workers = set(worker_pids)
        
        self.logger.info("Monitoring worker processes and collecting results...")
        
        try:
            # Monitor workers and collect results until all work is complete
            while active_workers:
                # Check for completed workers
                completed_workers = []
                for worker in workers:
                    if worker.is_alive():
                        # Worker is still running, check for results
                        pass
                    else:
                        # Worker has completed
                        if worker.pid in active_workers:
                            active_workers.remove(worker.pid)
                            completed_workers.append(worker)
                            
                            if worker.exitcode != 0:
                                self.logger.warning(f"Worker {worker.pid} exited with code {worker.exitcode}")
                            else:
                                self.logger.debug(f"Worker {worker.pid} completed successfully")
                
                # Collect any available results (non-blocking)
                self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
                
                # Log progress periodically
                if len(individual_results) + len(failed_executions) > 0 and (len(individual_results) + len(failed_executions)) % 5 == 0:
                    self.logger.info(f"Progress: {len(individual_results)} completed, {len(failed_executions)} failed, {len(active_workers)} workers active")
                
                # Small delay to prevent busy waiting
                time.sleep(0.1)
            
            # Step 5: Final result collection
            self.logger.info("All workers completed, collecting final results...")
            self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
            
        except KeyboardInterrupt:
            self.logger.warning("Received interrupt signal, initiating graceful shutdown...")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        except Exception as e:
            error_category = _categorize_error(e, "main execution")
            self.logger.error(f"Error during execution ({error_category}): {e}")
            self.logger.debug(f"Execution error details: {traceback.format_exc()}")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        finally:
            # Step 6: Cleanup resources
            self._cleanup_resources(workers, stub_queue, gpu_pool, result_queue, error_queue)
        
        # Step 7: Final summary
        total_processed = len(individual_results) + len(failed_executions)
        self.logger.info(f"Execution completed: {len(individual_results)} successful, {len(failed_executions)} failed")
        self.logger.info(f"Success rate: {len(individual_results)/total_processed*100:.1f}%" if total_processed > 0 else "No work processed")
        
        return ExecutionResults(
            execution_summary={},  # Will be filled by caller
            individual_results=individual_results,
            failed_executions=failed_executions,
            execution_time=0.0,  # Will be filled by caller
            progress=ExecutionProgress(
                total_stubs=len(pending_stubs),
                completed_stubs=len(individual_results),
                failed_stubs=len(failed_executions)
            )
        )
    
    def _collect_available_results(
        self,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue,
        individual_results: List[BenchmarkResult],
        failed_executions: List[BenchmarkFailure]
    ) -> None:
        """Collect all available results from queues without blocking.
        
        Args:
            result_queue: Queue containing successful benchmark results
            error_queue: Queue containing benchmark failures
            individual_results: List to append successful results to
            failed_executions: List to append failures to
        """
        # Collect all available successful results
        while True:
            try:
                result = result_queue.get_nowait()
                individual_results.append(result)
            except Empty:
                break
        
        # Collect all available errors
        while True:
            try:
                error = error_queue.get_nowait()
                if isinstance(error, BenchmarkFailure):
                    failed_executions.append(error)
                else:
                    # Handle worker-level errors
                    self.logger.error(f"Worker error: {error}")
            except Empty:
                break
    
    def _graceful_shutdown(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Perform graceful shutdown of worker processes and queues.
        
        Args:
            workers: List of worker processes to terminate
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.info("Initiating graceful shutdown...")
        
        try:
            # Signal workers to stop by adding sentinel values
            sentinel_count = 0
            for _ in range(len(workers)):
                try:
                    stub_queue.put_nowait(None)
                    sentinel_count += 1
                except Exception as e:
                    self.logger.debug(f"Could not add sentinel to queue: {e}")
                    break  # Queue is full, stop adding sentinels
            
            self.logger.info(f"Added {sentinel_count} shutdown signals to work queue")
            
            # Wait for workers to finish with timeout
            self.logger.info("Waiting for workers to finish...")
            for i, worker in enumerate(workers):
                try:
                    worker.join(timeout=30.0)  # 30 second timeout per worker
                    if worker.is_alive():
                        self.logger.warning(f"Force terminating worker {worker.pid}")
                        worker.terminate()
                        worker.join(timeout=5.0)
                        if worker.is_alive():
                            self.logger.error(f"Failed to terminate worker {worker.pid}, killing process")
                            worker.kill()
                            worker.join(timeout=2.0)
                    else:
                        self.logger.debug(f"Worker {worker.pid} terminated successfully")
                except Exception as e:
                    self.logger.error(f"Error during worker {worker.pid} shutdown: {e}")
            
            self.logger.info("Graceful shutdown completed")
            
        except Exception as e:
            self.logger.error(f"Error during graceful shutdown: {e}")
            # Force cleanup of remaining workers
            for worker in workers:
                if worker.is_alive():
                    try:
                        worker.kill()
                    except Exception:
                        pass
    
    def _cleanup_resources(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Clean up all multiprocessing resources.
        
        Args:
            workers: List of worker processes
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.debug("Cleaning up multiprocessing resources...")
        
        # Close all queues
        try:
            stub_queue.close()
            gpu_pool.close()
            result_queue.close()
            error_queue.close()
        except Exception as e:
            self.logger.warning(f"Error closing queues: {e}")
        
        # Join any remaining workers
        for worker in workers:
            if worker.is_alive():
                self.logger.warning(f"Force terminating remaining worker {worker.pid}")
                worker.terminate()
                worker.join(timeout=5.0)
                if worker.is_alive():
                    worker.kill()
        
        self.logger.debug("Resource cleanup completed")
    
    def _validate_gpu_ids(self, gpu_ids: List[int]) -> List[int]:
        """Validate and return GPU IDs."""
        if not gpu_ids:
            raise ValueError("gpu_ids cannot be empty")
        
        if not all(isinstance(gpu_id, int) and gpu_id >= 0 for gpu_id in gpu_ids):
            raise ValueError("All GPU IDs must be non-negative integers")
        
        # Remove duplicates while preserving order
        seen = set()
        unique_gpu_ids = []
        for gpu_id in gpu_ids:
            if gpu_id not in seen:
                seen.add(gpu_id)
                unique_gpu_ids.append(gpu_id)
        
        return unique_gpu_ids
    
    def _validate_max_concurrent_runs(self, max_concurrent_runs: int) -> int:
        """Validate and return max_concurrent_runs."""
        if not isinstance(max_concurrent_runs, int) or max_concurrent_runs <= 0:
            raise ValueError("max_concurrent_runs must be a positive integer")
        
        if max_concurrent_runs > len(self.gpu_ids) if hasattr(self, 'gpu_ids') else True:
            # This is just a warning - user might want more concurrent runs than GPUs
            # for benchmarks that don't require much GPU memory
            pass
        
        return max_concurrent_runs
    
    def _setup_logging(self) -> None:
        """Setup logging configuration."""
        self.logger = logging.getLogger(f"{__name__}.BenchmarkExecutor")
        
        if self.verbose and not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def _validate_gpu_setup(self) -> None:
        """Validate GPU availability."""
        try:
            validate_gpu_availability(self.gpu_ids)
            self.logger.info(f"GPU validation passed for devices: {self.gpu_ids}")
        except (RuntimeError, ValueError) as e:
            self.logger.error(f"GPU validation failed: {e}")
            raise
    
    def _validate_matrix_parameters(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig
    ) -> None:
        """Validate benchmark matrix parameters."""
        if not model_names:
            raise ValueError("model_names cannot be empty")
        
        if not sparse_attention_configs:
            raise ValueError("sparse_attention_configs cannot be empty")
        
        if not benchmark_configs:
            raise ValueError("benchmark_configs cannot be empty")
        
        # Validate model names
        for model_name in model_names:
            if not isinstance(model_name, str) or not model_name.strip():
                raise ValueError(f"Invalid model name: {model_name}")
        
        # Validate sparse attention configs
        for sparse_config in sparse_attention_configs:
            if not isinstance(sparse_config, tuple) or len(sparse_config) != 2:
                raise ValueError(f"sparse_attention_configs must be list of (name, config) tuples")
            
            config_name, config_obj = sparse_config
            if not isinstance(config_name, str) or not config_name.strip():
                raise ValueError(f"Sparse config name must be non-empty string: {config_name}")
        
        # Validate benchmark configs
        for benchmark_config in benchmark_configs:
            if not isinstance(benchmark_config, BenchmarkConfig):
                raise ValueError(f"benchmark_configs must contain BenchmarkConfig instances")
            # Additional validation could be added here if needed
        
        # Validate adapter config
        if not isinstance(adapter_config, AdapterConfig):
            raise ValueError("adapter_config must be an AdapterConfig instance")
            # Additional validation could be added here if needed
        
        self.logger.info("Matrix parameter validation passed")

```

```python
"""BenchmarkExecutor: Parallel GPU execution for benchmark matrix experiments.

This module contains the main BenchmarkExecutor class that orchestrates parallel 
execution of benchmark experiments across the model × sparse_attention_config × 
benchmark × subset matrix with dynamic GPU allocation and resumability.
"""

import logging
import multiprocessing
import os
import signal
import sys
import time
import traceback
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from queue import Empty
from contextlib import contextmanager
from sparse_attention_hub.metric_logging.logger import MicroMetricLogger

# Set multiprocessing start method to 'spawn' for CUDA compatibility
if multiprocessing.get_start_method(allow_none=True) != 'spawn':
    multiprocessing.set_start_method('spawn', force=True)

from sparse_attention_hub.sparse_attention.base import SparseAttentionConfig

from .executor_config import (
    AdapterConfig,
    BenchmarkConfig,
    BenchmarkStub,
    ExecutionResults,
    ExecutionProgress,
    BenchmarkResult,
    BenchmarkFailure,
    filter_existing_results,
    generate_benchmark_stubs,

)
from .benchmark_registry import (
    ensure_benchmarks_loaded,
    create_benchmark_instance
)

from .utils.gpu import (
    cleanup_gpu_memory,
    release_gpu_to_pool,
    acquire_gpu_from_pool,
    create_gpu_pool,
    validate_gpu_availability,
)

# Global flag for graceful shutdown
_shutdown_requested = False


@contextmanager
def set_gpu(gpu_id: int):
    """Context manager to set the current CUDA device.
    
    Args:
        gpu_id: The GPU device ID to set as current device
        
    Yields:
        None: The context manager yields control to the with block
    """
    import torch
    torch.cuda.set_device(gpu_id)
    yield


def _signal_handler(signum: int, frame: Any) -> None:
    """Handle shutdown signals gracefully."""
    global _shutdown_requested
    _shutdown_requested = True
    logging.getLogger(__name__).warning(f"Received signal {signum}, initiating graceful shutdown...")


def _categorize_error(error: Exception, context: str = "") -> str:
    """Categorize errors for better handling and reporting.
    
    Args:
        error: The exception that occurred
        context: Additional context about where the error occurred
        
    Returns:
        Error category string for classification
    """
    error_type = type(error).__name__
    error_msg = str(error).lower()
    
    # GPU-related errors
    if any(keyword in error_msg for keyword in ["cuda", "gpu", "out of memory", "oom"]):
        if "out of memory" in error_msg or "oom" in error_msg:
            return "GPU_OUT_OF_MEMORY"
        elif "cuda" in error_msg:
            return "GPU_CUDA_ERROR"
        else:
            return "GPU_ERROR"
    
    # Model loading errors
    elif any(keyword in error_msg for keyword in ["model", "huggingface", "transformers", "pretrained"]):
        if "not found" in error_msg or "doesn't exist" in error_msg:
            return "MODEL_NOT_FOUND"
        elif "tokenizer" in error_msg:
            return "TOKENIZER_ERROR"
        else:
            return "MODEL_LOADING_ERROR"
    
    # Dataset/benchmark errors
    elif any(keyword in error_msg for keyword in ["dataset", "benchmark", "subset"]):
        return "DATASET_ERROR"
    
    # Network/connection errors
    elif any(keyword in error_msg for keyword in ["connection", "timeout", "network", "http"]):
        return "NETWORK_ERROR"
    
    # File system errors
    elif any(keyword in error_msg for keyword in ["file", "directory", "permission", "disk"]):
        return "FILESYSTEM_ERROR"
    
    # Multiprocessing errors
    elif any(keyword in error_msg for keyword in ["queue", "process", "multiprocessing"]):
        return "MULTIPROCESSING_ERROR"
    
    # Default categories
    elif isinstance(error, TimeoutError):
        return "TIMEOUT_ERROR"
    elif isinstance(error, KeyboardInterrupt):
        return "INTERRUPT_ERROR"
    elif isinstance(error, MemoryError):
        return "SYSTEM_MEMORY_ERROR"
    else:
        return "UNKNOWN_ERROR"


def _benchmark_worker(
    stub_queue: multiprocessing.Queue,
    gpu_pool: multiprocessing.Queue,
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue,
    timeout_per_benchmark: float = 3600.0,
    max_records: Optional[int] = None,
    sampling_factor: float = 1.0
) -> None:
    """Worker function to execute benchmarks with dynamic GPU allocation.
    
    This function runs in a separate process and handles:
    1. Acquiring GPUs from the shared pool
    2. Creating model adapters with sparse attention configurations
    3. Executing benchmarks with proper error handling
    4. Releasing GPUs back to the pool
    5. Reporting results and errors
    
    Args:
        stub_queue: Queue containing BenchmarkStub instances to process
        gpu_pool: Shared queue containing available GPU IDs
        result_queue: Queue for reporting successful benchmark results
        error_queue: Queue for reporting benchmark failures
        timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
    """
    worker_id = os.getpid()
    logger = logging.getLogger(f"{__name__}.worker_{worker_id}")
    
    # Set up signal handlers for graceful shutdown
    signal.signal(signal.SIGTERM, _signal_handler)
    signal.signal(signal.SIGINT, _signal_handler)
    
    # Track resources for cleanup
    current_gpu_id: Optional[int] = None
    adapter = None

    try:
        while True:
            # Check for shutdown signal
            if _shutdown_requested:
                logger.info(f"Worker {worker_id}: Shutdown requested, exiting")
                break
            
            # Step 1: Get next benchmark stub from queue
            try:
                stub: BenchmarkStub = stub_queue.get(timeout=1.0)  # 1 second timeout
            except Empty:
                # No more work to do
                logger.info(f"Worker {worker_id}: No more work, shutting down")
                break
            
            # Check for sentinel value (shutdown signal)
            if stub is None:
                logger.info(f"Worker {worker_id}: Received shutdown signal, exiting")
                break
            
            # Step 2: Acquire GPU from pool
            try:
                current_gpu_id = acquire_gpu_from_pool(gpu_pool, timeout=30.0)
                logger.info(f"Worker {worker_id}: Acquired GPU {current_gpu_id} for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}")
            except Exception as e:
                error_category = _categorize_error(e, "GPU acquisition")
                error_msg = f"Failed to acquire GPU for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                error_queue.put(BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=0.0
                ))
                continue

            # Step 3: Run the benchmark on the current GPU
            start_time = time.time()
            execution_time = 0.0  # Initialize execution_time
            
            try:
                with set_gpu(current_gpu_id):
                    logger.debug(f"Worker {worker_id}: Set CUDA device to {current_gpu_id}")
                    execution_success = False

                    # Create model adapter with sparse attention config
                    logger.info(f"Worker {worker_id}: Creating model adapter for {stub.model_name}")
                    
                    # Import here to avoid issues with multiprocessing
                    from sparse_attention_hub.adapters.huggingface import ModelAdapterHF
                    
                    adapter = ModelAdapterHF(
                        model_name=stub.model_name,
                        sparse_attention_config=stub.sparse_attention_config,
                        model_kwargs=stub.adapter_config.model_kwargs,
                        tokenizer_kwargs=stub.adapter_config.tokenizer_kwargs
                    )
                    
                    # Create benchmark instance
                    logger.info(f"Worker {worker_id}: Creating benchmark instance for {stub.benchmark_name}")
                    benchmark = create_benchmark_instance(
                        benchmark_name=stub.benchmark_name,
                        subsets=[stub.subset] if stub.subset else None
                    )
                    
                    # Execute benchmark
                    logger.info(f"Worker {worker_id}: Executing benchmark {stub.benchmark_name} on GPU {current_gpu_id}")
                    metric_logger = MicroMetricLogger()
                    metric_logger.configure_logging(log_path=stub.result_dir, 
                                    enabled_metrics=["research_attention_density", "research_attention_output_error"],
                                    max_records=10000,
                                    sampling_factor=0.2
                                )
                    metrics = benchmark.run_benchmark(
                        adapter=adapter,
                        result_dir=stub.result_dir,
                        generation_kwargs=stub.generation_kwargs,
                        request_kwargs=stub.request_kwargs
                    )
                    metric_logger.flush()
                    
                    execution_time = time.time() - start_time
                    execution_success = True
                    
                    # Report successful result
                    result = BenchmarkResult(
                        stub=stub,
                        metrics=metrics,
                        execution_time=execution_time,
                        success=True
                    )
                    result_queue.put(result)
                    
                    logger.info(f"Worker {worker_id}: Successfully completed {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name} in {execution_time:.2f}s")

            except Exception as e:
                execution_time = time.time() - start_time  # Calculate execution time even on failure
                error_category = _categorize_error(e, "benchmark execution")
                error_msg = f"Benchmark execution failed: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                
                # Log detailed error information for debugging
                logger.debug(f"Worker {worker_id}: Error details: {traceback.format_exc()}")
                
                # Report failure
                failure = BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=execution_time
                )
                error_queue.put(failure)
                continue
            
            finally:
                # Step 4: Cleanup and release GPU
                _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
                current_gpu_id = None
                adapter = None
    
    except Exception as worker_error:
        logger.error(f"Worker {worker_id}: Critical worker error: {worker_error}")
        logger.debug(f"Worker {worker_id}: Critical error details: {traceback.format_exc()}")
        
        # Cleanup any remaining resources
        _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
        
        # Report worker failure to error queue
        error_queue.put({
            "worker_id": worker_id,
            "error_type": "WORKER_CRITICAL_ERROR",
            "error_message": str(worker_error),
            "error_traceback": traceback.format_exc()
        })


def _cleanup_worker_resources(
    gpu_pool: multiprocessing.Queue,
    gpu_id: Optional[int],
    adapter: Any,
    logger: logging.Logger,
    worker_id: int
) -> None:
    """Clean up worker resources including GPU memory and adapter.
    
    Args:
        gpu_pool: GPU pool queue to return GPU to
        gpu_id: GPU ID to release (can be None) - this is the actual GPU ID from the pool
        adapter: Model adapter to cleanup (can be None)
        logger: Logger instance for the worker
        worker_id: Worker process ID for logging
    """
    try:
        # Clean up adapter if it exists
        if adapter is not None:
            try:
                # Force garbage collection of adapter
                del adapter
                import gc
                gc.collect()
                logger.debug(f"Worker {worker_id}: Cleaned up model adapter")
            except Exception as e:
                logger.warning(f"Worker {worker_id}: Failed to cleanup adapter: {e}")
        
        # Clean up GPU memory and release GPU
        if gpu_id is not None:
            try:
                cleanup_gpu_memory(gpu_id)
                logger.debug(f"Worker {worker_id}: Cleaned up GPU memory for GPU {gpu_id}")
            
                # Release GPU back to pool (always use the original GPU ID for the pool)
                release_gpu_to_pool(gpu_pool, gpu_id)
                logger.debug(f"Worker {worker_id}: Released GPU {gpu_id} back to pool")
                
            except Exception as cleanup_error:
                logger.warning(f"Worker {worker_id}: GPU cleanup failed for GPU {gpu_id}: {cleanup_error}")
                # Try to release GPU even if cleanup failed
                try:
                    release_gpu_to_pool(gpu_pool, gpu_id)
                except Exception:
                    logger.error(f"Worker {worker_id}: Failed to release GPU {gpu_id} to pool")
    
    except Exception as e:
        logger.error(f"Worker {worker_id}: Error during resource cleanup: {e}")


class BenchmarkExecutor:
    """Parallel benchmark executor with dynamic GPU allocation and resumability.
    
    Executes benchmark experiments across a model × sparse_attention_config × 
    benchmark × subset matrix with parallel GPU execution and automatic resumability.
    
    Features:
    - Dynamic GPU allocation from specified gpu_ids
    - Subset-level resumability (skip completed experiments)
    - Parallel execution with configurable concurrency
    - Comprehensive error handling and logging
    - Result aggregation and summary generation
    
    Example:
        >>> executor = BenchmarkExecutor(
        ...     gpu_ids=[0, 1, 2, 3],
        ...     max_concurrent_runs=4,
        ...     base_result_dir="./benchmark_results"
        ... )
        >>> 
        >>> results = executor.run_benchmark_matrix(
        ...     model_names=["model1", "model2"],
        ...     sparse_attention_configs=[("dense", None), ("sparse", config)],
        ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
        ...     adapter_config=AdapterConfig()
        ... )
    """
    
    def __init__(
        self,
        gpu_ids: List[int],
        max_concurrent_runs: int,
        base_result_dir: str = "./benchmark_results",
        enable_resumability: bool = True,
        result_file_validation: bool = True,
        required_result_files: Optional[List[str]] = None,
        timeout_per_benchmark: float = 360000.0,  # 100 hours default
        verbose: bool = True,
        max_records: Optional[int] = None,  # Maximum number of metric events to log
        sampling_factor: float = 1.0,  # Probability of logging each metric event (0.0-1.0)
    ):
        """Initialize the BenchmarkExecutor.
        
        Args:
            gpu_ids: List of GPU device IDs to use for parallel execution
            max_concurrent_runs: Maximum number of concurrent benchmark runs
            base_result_dir: Base directory for storing benchmark results
            enable_resumability: Whether to skip already completed experiments
            result_file_validation: Whether to check for specific result files
            required_result_files: List of files required for completion check
            timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
            verbose: Whether to enable verbose logging
            max_records: Maximum number of metric events to log (None for unlimited)
            sampling_factor: Probability of logging each metric event (0.0-1.0)
            
        Raises:
            ValueError: If configuration parameters are invalid
            RuntimeError: If GPU validation fails
        """
        # Validate and store configuration
        self.gpu_ids = self._validate_gpu_ids(gpu_ids)
        self.max_concurrent_runs = self._validate_max_concurrent_runs(max_concurrent_runs)
        self.base_result_dir = Path(base_result_dir).resolve()
        self.enable_resumability = enable_resumability
        self.result_file_validation = result_file_validation
        self.required_result_files = required_result_files or ["raw_results.csv"]
        self.timeout_per_benchmark = timeout_per_benchmark
        self.verbose = verbose
        
        # Metric logging configuration
        self.max_records = max_records
        self.sampling_factor = max(0.0, min(1.0, sampling_factor))  # Clamp to [0.0, 1.0]
        
        # Initialize logging
        self._setup_logging()
        
        # Set up signal handlers for graceful shutdown
        self._setup_signal_handlers()
        
        # Validate GPU availability
        self._validate_gpu_setup()
        
        # Load all available benchmarks
        ensure_benchmarks_loaded()
        

        
        self.logger.info(f"BenchmarkExecutor initialized:")
        self.logger.info(f"  GPU IDs: {self.gpu_ids}")
        self.logger.info(f"  Max concurrent runs: {self.max_concurrent_runs}")
        self.logger.info(f"  Base result directory: {self.base_result_dir}")
        self.logger.info(f"  Resumability: {'enabled' if self.enable_resumability else 'disabled'}")
    
    def _setup_signal_handlers(self) -> None:
        """Set up signal handlers for graceful shutdown."""
        signal.signal(signal.SIGTERM, _signal_handler)
        signal.signal(signal.SIGINT, _signal_handler)
        
        # Register cleanup function to run at exit
        import atexit
        atexit.register(self._cleanup_on_exit)
    
    def _cleanup_on_exit(self) -> None:
        """Cleanup function called when the process exits."""
        if hasattr(self, 'logger'):
            self.logger.info("BenchmarkExecutor cleanup on exit")
        
        # Reset global shutdown flag
        global _shutdown_requested
        _shutdown_requested = False

    def run_benchmark_matrix(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig,
        generation_kwargs: Optional[Dict[str, Any]] = None,
        request_kwargs: Optional[Dict[str, Any]] = None
    ) -> ExecutionResults:
        """Execute benchmark matrix with parallel GPU execution.
        
        Runs all combinations of model_names × sparse_attention_configs × 
        benchmark_configs × subsets in parallel across available GPUs.
        
        Args:
            model_names: List of model names to benchmark
            sparse_attention_configs: List of (name, config) tuples for sparse attention
            benchmark_configs: List of benchmark configurations
            adapter_config: Configuration for model adapter
            generation_kwargs: Optional generation parameters
            request_kwargs: Optional request processing parameters
            
        Returns:
            ExecutionResults containing comprehensive execution information
            
        Example:
            >>> results = executor.run_benchmark_matrix(
            ...     model_names=["meta-llama/Llama-3.1-8B-Instruct"],
            ...     sparse_attention_configs=[("dense", None)],
            ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
            ...     adapter_config=AdapterConfig()
            ... )
            >>> print(f"Completed: {results.completed_count}/{results.total_count}")
        """
        # Initialize execution
        self.logger.info("Starting benchmark matrix execution")
        
        try:
            # Step 1: Validate input parameters
            self._validate_matrix_parameters(
                model_names, sparse_attention_configs, benchmark_configs, adapter_config
            )
            
            # Step 2: Generate benchmark stubs (matrix expansion)
            self.logger.info("Generating benchmark matrix...")
            all_stubs = generate_benchmark_stubs(
                model_names=model_names,
                sparse_attention_configs=sparse_attention_configs,
                benchmark_configs=benchmark_configs,
                adapter_config=adapter_config,
                generation_kwargs=generation_kwargs,
                request_kwargs=request_kwargs,
                base_result_dir=str(self.base_result_dir)
            )
            
            total_combinations = len(all_stubs)
            self.logger.info(f"Generated {total_combinations} benchmark combinations:")
            self.logger.info(f"  {len(model_names)} models × {len(sparse_attention_configs)} sparse configs × "
                           f"{sum(len(b.subsets) if b.subsets else 1 for b in benchmark_configs)} benchmark-subsets")
            
            # Step 3: Filter for resumability (if enabled)
            if self.enable_resumability:
                self.logger.info("Checking for existing results (resumability)...")
                pending_stubs, completed_stubs = filter_existing_results(
                    all_stubs,
                    check_result_files=self.result_file_validation,
                    required_files=self.required_result_files,
                    verbose=self.verbose
                )
                
                self.logger.info(f"Resumability check: {len(completed_stubs)} completed, {len(pending_stubs)} pending")
            else:
                pending_stubs = all_stubs
                completed_stubs = []
                self.logger.info("Resumability disabled - will execute all combinations")
            
            # Step 4: Early exit if nothing to do
            if not pending_stubs:
                self.logger.info("No pending benchmarks to execute. All experiments already completed!")
                
                execution_summary = {
                    "total_combinations": total_combinations,
                    "models": model_names,
                    "sparse_configs": [name for name, _ in sparse_attention_configs],
                    "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs]
                }
                
                progress = ExecutionProgress(
                    total_stubs=total_combinations,
                    skipped_stubs=len(completed_stubs),
                    completed_stubs=0,
                    failed_stubs=0
                )
                
                return ExecutionResults(
                    execution_summary=execution_summary,
                    individual_results=[],
                    failed_executions=[],
                    execution_time=0.0,
                    progress=progress
                )
            

            
            # Step 6: Execute pending benchmarks with worker processes
            self.logger.info(f"Executing {len(pending_stubs)} pending benchmarks with {self.max_concurrent_runs} workers...")
            
            start_time = time.time()
            results = self._execute_with_workers(pending_stubs)
            execution_time = time.time() - start_time
            
            # Step 7: Aggregate results
            execution_summary = {
                "total_combinations": total_combinations,
                "models": model_names,
                "sparse_configs": [name for name, _ in sparse_attention_configs],
                "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs],
                "execution_time_seconds": execution_time,
                "gpu_ids_used": self.gpu_ids,
                "max_concurrent_runs": self.max_concurrent_runs
            }
            
            progress = ExecutionProgress(
                total_stubs=total_combinations,
                skipped_stubs=len(completed_stubs),
                completed_stubs=len(results.individual_results),
                failed_stubs=len(results.failed_executions)
            )
            
            final_results = ExecutionResults(
                execution_summary=execution_summary,
                individual_results=results.individual_results,
                failed_executions=results.failed_executions,
                execution_time=execution_time,
                progress=progress
            )
            
            self.logger.info(f"Benchmark matrix execution completed:")
            self.logger.info(f"  Total time: {execution_time:.2f}s")
            self.logger.info(f"  Successful: {len(results.individual_results)}")
            self.logger.info(f"  Failed: {len(results.failed_executions)}")
            self.logger.info(f"  Skipped: {len(completed_stubs)}")
            
            return final_results
            
        except Exception as e:
            self.logger.error(f"Benchmark execution failed: {e}")
            raise
    
    def _execute_with_workers(self, pending_stubs: List[BenchmarkStub]) -> ExecutionResults:
        """Execute benchmark stubs using worker processes.
        
        This method implements comprehensive process pool management with:
        - Dynamic work distribution through shared queues
        - Graceful process startup and shutdown
        - Real-time result collection and error handling
        - Process health monitoring and recovery
        
        Args:
            pending_stubs: List of benchmark stubs to execute
            
        Returns:
            ExecutionResults containing all results and failures
        """
        self.logger.info(f"Starting execution with {len(pending_stubs)} stubs using {self.max_concurrent_runs} workers")
        
        # Step 1: Create and configure shared queues
        stub_queue = multiprocessing.Queue(maxsize=len(pending_stubs) + 100)  # Buffer for queue operations
        gpu_pool = create_gpu_pool(self.gpu_ids)
        result_queue = multiprocessing.Queue()
        error_queue = multiprocessing.Queue()
        
        # Step 2: Populate work queue with all stubs
        self.logger.info("Populating work queue...")
        for stub in pending_stubs:
            stub_queue.put(stub)
        
        # Add sentinel values to signal workers to stop
        for _ in range(self.max_concurrent_runs):
            stub_queue.put(None)  # Sentinel value
        
        # Step 3: Create and start worker processes
        workers = []
        worker_pids = []
        
        self.logger.info(f"Starting {self.max_concurrent_runs} worker processes...")
        for i in range(self.max_concurrent_runs):
            worker = multiprocessing.Process(
                target=_benchmark_worker,
                args=(stub_queue, gpu_pool, result_queue, error_queue, self.timeout_per_benchmark),
                name=f"benchmark_worker_{i}"
            )
            worker.daemon = True  # Ensure workers are terminated when main process exits
            worker.start()
            workers.append(worker)
            worker_pids.append(worker.pid)
            self.logger.debug(f"Started worker {i+1}/{self.max_concurrent_runs} (PID: {worker.pid})")
        
        self.logger.info(f"All {self.max_concurrent_runs} workers started successfully")
        
        # Step 4: Monitor and collect results in real-time
        individual_results: List[BenchmarkResult] = []
        failed_executions: List[BenchmarkFailure] = []
        active_workers = set(worker_pids)
        
        self.logger.info("Monitoring worker processes and collecting results...")
        
        try:
            # Monitor workers and collect results until all work is complete
            while active_workers:
                # Check for completed workers
                completed_workers = []
                for worker in workers:
                    if worker.is_alive():
                        # Worker is still running, check for results
                        pass
                    else:
                        # Worker has completed
                        if worker.pid in active_workers:
                            active_workers.remove(worker.pid)
                            completed_workers.append(worker)
                            
                            if worker.exitcode != 0:
                                self.logger.warning(f"Worker {worker.pid} exited with code {worker.exitcode}")
                            else:
                                self.logger.debug(f"Worker {worker.pid} completed successfully")
                
                # Collect any available results (non-blocking)
                self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
                
                # Log progress periodically
                if len(individual_results) + len(failed_executions) > 0 and (len(individual_results) + len(failed_executions)) % 5 == 0:
                    self.logger.info(f"Progress: {len(individual_results)} completed, {len(failed_executions)} failed, {len(active_workers)} workers active")
                
                # Small delay to prevent busy waiting
                time.sleep(0.1)
            
            # Step 5: Final result collection
            self.logger.info("All workers completed, collecting final results...")
            self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
            
        except KeyboardInterrupt:
            self.logger.warning("Received interrupt signal, initiating graceful shutdown...")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        except Exception as e:
            error_category = _categorize_error(e, "main execution")
            self.logger.error(f"Error during execution ({error_category}): {e}")
            self.logger.debug(f"Execution error details: {traceback.format_exc()}")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        finally:
            # Step 6: Cleanup resources
            self._cleanup_resources(workers, stub_queue, gpu_pool, result_queue, error_queue)
        
        # Step 7: Final summary
        total_processed = len(individual_results) + len(failed_executions)
        self.logger.info(f"Execution completed: {len(individual_results)} successful, {len(failed_executions)} failed")
        self.logger.info(f"Success rate: {len(individual_results)/total_processed*100:.1f}%" if total_processed > 0 else "No work processed")
        
        return ExecutionResults(
            execution_summary={},  # Will be filled by caller
            individual_results=individual_results,
            failed_executions=failed_executions,
            execution_time=0.0,  # Will be filled by caller
            progress=ExecutionProgress(
                total_stubs=len(pending_stubs),
                completed_stubs=len(individual_results),
                failed_stubs=len(failed_executions)
            )
        )
    
    def _collect_available_results(
        self,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue,
        individual_results: List[BenchmarkResult],
        failed_executions: List[BenchmarkFailure]
    ) -> None:
        """Collect all available results from queues without blocking.
        
        Args:
            result_queue: Queue containing successful benchmark results
            error_queue: Queue containing benchmark failures
            individual_results: List to append successful results to
            failed_executions: List to append failures to
        """
        # Collect all available successful results
        while True:
            try:
                result = result_queue.get_nowait()
                individual_results.append(result)
            except Empty:
                break
        
        # Collect all available errors
        while True:
            try:
                error = error_queue.get_nowait()
                if isinstance(error, BenchmarkFailure):
                    failed_executions.append(error)
                else:
                    # Handle worker-level errors
                    self.logger.error(f"Worker error: {error}")
            except Empty:
                break
    
    def _graceful_shutdown(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Perform graceful shutdown of worker processes and queues.
        
        Args:
            workers: List of worker processes to terminate
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.info("Initiating graceful shutdown...")
        
        try:
            # Signal workers to stop by adding sentinel values
            sentinel_count = 0
            for _ in range(len(workers)):
                try:
                    stub_queue.put_nowait(None)
                    sentinel_count += 1
                except Exception as e:
                    self.logger.debug(f"Could not add sentinel to queue: {e}")
                    break  # Queue is full, stop adding sentinels
            
            self.logger.info(f"Added {sentinel_count} shutdown signals to work queue")
            
            # Wait for workers to finish with timeout
            self.logger.info("Waiting for workers to finish...")
            for i, worker in enumerate(workers):
                try:
                    worker.join(timeout=30.0)  # 30 second timeout per worker
                    if worker.is_alive():
                        self.logger.warning(f"Force terminating worker {worker.pid}")
                        worker.terminate()
                        worker.join(timeout=5.0)
                        if worker.is_alive():
                            self.logger.error(f"Failed to terminate worker {worker.pid}, killing process")
                            worker.kill()
                            worker.join(timeout=2.0)
                    else:
                        self.logger.debug(f"Worker {worker.pid} terminated successfully")
                except Exception as e:
                    self.logger.error(f"Error during worker {worker.pid} shutdown: {e}")
            
            self.logger.info("Graceful shutdown completed")
            
        except Exception as e:
            self.logger.error(f"Error during graceful shutdown: {e}")
            # Force cleanup of remaining workers
            for worker in workers:
                if worker.is_alive():
                    try:
                        worker.kill()
                    except Exception:
                        pass
    
    def _cleanup_resources(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Clean up all multiprocessing resources.
        
        Args:
            workers: List of worker processes
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.debug("Cleaning up multiprocessing resources...")
        
        # Close all queues
        try:
            stub_queue.close()
            gpu_pool.close()
            result_queue.close()
            error_queue.close()
        except Exception as e:
            self.logger.warning(f"Error closing queues: {e}")
        
        # Join any remaining workers
        for worker in workers:
            if worker.is_alive():
                self.logger.warning(f"Force terminating remaining worker {worker.pid}")
                worker.terminate()
                worker.join(timeout=5.0)
                if worker.is_alive():
                    worker.kill()
        
        self.logger.debug("Resource cleanup completed")
    
    def _validate_gpu_ids(self, gpu_ids: List[int]) -> List[int]:
        """Validate and return GPU IDs."""
        if not gpu_ids:
            raise ValueError("gpu_ids cannot be empty")
        
        if not all(isinstance(gpu_id, int) and gpu_id >= 0 for gpu_id in gpu_ids):
            raise ValueError("All GPU IDs must be non-negative integers")
        
        # Remove duplicates while preserving order
        seen = set()
        unique_gpu_ids = []
        for gpu_id in gpu_ids:
            if gpu_id not in seen:
                seen.add(gpu_id)
                unique_gpu_ids.append(gpu_id)
        
        return unique_gpu_ids
    
    def _validate_max_concurrent_runs(self, max_concurrent_runs: int) -> int:
        """Validate and return max_concurrent_runs."""
        if not isinstance(max_concurrent_runs, int) or max_concurrent_runs <= 0:
            raise ValueError("max_concurrent_runs must be a positive integer")
        
        if max_concurrent_runs > len(self.gpu_ids) if hasattr(self, 'gpu_ids') else True:
            # This is just a warning - user might want more concurrent runs than GPUs
            # for benchmarks that don't require much GPU memory
            pass
        
        return max_concurrent_runs
    
    def _setup_logging(self) -> None:
        """Setup logging configuration."""
        self.logger = logging.getLogger(f"{__name__}.BenchmarkExecutor")
        
        if self.verbose and not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def _validate_gpu_setup(self) -> None:
        """Validate GPU availability."""
        try:
            validate_gpu_availability(self.gpu_ids)
            self.logger.info(f"GPU validation passed for devices: {self.gpu_ids}")
        except (RuntimeError, ValueError) as e:
            self.logger.error(f"GPU validation failed: {e}")
            raise
    
    def _validate_matrix_parameters(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig
    ) -> None:
        """Validate benchmark matrix parameters."""
        if not model_names:
            raise ValueError("model_names cannot be empty")
        
        if not sparse_attention_configs:
            raise ValueError("sparse_attention_configs cannot be empty")
        
        if not benchmark_configs:
            raise ValueError("benchmark_configs cannot be empty")
        
        # Validate model names
        for model_name in model_names:
            if not isinstance(model_name, str) or not model_name.strip():
                raise ValueError(f"Invalid model name: {model_name}")
        
        # Validate sparse attention configs
        for sparse_config in sparse_attention_configs:
            if not isinstance(sparse_config, tuple) or len(sparse_config) != 2:
                raise ValueError(f"sparse_attention_configs must be list of (name, config) tuples")
            
            config_name, config_obj = sparse_config
            if not isinstance(config_name, str) or not config_name.strip():
                raise ValueError(f"Sparse config name must be non-empty string: {config_name}")
        
        # Validate benchmark configs
        for benchmark_config in benchmark_configs:
            if not isinstance(benchmark_config, BenchmarkConfig):
                raise ValueError(f"benchmark_configs must contain BenchmarkConfig instances")
            # Additional validation could be added here if needed
        
        # Validate adapter config
        if not isinstance(adapter_config, AdapterConfig):
            raise ValueError("adapter_config must be an AdapterConfig instance")
            # Additional validation could be added here if needed
        
        self.logger.info("Matrix parameter validation passed")

```

```python
"""BenchmarkExecutor: Parallel GPU execution for benchmark matrix experiments.

This module contains the main BenchmarkExecutor class that orchestrates parallel 
execution of benchmark experiments across the model × sparse_attention_config × 
benchmark × subset matrix with dynamic GPU allocation and resumability.
"""

import logging
import multiprocessing
import os
import signal
import sys
import time
import traceback
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from queue import Empty
from contextlib import contextmanager
from sparse_attention_hub.metric_logging.logger import MicroMetricLogger

# Set multiprocessing start method to 'spawn' for CUDA compatibility
if multiprocessing.get_start_method(allow_none=True) != 'spawn':
    multiprocessing.set_start_method('spawn', force=True)

from sparse_attention_hub.sparse_attention.base import SparseAttentionConfig

from .executor_config import (
    AdapterConfig,
    BenchmarkConfig,
    BenchmarkStub,
    ExecutionResults,
    ExecutionProgress,
    BenchmarkResult,
    BenchmarkFailure,
    filter_existing_results,
    generate_benchmark_stubs,

)
from .benchmark_registry import (
    ensure_benchmarks_loaded,
    create_benchmark_instance
)

from .utils.gpu import (
    cleanup_gpu_memory,
    release_gpu_to_pool,
    acquire_gpu_from_pool,
    create_gpu_pool,
    validate_gpu_availability,
)

# Global flag for graceful shutdown
_shutdown_requested = False


@contextmanager
def set_gpu(gpu_id: int):
    """Context manager to set the current CUDA device.
    
    Args:
        gpu_id: The GPU device ID to set as current device
        
    Yields:
        None: The context manager yields control to the with block
    """
    import torch
    torch.cuda.set_device(gpu_id)
    yield


def _signal_handler(signum: int, frame: Any) -> None:
    """Handle shutdown signals gracefully."""
    global _shutdown_requested
    _shutdown_requested = True
    logging.getLogger(__name__).warning(f"Received signal {signum}, initiating graceful shutdown...")


def _categorize_error(error: Exception, context: str = "") -> str:
    """Categorize errors for better handling and reporting.
    
    Args:
        error: The exception that occurred
        context: Additional context about where the error occurred
        
    Returns:
        Error category string for classification
    """
    error_type = type(error).__name__
    error_msg = str(error).lower()
    
    # GPU-related errors
    if any(keyword in error_msg for keyword in ["cuda", "gpu", "out of memory", "oom"]):
        if "out of memory" in error_msg or "oom" in error_msg:
            return "GPU_OUT_OF_MEMORY"
        elif "cuda" in error_msg:
            return "GPU_CUDA_ERROR"
        else:
            return "GPU_ERROR"
    
    # Model loading errors
    elif any(keyword in error_msg for keyword in ["model", "huggingface", "transformers", "pretrained"]):
        if "not found" in error_msg or "doesn't exist" in error_msg:
            return "MODEL_NOT_FOUND"
        elif "tokenizer" in error_msg:
            return "TOKENIZER_ERROR"
        else:
            return "MODEL_LOADING_ERROR"
    
    # Dataset/benchmark errors
    elif any(keyword in error_msg for keyword in ["dataset", "benchmark", "subset"]):
        return "DATASET_ERROR"
    
    # Network/connection errors
    elif any(keyword in error_msg for keyword in ["connection", "timeout", "network", "http"]):
        return "NETWORK_ERROR"
    
    # File system errors
    elif any(keyword in error_msg for keyword in ["file", "directory", "permission", "disk"]):
        return "FILESYSTEM_ERROR"
    
    # Multiprocessing errors
    elif any(keyword in error_msg for keyword in ["queue", "process", "multiprocessing"]):
        return "MULTIPROCESSING_ERROR"
    
    # Default categories
    elif isinstance(error, TimeoutError):
        return "TIMEOUT_ERROR"
    elif isinstance(error, KeyboardInterrupt):
        return "INTERRUPT_ERROR"
    elif isinstance(error, MemoryError):
        return "SYSTEM_MEMORY_ERROR"
    else:
        return "UNKNOWN_ERROR"


def _benchmark_worker(
    stub_queue: multiprocessing.Queue,
    gpu_pool: multiprocessing.Queue,
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue,
    timeout_per_benchmark: float = 3600.0,
    max_records: Optional[int] = None,
    sampling_factor: float = 1.0
) -> None:
    """Worker function to execute benchmarks with dynamic GPU allocation.
    
    This function runs in a separate process and handles:
    1. Acquiring GPUs from the shared pool
    2. Creating model adapters with sparse attention configurations
    3. Executing benchmarks with proper error handling
    4. Releasing GPUs back to the pool
    5. Reporting results and errors
    
    Args:
        stub_queue: Queue containing BenchmarkStub instances to process
        gpu_pool: Shared queue containing available GPU IDs
        result_queue: Queue for reporting successful benchmark results
        error_queue: Queue for reporting benchmark failures
        timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
        max_records: Maximum number of metric events to log (None for unlimited)
        sampling_factor: Probability of logging each metric event (0.0-1.0)
    """
    worker_id = os.getpid()
    logger = logging.getLogger(f"{__name__}.worker_{worker_id}")
    
    # Set up signal handlers for graceful shutdown
    signal.signal(signal.SIGTERM, _signal_handler)
    signal.signal(signal.SIGINT, _signal_handler)
    
    # Track resources for cleanup
    current_gpu_id: Optional[int] = None
    adapter = None

    try:
        while True:
            # Check for shutdown signal
            if _shutdown_requested:
                logger.info(f"Worker {worker_id}: Shutdown requested, exiting")
                break
            
            # Step 1: Get next benchmark stub from queue
            try:
                stub: BenchmarkStub = stub_queue.get(timeout=1.0)  # 1 second timeout
            except Empty:
                # No more work to do
                logger.info(f"Worker {worker_id}: No more work, shutting down")
                break
            
            # Check for sentinel value (shutdown signal)
            if stub is None:
                logger.info(f"Worker {worker_id}: Received shutdown signal, exiting")
                break
            
            # Step 2: Acquire GPU from pool
            try:
                current_gpu_id = acquire_gpu_from_pool(gpu_pool, timeout=30.0)
                logger.info(f"Worker {worker_id}: Acquired GPU {current_gpu_id} for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}")
            except Exception as e:
                error_category = _categorize_error(e, "GPU acquisition")
                error_msg = f"Failed to acquire GPU for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                error_queue.put(BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=0.0
                ))
                continue

            # Step 3: Run the benchmark on the current GPU
            start_time = time.time()
            execution_time = 0.0  # Initialize execution_time
            
            try:
                with set_gpu(current_gpu_id):
                    logger.debug(f"Worker {worker_id}: Set CUDA device to {current_gpu_id}")
                    execution_success = False

                    # Create model adapter with sparse attention config
                    logger.info(f"Worker {worker_id}: Creating model adapter for {stub.model_name}")
                    
                    # Import here to avoid issues with multiprocessing
                    from sparse_attention_hub.adapters.huggingface import ModelAdapterHF
                    
                    adapter = ModelAdapterHF(
                        model_name=stub.model_name,
                        sparse_attention_config=stub.sparse_attention_config,
                        model_kwargs=stub.adapter_config.model_kwargs,
                        tokenizer_kwargs=stub.adapter_config.tokenizer_kwargs
                    )
                    
                    # Create benchmark instance
                    logger.info(f"Worker {worker_id}: Creating benchmark instance for {stub.benchmark_name}")
                    benchmark = create_benchmark_instance(
                        benchmark_name=stub.benchmark_name,
                        subsets=[stub.subset] if stub.subset else None
                    )
                    
                    # Execute benchmark
                    logger.info(f"Worker {worker_id}: Executing benchmark {stub.benchmark_name} on GPU {current_gpu_id}")
                    metric_logger = MicroMetricLogger()
                    metric_logger.configure_logging(log_path=stub.result_dir, 
                                    enabled_metrics=["research_attention_density", "research_attention_output_error"],
                                    max_records=10000,
                                    sampling_factor=0.2
                                )
                    metrics = benchmark.run_benchmark(
                        adapter=adapter,
                        result_dir=stub.result_dir,
                        generation_kwargs=stub.generation_kwargs,
                        request_kwargs=stub.request_kwargs
                    )
                    metric_logger.flush()
                    
                    execution_time = time.time() - start_time
                    execution_success = True
                    
                    # Report successful result
                    result = BenchmarkResult(
                        stub=stub,
                        metrics=metrics,
                        execution_time=execution_time,
                        success=True
                    )
                    result_queue.put(result)
                    
                    logger.info(f"Worker {worker_id}: Successfully completed {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name} in {execution_time:.2f}s")

            except Exception as e:
                execution_time = time.time() - start_time  # Calculate execution time even on failure
                error_category = _categorize_error(e, "benchmark execution")
                error_msg = f"Benchmark execution failed: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                
                # Log detailed error information for debugging
                logger.debug(f"Worker {worker_id}: Error details: {traceback.format_exc()}")
                
                # Report failure
                failure = BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=execution_time
                )
                error_queue.put(failure)
                continue
            
            finally:
                # Step 4: Cleanup and release GPU
                _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
                current_gpu_id = None
                adapter = None
    
    except Exception as worker_error:
        logger.error(f"Worker {worker_id}: Critical worker error: {worker_error}")
        logger.debug(f"Worker {worker_id}: Critical error details: {traceback.format_exc()}")
        
        # Cleanup any remaining resources
        _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
        
        # Report worker failure to error queue
        error_queue.put({
            "worker_id": worker_id,
            "error_type": "WORKER_CRITICAL_ERROR",
            "error_message": str(worker_error),
            "error_traceback": traceback.format_exc()
        })


def _cleanup_worker_resources(
    gpu_pool: multiprocessing.Queue,
    gpu_id: Optional[int],
    adapter: Any,
    logger: logging.Logger,
    worker_id: int
) -> None:
    """Clean up worker resources including GPU memory and adapter.
    
    Args:
        gpu_pool: GPU pool queue to return GPU to
        gpu_id: GPU ID to release (can be None) - this is the actual GPU ID from the pool
        adapter: Model adapter to cleanup (can be None)
        logger: Logger instance for the worker
        worker_id: Worker process ID for logging
    """
    try:
        # Clean up adapter if it exists
        if adapter is not None:
            try:
                # Force garbage collection of adapter
                del adapter
                import gc
                gc.collect()
                logger.debug(f"Worker {worker_id}: Cleaned up model adapter")
            except Exception as e:
                logger.warning(f"Worker {worker_id}: Failed to cleanup adapter: {e}")
        
        # Clean up GPU memory and release GPU
        if gpu_id is not None:
            try:
                cleanup_gpu_memory(gpu_id)
                logger.debug(f"Worker {worker_id}: Cleaned up GPU memory for GPU {gpu_id}")
            
                # Release GPU back to pool (always use the original GPU ID for the pool)
                release_gpu_to_pool(gpu_pool, gpu_id)
                logger.debug(f"Worker {worker_id}: Released GPU {gpu_id} back to pool")
                
            except Exception as cleanup_error:
                logger.warning(f"Worker {worker_id}: GPU cleanup failed for GPU {gpu_id}: {cleanup_error}")
                # Try to release GPU even if cleanup failed
                try:
                    release_gpu_to_pool(gpu_pool, gpu_id)
                except Exception:
                    logger.error(f"Worker {worker_id}: Failed to release GPU {gpu_id} to pool")
    
    except Exception as e:
        logger.error(f"Worker {worker_id}: Error during resource cleanup: {e}")


class BenchmarkExecutor:
    """Parallel benchmark executor with dynamic GPU allocation and resumability.
    
    Executes benchmark experiments across a model × sparse_attention_config × 
    benchmark × subset matrix with parallel GPU execution and automatic resumability.
    
    Features:
    - Dynamic GPU allocation from specified gpu_ids
    - Subset-level resumability (skip completed experiments)
    - Parallel execution with configurable concurrency
    - Comprehensive error handling and logging
    - Result aggregation and summary generation
    
    Example:
        >>> executor = BenchmarkExecutor(
        ...     gpu_ids=[0, 1, 2, 3],
        ...     max_concurrent_runs=4,
        ...     base_result_dir="./benchmark_results"
        ... )
        >>> 
        >>> results = executor.run_benchmark_matrix(
        ...     model_names=["model1", "model2"],
        ...     sparse_attention_configs=[("dense", None), ("sparse", config)],
        ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
        ...     adapter_config=AdapterConfig()
        ... )
    """
    
    def __init__(
        self,
        gpu_ids: List[int],
        max_concurrent_runs: int,
        base_result_dir: str = "./benchmark_results",
        enable_resumability: bool = True,
        result_file_validation: bool = True,
        required_result_files: Optional[List[str]] = None,
        timeout_per_benchmark: float = 360000.0,  # 100 hours default
        verbose: bool = True,
        max_records: Optional[int] = None,  # Maximum number of metric events to log
        sampling_factor: float = 1.0,  # Probability of logging each metric event (0.0-1.0)
    ):
        """Initialize the BenchmarkExecutor.
        
        Args:
            gpu_ids: List of GPU device IDs to use for parallel execution
            max_concurrent_runs: Maximum number of concurrent benchmark runs
            base_result_dir: Base directory for storing benchmark results
            enable_resumability: Whether to skip already completed experiments
            result_file_validation: Whether to check for specific result files
            required_result_files: List of files required for completion check
            timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
            verbose: Whether to enable verbose logging
            max_records: Maximum number of metric events to log (None for unlimited)
            sampling_factor: Probability of logging each metric event (0.0-1.0)
            
        Raises:
            ValueError: If configuration parameters are invalid
            RuntimeError: If GPU validation fails
        """
        # Validate and store configuration
        self.gpu_ids = self._validate_gpu_ids(gpu_ids)
        self.max_concurrent_runs = self._validate_max_concurrent_runs(max_concurrent_runs)
        self.base_result_dir = Path(base_result_dir).resolve()
        self.enable_resumability = enable_resumability
        self.result_file_validation = result_file_validation
        self.required_result_files = required_result_files or ["raw_results.csv"]
        self.timeout_per_benchmark = timeout_per_benchmark
        self.verbose = verbose
        
        # Metric logging configuration
        self.max_records = max_records
        self.sampling_factor = max(0.0, min(1.0, sampling_factor))  # Clamp to [0.0, 1.0]
        
        # Initialize logging
        self._setup_logging()
        
        # Set up signal handlers for graceful shutdown
        self._setup_signal_handlers()
        
        # Validate GPU availability
        self._validate_gpu_setup()
        
        # Load all available benchmarks
        ensure_benchmarks_loaded()
        

        
        self.logger.info(f"BenchmarkExecutor initialized:")
        self.logger.info(f"  GPU IDs: {self.gpu_ids}")
        self.logger.info(f"  Max concurrent runs: {self.max_concurrent_runs}")
        self.logger.info(f"  Base result directory: {self.base_result_dir}")
        self.logger.info(f"  Resumability: {'enabled' if self.enable_resumability else 'disabled'}")
    
    def _setup_signal_handlers(self) -> None:
        """Set up signal handlers for graceful shutdown."""
        signal.signal(signal.SIGTERM, _signal_handler)
        signal.signal(signal.SIGINT, _signal_handler)
        
        # Register cleanup function to run at exit
        import atexit
        atexit.register(self._cleanup_on_exit)
    
    def _cleanup_on_exit(self) -> None:
        """Cleanup function called when the process exits."""
        if hasattr(self, 'logger'):
            self.logger.info("BenchmarkExecutor cleanup on exit")
        
        # Reset global shutdown flag
        global _shutdown_requested
        _shutdown_requested = False

    def run_benchmark_matrix(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig,
        generation_kwargs: Optional[Dict[str, Any]] = None,
        request_kwargs: Optional[Dict[str, Any]] = None
    ) -> ExecutionResults:
        """Execute benchmark matrix with parallel GPU execution.
        
        Runs all combinations of model_names × sparse_attention_configs × 
        benchmark_configs × subsets in parallel across available GPUs.
        
        Args:
            model_names: List of model names to benchmark
            sparse_attention_configs: List of (name, config) tuples for sparse attention
            benchmark_configs: List of benchmark configurations
            adapter_config: Configuration for model adapter
            generation_kwargs: Optional generation parameters
            request_kwargs: Optional request processing parameters
            
        Returns:
            ExecutionResults containing comprehensive execution information
            
        Example:
            >>> results = executor.run_benchmark_matrix(
            ...     model_names=["meta-llama/Llama-3.1-8B-Instruct"],
            ...     sparse_attention_configs=[("dense", None)],
            ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
            ...     adapter_config=AdapterConfig()
            ... )
            >>> print(f"Completed: {results.completed_count}/{results.total_count}")
        """
        # Initialize execution
        self.logger.info("Starting benchmark matrix execution")
        
        try:
            # Step 1: Validate input parameters
            self._validate_matrix_parameters(
                model_names, sparse_attention_configs, benchmark_configs, adapter_config
            )
            
            # Step 2: Generate benchmark stubs (matrix expansion)
            self.logger.info("Generating benchmark matrix...")
            all_stubs = generate_benchmark_stubs(
                model_names=model_names,
                sparse_attention_configs=sparse_attention_configs,
                benchmark_configs=benchmark_configs,
                adapter_config=adapter_config,
                generation_kwargs=generation_kwargs,
                request_kwargs=request_kwargs,
                base_result_dir=str(self.base_result_dir)
            )
            
            total_combinations = len(all_stubs)
            self.logger.info(f"Generated {total_combinations} benchmark combinations:")
            self.logger.info(f"  {len(model_names)} models × {len(sparse_attention_configs)} sparse configs × "
                           f"{sum(len(b.subsets) if b.subsets else 1 for b in benchmark_configs)} benchmark-subsets")
            
            # Step 3: Filter for resumability (if enabled)
            if self.enable_resumability:
                self.logger.info("Checking for existing results (resumability)...")
                pending_stubs, completed_stubs = filter_existing_results(
                    all_stubs,
                    check_result_files=self.result_file_validation,
                    required_files=self.required_result_files,
                    verbose=self.verbose
                )
                
                self.logger.info(f"Resumability check: {len(completed_stubs)} completed, {len(pending_stubs)} pending")
            else:
                pending_stubs = all_stubs
                completed_stubs = []
                self.logger.info("Resumability disabled - will execute all combinations")
            
            # Step 4: Early exit if nothing to do
            if not pending_stubs:
                self.logger.info("No pending benchmarks to execute. All experiments already completed!")
                
                execution_summary = {
                    "total_combinations": total_combinations,
                    "models": model_names,
                    "sparse_configs": [name for name, _ in sparse_attention_configs],
                    "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs]
                }
                
                progress = ExecutionProgress(
                    total_stubs=total_combinations,
                    skipped_stubs=len(completed_stubs),
                    completed_stubs=0,
                    failed_stubs=0
                )
                
                return ExecutionResults(
                    execution_summary=execution_summary,
                    individual_results=[],
                    failed_executions=[],
                    execution_time=0.0,
                    progress=progress
                )
            

            
            # Step 6: Execute pending benchmarks with worker processes
            self.logger.info(f"Executing {len(pending_stubs)} pending benchmarks with {self.max_concurrent_runs} workers...")
            
            start_time = time.time()
            results = self._execute_with_workers(pending_stubs)
            execution_time = time.time() - start_time
            
            # Step 7: Aggregate results
            execution_summary = {
                "total_combinations": total_combinations,
                "models": model_names,
                "sparse_configs": [name for name, _ in sparse_attention_configs],
                "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs],
                "execution_time_seconds": execution_time,
                "gpu_ids_used": self.gpu_ids,
                "max_concurrent_runs": self.max_concurrent_runs
            }
            
            progress = ExecutionProgress(
                total_stubs=total_combinations,
                skipped_stubs=len(completed_stubs),
                completed_stubs=len(results.individual_results),
                failed_stubs=len(results.failed_executions)
            )
            
            final_results = ExecutionResults(
                execution_summary=execution_summary,
                individual_results=results.individual_results,
                failed_executions=results.failed_executions,
                execution_time=execution_time,
                progress=progress
            )
            
            self.logger.info(f"Benchmark matrix execution completed:")
            self.logger.info(f"  Total time: {execution_time:.2f}s")
            self.logger.info(f"  Successful: {len(results.individual_results)}")
            self.logger.info(f"  Failed: {len(results.failed_executions)}")
            self.logger.info(f"  Skipped: {len(completed_stubs)}")
            
            return final_results
            
        except Exception as e:
            self.logger.error(f"Benchmark execution failed: {e}")
            raise
    
    def _execute_with_workers(self, pending_stubs: List[BenchmarkStub]) -> ExecutionResults:
        """Execute benchmark stubs using worker processes.
        
        This method implements comprehensive process pool management with:
        - Dynamic work distribution through shared queues
        - Graceful process startup and shutdown
        - Real-time result collection and error handling
        - Process health monitoring and recovery
        
        Args:
            pending_stubs: List of benchmark stubs to execute
            
        Returns:
            ExecutionResults containing all results and failures
        """
        self.logger.info(f"Starting execution with {len(pending_stubs)} stubs using {self.max_concurrent_runs} workers")
        
        # Step 1: Create and configure shared queues
        stub_queue = multiprocessing.Queue(maxsize=len(pending_stubs) + 100)  # Buffer for queue operations
        gpu_pool = create_gpu_pool(self.gpu_ids)
        result_queue = multiprocessing.Queue()
        error_queue = multiprocessing.Queue()
        
        # Step 2: Populate work queue with all stubs
        self.logger.info("Populating work queue...")
        for stub in pending_stubs:
            stub_queue.put(stub)
        
        # Add sentinel values to signal workers to stop
        for _ in range(self.max_concurrent_runs):
            stub_queue.put(None)  # Sentinel value
        
        # Step 3: Create and start worker processes
        workers = []
        worker_pids = []
        
        self.logger.info(f"Starting {self.max_concurrent_runs} worker processes...")
        for i in range(self.max_concurrent_runs):
            worker = multiprocessing.Process(
                target=_benchmark_worker,
                args=(stub_queue, gpu_pool, result_queue, error_queue, self.timeout_per_benchmark),
                name=f"benchmark_worker_{i}"
            )
            worker.daemon = True  # Ensure workers are terminated when main process exits
            worker.start()
            workers.append(worker)
            worker_pids.append(worker.pid)
            self.logger.debug(f"Started worker {i+1}/{self.max_concurrent_runs} (PID: {worker.pid})")
        
        self.logger.info(f"All {self.max_concurrent_runs} workers started successfully")
        
        # Step 4: Monitor and collect results in real-time
        individual_results: List[BenchmarkResult] = []
        failed_executions: List[BenchmarkFailure] = []
        active_workers = set(worker_pids)
        
        self.logger.info("Monitoring worker processes and collecting results...")
        
        try:
            # Monitor workers and collect results until all work is complete
            while active_workers:
                # Check for completed workers
                completed_workers = []
                for worker in workers:
                    if worker.is_alive():
                        # Worker is still running, check for results
                        pass
                    else:
                        # Worker has completed
                        if worker.pid in active_workers:
                            active_workers.remove(worker.pid)
                            completed_workers.append(worker)
                            
                            if worker.exitcode != 0:
                                self.logger.warning(f"Worker {worker.pid} exited with code {worker.exitcode}")
                            else:
                                self.logger.debug(f"Worker {worker.pid} completed successfully")
                
                # Collect any available results (non-blocking)
                self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
                
                # Log progress periodically
                if len(individual_results) + len(failed_executions) > 0 and (len(individual_results) + len(failed_executions)) % 5 == 0:
                    self.logger.info(f"Progress: {len(individual_results)} completed, {len(failed_executions)} failed, {len(active_workers)} workers active")
                
                # Small delay to prevent busy waiting
                time.sleep(0.1)
            
            # Step 5: Final result collection
            self.logger.info("All workers completed, collecting final results...")
            self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
            
        except KeyboardInterrupt:
            self.logger.warning("Received interrupt signal, initiating graceful shutdown...")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        except Exception as e:
            error_category = _categorize_error(e, "main execution")
            self.logger.error(f"Error during execution ({error_category}): {e}")
            self.logger.debug(f"Execution error details: {traceback.format_exc()}")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        finally:
            # Step 6: Cleanup resources
            self._cleanup_resources(workers, stub_queue, gpu_pool, result_queue, error_queue)
        
        # Step 7: Final summary
        total_processed = len(individual_results) + len(failed_executions)
        self.logger.info(f"Execution completed: {len(individual_results)} successful, {len(failed_executions)} failed")
        self.logger.info(f"Success rate: {len(individual_results)/total_processed*100:.1f}%" if total_processed > 0 else "No work processed")
        
        return ExecutionResults(
            execution_summary={},  # Will be filled by caller
            individual_results=individual_results,
            failed_executions=failed_executions,
            execution_time=0.0,  # Will be filled by caller
            progress=ExecutionProgress(
                total_stubs=len(pending_stubs),
                completed_stubs=len(individual_results),
                failed_stubs=len(failed_executions)
            )
        )
    
    def _collect_available_results(
        self,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue,
        individual_results: List[BenchmarkResult],
        failed_executions: List[BenchmarkFailure]
    ) -> None:
        """Collect all available results from queues without blocking.
        
        Args:
            result_queue: Queue containing successful benchmark results
            error_queue: Queue containing benchmark failures
            individual_results: List to append successful results to
            failed_executions: List to append failures to
        """
        # Collect all available successful results
        while True:
            try:
                result = result_queue.get_nowait()
                individual_results.append(result)
            except Empty:
                break
        
        # Collect all available errors
        while True:
            try:
                error = error_queue.get_nowait()
                if isinstance(error, BenchmarkFailure):
                    failed_executions.append(error)
                else:
                    # Handle worker-level errors
                    self.logger.error(f"Worker error: {error}")
            except Empty:
                break
    
    def _graceful_shutdown(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Perform graceful shutdown of worker processes and queues.
        
        Args:
            workers: List of worker processes to terminate
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.info("Initiating graceful shutdown...")
        
        try:
            # Signal workers to stop by adding sentinel values
            sentinel_count = 0
            for _ in range(len(workers)):
                try:
                    stub_queue.put_nowait(None)
                    sentinel_count += 1
                except Exception as e:
                    self.logger.debug(f"Could not add sentinel to queue: {e}")
                    break  # Queue is full, stop adding sentinels
            
            self.logger.info(f"Added {sentinel_count} shutdown signals to work queue")
            
            # Wait for workers to finish with timeout
            self.logger.info("Waiting for workers to finish...")
            for i, worker in enumerate(workers):
                try:
                    worker.join(timeout=30.0)  # 30 second timeout per worker
                    if worker.is_alive():
                        self.logger.warning(f"Force terminating worker {worker.pid}")
                        worker.terminate()
                        worker.join(timeout=5.0)
                        if worker.is_alive():
                            self.logger.error(f"Failed to terminate worker {worker.pid}, killing process")
                            worker.kill()
                            worker.join(timeout=2.0)
                    else:
                        self.logger.debug(f"Worker {worker.pid} terminated successfully")
                except Exception as e:
                    self.logger.error(f"Error during worker {worker.pid} shutdown: {e}")
            
            self.logger.info("Graceful shutdown completed")
            
        except Exception as e:
            self.logger.error(f"Error during graceful shutdown: {e}")
            # Force cleanup of remaining workers
            for worker in workers:
                if worker.is_alive():
                    try:
                        worker.kill()
                    except Exception:
                        pass
    
    def _cleanup_resources(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Clean up all multiprocessing resources.
        
        Args:
            workers: List of worker processes
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.debug("Cleaning up multiprocessing resources...")
        
        # Close all queues
        try:
            stub_queue.close()
            gpu_pool.close()
            result_queue.close()
            error_queue.close()
        except Exception as e:
            self.logger.warning(f"Error closing queues: {e}")
        
        # Join any remaining workers
        for worker in workers:
            if worker.is_alive():
                self.logger.warning(f"Force terminating remaining worker {worker.pid}")
                worker.terminate()
                worker.join(timeout=5.0)
                if worker.is_alive():
                    worker.kill()
        
        self.logger.debug("Resource cleanup completed")
    
    def _validate_gpu_ids(self, gpu_ids: List[int]) -> List[int]:
        """Validate and return GPU IDs."""
        if not gpu_ids:
            raise ValueError("gpu_ids cannot be empty")
        
        if not all(isinstance(gpu_id, int) and gpu_id >= 0 for gpu_id in gpu_ids):
            raise ValueError("All GPU IDs must be non-negative integers")
        
        # Remove duplicates while preserving order
        seen = set()
        unique_gpu_ids = []
        for gpu_id in gpu_ids:
            if gpu_id not in seen:
                seen.add(gpu_id)
                unique_gpu_ids.append(gpu_id)
        
        return unique_gpu_ids
    
    def _validate_max_concurrent_runs(self, max_concurrent_runs: int) -> int:
        """Validate and return max_concurrent_runs."""
        if not isinstance(max_concurrent_runs, int) or max_concurrent_runs <= 0:
            raise ValueError("max_concurrent_runs must be a positive integer")
        
        if max_concurrent_runs > len(self.gpu_ids) if hasattr(self, 'gpu_ids') else True:
            # This is just a warning - user might want more concurrent runs than GPUs
            # for benchmarks that don't require much GPU memory
            pass
        
        return max_concurrent_runs
    
    def _setup_logging(self) -> None:
        """Setup logging configuration."""
        self.logger = logging.getLogger(f"{__name__}.BenchmarkExecutor")
        
        if self.verbose and not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def _validate_gpu_setup(self) -> None:
        """Validate GPU availability."""
        try:
            validate_gpu_availability(self.gpu_ids)
            self.logger.info(f"GPU validation passed for devices: {self.gpu_ids}")
        except (RuntimeError, ValueError) as e:
            self.logger.error(f"GPU validation failed: {e}")
            raise
    
    def _validate_matrix_parameters(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig
    ) -> None:
        """Validate benchmark matrix parameters."""
        if not model_names:
            raise ValueError("model_names cannot be empty")
        
        if not sparse_attention_configs:
            raise ValueError("sparse_attention_configs cannot be empty")
        
        if not benchmark_configs:
            raise ValueError("benchmark_configs cannot be empty")
        
        # Validate model names
        for model_name in model_names:
            if not isinstance(model_name, str) or not model_name.strip():
                raise ValueError(f"Invalid model name: {model_name}")
        
        # Validate sparse attention configs
        for sparse_config in sparse_attention_configs:
            if not isinstance(sparse_config, tuple) or len(sparse_config) != 2:
                raise ValueError(f"sparse_attention_configs must be list of (name, config) tuples")
            
            config_name, config_obj = sparse_config
            if not isinstance(config_name, str) or not config_name.strip():
                raise ValueError(f"Sparse config name must be non-empty string: {config_name}")
        
        # Validate benchmark configs
        for benchmark_config in benchmark_configs:
            if not isinstance(benchmark_config, BenchmarkConfig):
                raise ValueError(f"benchmark_configs must contain BenchmarkConfig instances")
            # Additional validation could be added here if needed
        
        # Validate adapter config
        if not isinstance(adapter_config, AdapterConfig):
            raise ValueError("adapter_config must be an AdapterConfig instance")
            # Additional validation could be added here if needed
        
        self.logger.info("Matrix parameter validation passed")

```

```python
"""BenchmarkExecutor: Parallel GPU execution for benchmark matrix experiments.

This module contains the main BenchmarkExecutor class that orchestrates parallel 
execution of benchmark experiments across the model × sparse_attention_config × 
benchmark × subset matrix with dynamic GPU allocation and resumability.
"""

import logging
import multiprocessing
import os
import signal
import sys
import time
import traceback
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from queue import Empty
from contextlib import contextmanager
from sparse_attention_hub.metric_logging.logger import MicroMetricLogger

# Set multiprocessing start method to 'spawn' for CUDA compatibility
if multiprocessing.get_start_method(allow_none=True) != 'spawn':
    multiprocessing.set_start_method('spawn', force=True)

from sparse_attention_hub.sparse_attention.base import SparseAttentionConfig

from .executor_config import (
    AdapterConfig,
    BenchmarkConfig,
    BenchmarkStub,
    ExecutionResults,
    ExecutionProgress,
    BenchmarkResult,
    BenchmarkFailure,
    filter_existing_results,
    generate_benchmark_stubs,

)
from .benchmark_registry import (
    ensure_benchmarks_loaded,
    create_benchmark_instance
)

from .utils.gpu import (
    cleanup_gpu_memory,
    release_gpu_to_pool,
    acquire_gpu_from_pool,
    create_gpu_pool,
    validate_gpu_availability,
)

# Global flag for graceful shutdown
_shutdown_requested = False


@contextmanager
def set_gpu(gpu_id: int):
    """Context manager to set the current CUDA device.
    
    Args:
        gpu_id: The GPU device ID to set as current device
        
    Yields:
        None: The context manager yields control to the with block
    """
    import torch
    torch.cuda.set_device(gpu_id)
    yield


def _signal_handler(signum: int, frame: Any) -> None:
    """Handle shutdown signals gracefully."""
    global _shutdown_requested
    _shutdown_requested = True
    logging.getLogger(__name__).warning(f"Received signal {signum}, initiating graceful shutdown...")


def _categorize_error(error: Exception, context: str = "") -> str:
    """Categorize errors for better handling and reporting.
    
    Args:
        error: The exception that occurred
        context: Additional context about where the error occurred
        
    Returns:
        Error category string for classification
    """
    error_type = type(error).__name__
    error_msg = str(error).lower()
    
    # GPU-related errors
    if any(keyword in error_msg for keyword in ["cuda", "gpu", "out of memory", "oom"]):
        if "out of memory" in error_msg or "oom" in error_msg:
            return "GPU_OUT_OF_MEMORY"
        elif "cuda" in error_msg:
            return "GPU_CUDA_ERROR"
        else:
            return "GPU_ERROR"
    
    # Model loading errors
    elif any(keyword in error_msg for keyword in ["model", "huggingface", "transformers", "pretrained"]):
        if "not found" in error_msg or "doesn't exist" in error_msg:
            return "MODEL_NOT_FOUND"
        elif "tokenizer" in error_msg:
            return "TOKENIZER_ERROR"
        else:
            return "MODEL_LOADING_ERROR"
    
    # Dataset/benchmark errors
    elif any(keyword in error_msg for keyword in ["dataset", "benchmark", "subset"]):
        return "DATASET_ERROR"
    
    # Network/connection errors
    elif any(keyword in error_msg for keyword in ["connection", "timeout", "network", "http"]):
        return "NETWORK_ERROR"
    
    # File system errors
    elif any(keyword in error_msg for keyword in ["file", "directory", "permission", "disk"]):
        return "FILESYSTEM_ERROR"
    
    # Multiprocessing errors
    elif any(keyword in error_msg for keyword in ["queue", "process", "multiprocessing"]):
        return "MULTIPROCESSING_ERROR"
    
    # Default categories
    elif isinstance(error, TimeoutError):
        return "TIMEOUT_ERROR"
    elif isinstance(error, KeyboardInterrupt):
        return "INTERRUPT_ERROR"
    elif isinstance(error, MemoryError):
        return "SYSTEM_MEMORY_ERROR"
    else:
        return "UNKNOWN_ERROR"


def _benchmark_worker(
    stub_queue: multiprocessing.Queue,
    gpu_pool: multiprocessing.Queue,
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue,
    timeout_per_benchmark: float = 3600.0,
    max_records: Optional[int] = None,
    sampling_factor: float = 1.0
) -> None:
    """Worker function to execute benchmarks with dynamic GPU allocation.
    
    This function runs in a separate process and handles:
    1. Acquiring GPUs from the shared pool
    2. Creating model adapters with sparse attention configurations
    3. Executing benchmarks with proper error handling
    4. Releasing GPUs back to the pool
    5. Reporting results and errors
    
    Args:
        stub_queue: Queue containing BenchmarkStub instances to process
        gpu_pool: Shared queue containing available GPU IDs
        result_queue: Queue for reporting successful benchmark results
        error_queue: Queue for reporting benchmark failures
        timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
        max_records: Maximum number of metric events to log (None for unlimited)
        sampling_factor: Probability of logging each metric event (0.0-1.0)
    """
    worker_id = os.getpid()
    logger = logging.getLogger(f"{__name__}.worker_{worker_id}")
    
    # Set up signal handlers for graceful shutdown
    signal.signal(signal.SIGTERM, _signal_handler)
    signal.signal(signal.SIGINT, _signal_handler)
    
    # Track resources for cleanup
    current_gpu_id: Optional[int] = None
    adapter = None

    try:
        while True:
            # Check for shutdown signal
            if _shutdown_requested:
                logger.info(f"Worker {worker_id}: Shutdown requested, exiting")
                break
            
            # Step 1: Get next benchmark stub from queue
            try:
                stub: BenchmarkStub = stub_queue.get(timeout=1.0)  # 1 second timeout
            except Empty:
                # No more work to do
                logger.info(f"Worker {worker_id}: No more work, shutting down")
                break
            
            # Check for sentinel value (shutdown signal)
            if stub is None:
                logger.info(f"Worker {worker_id}: Received shutdown signal, exiting")
                break
            
            # Step 2: Acquire GPU from pool
            try:
                current_gpu_id = acquire_gpu_from_pool(gpu_pool, timeout=30.0)
                logger.info(f"Worker {worker_id}: Acquired GPU {current_gpu_id} for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}")
            except Exception as e:
                error_category = _categorize_error(e, "GPU acquisition")
                error_msg = f"Failed to acquire GPU for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                error_queue.put(BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=0.0
                ))
                continue

            # Step 3: Run the benchmark on the current GPU
            start_time = time.time()
            execution_time = 0.0  # Initialize execution_time
            
            try:
                with set_gpu(current_gpu_id):
                    logger.debug(f"Worker {worker_id}: Set CUDA device to {current_gpu_id}")
                    execution_success = False

                    # Create model adapter with sparse attention config
                    logger.info(f"Worker {worker_id}: Creating model adapter for {stub.model_name}")
                    
                    # Import here to avoid issues with multiprocessing
                    from sparse_attention_hub.adapters.huggingface import ModelAdapterHF
                    
                    adapter = ModelAdapterHF(
                        model_name=stub.model_name,
                        sparse_attention_config=stub.sparse_attention_config,
                        model_kwargs=stub.adapter_config.model_kwargs,
                        tokenizer_kwargs=stub.adapter_config.tokenizer_kwargs
                    )
                    
                    # Create benchmark instance
                    logger.info(f"Worker {worker_id}: Creating benchmark instance for {stub.benchmark_name}")
                    benchmark = create_benchmark_instance(
                        benchmark_name=stub.benchmark_name,
                        subsets=[stub.subset] if stub.subset else None
                    )
                    
                    # Execute benchmark
                    logger.info(f"Worker {worker_id}: Executing benchmark {stub.benchmark_name} on GPU {current_gpu_id}")
                    metric_logger = MicroMetricLogger()
                    metric_logger.configure_logging(
                        log_path=stub.result_dir, 
                        enabled_metrics=["research_attention_density", "research_attention_output_error"],
                        max_records=max_records,
                        sampling_factor=sampling_factor
                    )
                    metrics = benchmark.run_benchmark(
                        adapter=adapter,
                        result_dir=stub.result_dir,
                        generation_kwargs=stub.generation_kwargs,
                        request_kwargs=stub.request_kwargs
                    )
                    metric_logger.flush()
                    
                    execution_time = time.time() - start_time
                    execution_success = True
                    
                    # Report successful result
                    result = BenchmarkResult(
                        stub=stub,
                        metrics=metrics,
                        execution_time=execution_time,
                        success=True
                    )
                    result_queue.put(result)
                    
                    logger.info(f"Worker {worker_id}: Successfully completed {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name} in {execution_time:.2f}s")

            except Exception as e:
                execution_time = time.time() - start_time  # Calculate execution time even on failure
                error_category = _categorize_error(e, "benchmark execution")
                error_msg = f"Benchmark execution failed: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                
                # Log detailed error information for debugging
                logger.debug(f"Worker {worker_id}: Error details: {traceback.format_exc()}")
                
                # Report failure
                failure = BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=execution_time
                )
                error_queue.put(failure)
                continue
            
            finally:
                # Step 4: Cleanup and release GPU
                _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
                current_gpu_id = None
                adapter = None
    
    except Exception as worker_error:
        logger.error(f"Worker {worker_id}: Critical worker error: {worker_error}")
        logger.debug(f"Worker {worker_id}: Critical error details: {traceback.format_exc()}")
        
        # Cleanup any remaining resources
        _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
        
        # Report worker failure to error queue
        error_queue.put({
            "worker_id": worker_id,
            "error_type": "WORKER_CRITICAL_ERROR",
            "error_message": str(worker_error),
            "error_traceback": traceback.format_exc()
        })


def _cleanup_worker_resources(
    gpu_pool: multiprocessing.Queue,
    gpu_id: Optional[int],
    adapter: Any,
    logger: logging.Logger,
    worker_id: int
) -> None:
    """Clean up worker resources including GPU memory and adapter.
    
    Args:
        gpu_pool: GPU pool queue to return GPU to
        gpu_id: GPU ID to release (can be None) - this is the actual GPU ID from the pool
        adapter: Model adapter to cleanup (can be None)
        logger: Logger instance for the worker
        worker_id: Worker process ID for logging
    """
    try:
        # Clean up adapter if it exists
        if adapter is not None:
            try:
                # Force garbage collection of adapter
                del adapter
                import gc
                gc.collect()
                logger.debug(f"Worker {worker_id}: Cleaned up model adapter")
            except Exception as e:
                logger.warning(f"Worker {worker_id}: Failed to cleanup adapter: {e}")
        
        # Clean up GPU memory and release GPU
        if gpu_id is not None:
            try:
                cleanup_gpu_memory(gpu_id)
                logger.debug(f"Worker {worker_id}: Cleaned up GPU memory for GPU {gpu_id}")
            
                # Release GPU back to pool (always use the original GPU ID for the pool)
                release_gpu_to_pool(gpu_pool, gpu_id)
                logger.debug(f"Worker {worker_id}: Released GPU {gpu_id} back to pool")
                
            except Exception as cleanup_error:
                logger.warning(f"Worker {worker_id}: GPU cleanup failed for GPU {gpu_id}: {cleanup_error}")
                # Try to release GPU even if cleanup failed
                try:
                    release_gpu_to_pool(gpu_pool, gpu_id)
                except Exception:
                    logger.error(f"Worker {worker_id}: Failed to release GPU {gpu_id} to pool")
    
    except Exception as e:
        logger.error(f"Worker {worker_id}: Error during resource cleanup: {e}")


class BenchmarkExecutor:
    """Parallel benchmark executor with dynamic GPU allocation and resumability.
    
    Executes benchmark experiments across a model × sparse_attention_config × 
    benchmark × subset matrix with parallel GPU execution and automatic resumability.
    
    Features:
    - Dynamic GPU allocation from specified gpu_ids
    - Subset-level resumability (skip completed experiments)
    - Parallel execution with configurable concurrency
    - Comprehensive error handling and logging
    - Result aggregation and summary generation
    
    Example:
        >>> executor = BenchmarkExecutor(
        ...     gpu_ids=[0, 1, 2, 3],
        ...     max_concurrent_runs=4,
        ...     base_result_dir="./benchmark_results"
        ... )
        >>> 
        >>> results = executor.run_benchmark_matrix(
        ...     model_names=["model1", "model2"],
        ...     sparse_attention_configs=[("dense", None), ("sparse", config)],
        ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
        ...     adapter_config=AdapterConfig()
        ... )
    """
    
    def __init__(
        self,
        gpu_ids: List[int],
        max_concurrent_runs: int,
        base_result_dir: str = "./benchmark_results",
        enable_resumability: bool = True,
        result_file_validation: bool = True,
        required_result_files: Optional[List[str]] = None,
        timeout_per_benchmark: float = 360000.0,  # 100 hours default
        verbose: bool = True,
        max_records: Optional[int] = None,  # Maximum number of metric events to log
        sampling_factor: float = 1.0,  # Probability of logging each metric event (0.0-1.0)
    ):
        """Initialize the BenchmarkExecutor.
        
        Args:
            gpu_ids: List of GPU device IDs to use for parallel execution
            max_concurrent_runs: Maximum number of concurrent benchmark runs
            base_result_dir: Base directory for storing benchmark results
            enable_resumability: Whether to skip already completed experiments
            result_file_validation: Whether to check for specific result files
            required_result_files: List of files required for completion check
            timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
            verbose: Whether to enable verbose logging
            max_records: Maximum number of metric events to log (None for unlimited)
            sampling_factor: Probability of logging each metric event (0.0-1.0)
            
        Raises:
            ValueError: If configuration parameters are invalid
            RuntimeError: If GPU validation fails
        """
        # Validate and store configuration
        self.gpu_ids = self._validate_gpu_ids(gpu_ids)
        self.max_concurrent_runs = self._validate_max_concurrent_runs(max_concurrent_runs)
        self.base_result_dir = Path(base_result_dir).resolve()
        self.enable_resumability = enable_resumability
        self.result_file_validation = result_file_validation
        self.required_result_files = required_result_files or ["raw_results.csv"]
        self.timeout_per_benchmark = timeout_per_benchmark
        self.verbose = verbose
        
        # Metric logging configuration
        self.max_records = max_records
        self.sampling_factor = max(0.0, min(1.0, sampling_factor))  # Clamp to [0.0, 1.0]
        
        # Initialize logging
        self._setup_logging()
        
        # Set up signal handlers for graceful shutdown
        self._setup_signal_handlers()
        
        # Validate GPU availability
        self._validate_gpu_setup()
        
        # Load all available benchmarks
        ensure_benchmarks_loaded()
        

        
        self.logger.info(f"BenchmarkExecutor initialized:")
        self.logger.info(f"  GPU IDs: {self.gpu_ids}")
        self.logger.info(f"  Max concurrent runs: {self.max_concurrent_runs}")
        self.logger.info(f"  Base result directory: {self.base_result_dir}")
        self.logger.info(f"  Resumability: {'enabled' if self.enable_resumability else 'disabled'}")
    
    def _setup_signal_handlers(self) -> None:
        """Set up signal handlers for graceful shutdown."""
        signal.signal(signal.SIGTERM, _signal_handler)
        signal.signal(signal.SIGINT, _signal_handler)
        
        # Register cleanup function to run at exit
        import atexit
        atexit.register(self._cleanup_on_exit)
    
    def _cleanup_on_exit(self) -> None:
        """Cleanup function called when the process exits."""
        if hasattr(self, 'logger'):
            self.logger.info("BenchmarkExecutor cleanup on exit")
        
        # Reset global shutdown flag
        global _shutdown_requested
        _shutdown_requested = False

    def run_benchmark_matrix(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig,
        generation_kwargs: Optional[Dict[str, Any]] = None,
        request_kwargs: Optional[Dict[str, Any]] = None
    ) -> ExecutionResults:
        """Execute benchmark matrix with parallel GPU execution.
        
        Runs all combinations of model_names × sparse_attention_configs × 
        benchmark_configs × subsets in parallel across available GPUs.
        
        Args:
            model_names: List of model names to benchmark
            sparse_attention_configs: List of (name, config) tuples for sparse attention
            benchmark_configs: List of benchmark configurations
            adapter_config: Configuration for model adapter
            generation_kwargs: Optional generation parameters
            request_kwargs: Optional request processing parameters
            
        Returns:
            ExecutionResults containing comprehensive execution information
            
        Example:
            >>> results = executor.run_benchmark_matrix(
            ...     model_names=["meta-llama/Llama-3.1-8B-Instruct"],
            ...     sparse_attention_configs=[("dense", None)],
            ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
            ...     adapter_config=AdapterConfig()
            ... )
            >>> print(f"Completed: {results.completed_count}/{results.total_count}")
        """
        # Initialize execution
        self.logger.info("Starting benchmark matrix execution")
        
        try:
            # Step 1: Validate input parameters
            self._validate_matrix_parameters(
                model_names, sparse_attention_configs, benchmark_configs, adapter_config
            )
            
            # Step 2: Generate benchmark stubs (matrix expansion)
            self.logger.info("Generating benchmark matrix...")
            all_stubs = generate_benchmark_stubs(
                model_names=model_names,
                sparse_attention_configs=sparse_attention_configs,
                benchmark_configs=benchmark_configs,
                adapter_config=adapter_config,
                generation_kwargs=generation_kwargs,
                request_kwargs=request_kwargs,
                base_result_dir=str(self.base_result_dir)
            )
            
            total_combinations = len(all_stubs)
            self.logger.info(f"Generated {total_combinations} benchmark combinations:")
            self.logger.info(f"  {len(model_names)} models × {len(sparse_attention_configs)} sparse configs × "
                           f"{sum(len(b.subsets) if b.subsets else 1 for b in benchmark_configs)} benchmark-subsets")
            
            # Step 3: Filter for resumability (if enabled)
            if self.enable_resumability:
                self.logger.info("Checking for existing results (resumability)...")
                pending_stubs, completed_stubs = filter_existing_results(
                    all_stubs,
                    check_result_files=self.result_file_validation,
                    required_files=self.required_result_files,
                    verbose=self.verbose
                )
                
                self.logger.info(f"Resumability check: {len(completed_stubs)} completed, {len(pending_stubs)} pending")
            else:
                pending_stubs = all_stubs
                completed_stubs = []
                self.logger.info("Resumability disabled - will execute all combinations")
            
            # Step 4: Early exit if nothing to do
            if not pending_stubs:
                self.logger.info("No pending benchmarks to execute. All experiments already completed!")
                
                execution_summary = {
                    "total_combinations": total_combinations,
                    "models": model_names,
                    "sparse_configs": [name for name, _ in sparse_attention_configs],
                    "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs]
                }
                
                progress = ExecutionProgress(
                    total_stubs=total_combinations,
                    skipped_stubs=len(completed_stubs),
                    completed_stubs=0,
                    failed_stubs=0
                )
                
                return ExecutionResults(
                    execution_summary=execution_summary,
                    individual_results=[],
                    failed_executions=[],
                    execution_time=0.0,
                    progress=progress
                )
            

            
            # Step 6: Execute pending benchmarks with worker processes
            self.logger.info(f"Executing {len(pending_stubs)} pending benchmarks with {self.max_concurrent_runs} workers...")
            
            start_time = time.time()
            results = self._execute_with_workers(pending_stubs)
            execution_time = time.time() - start_time
            
            # Step 7: Aggregate results
            execution_summary = {
                "total_combinations": total_combinations,
                "models": model_names,
                "sparse_configs": [name for name, _ in sparse_attention_configs],
                "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs],
                "execution_time_seconds": execution_time,
                "gpu_ids_used": self.gpu_ids,
                "max_concurrent_runs": self.max_concurrent_runs
            }
            
            progress = ExecutionProgress(
                total_stubs=total_combinations,
                skipped_stubs=len(completed_stubs),
                completed_stubs=len(results.individual_results),
                failed_stubs=len(results.failed_executions)
            )
            
            final_results = ExecutionResults(
                execution_summary=execution_summary,
                individual_results=results.individual_results,
                failed_executions=results.failed_executions,
                execution_time=execution_time,
                progress=progress
            )
            
            self.logger.info(f"Benchmark matrix execution completed:")
            self.logger.info(f"  Total time: {execution_time:.2f}s")
            self.logger.info(f"  Successful: {len(results.individual_results)}")
            self.logger.info(f"  Failed: {len(results.failed_executions)}")
            self.logger.info(f"  Skipped: {len(completed_stubs)}")
            
            return final_results
            
        except Exception as e:
            self.logger.error(f"Benchmark execution failed: {e}")
            raise
    
    def _execute_with_workers(self, pending_stubs: List[BenchmarkStub]) -> ExecutionResults:
        """Execute benchmark stubs using worker processes.
        
        This method implements comprehensive process pool management with:
        - Dynamic work distribution through shared queues
        - Graceful process startup and shutdown
        - Real-time result collection and error handling
        - Process health monitoring and recovery
        
        Args:
            pending_stubs: List of benchmark stubs to execute
            
        Returns:
            ExecutionResults containing all results and failures
        """
        self.logger.info(f"Starting execution with {len(pending_stubs)} stubs using {self.max_concurrent_runs} workers")
        
        # Step 1: Create and configure shared queues
        stub_queue = multiprocessing.Queue(maxsize=len(pending_stubs) + 100)  # Buffer for queue operations
        gpu_pool = create_gpu_pool(self.gpu_ids)
        result_queue = multiprocessing.Queue()
        error_queue = multiprocessing.Queue()
        
        # Step 2: Populate work queue with all stubs
        self.logger.info("Populating work queue...")
        for stub in pending_stubs:
            stub_queue.put(stub)
        
        # Add sentinel values to signal workers to stop
        for _ in range(self.max_concurrent_runs):
            stub_queue.put(None)  # Sentinel value
        
        # Step 3: Create and start worker processes
        workers = []
        worker_pids = []
        
        self.logger.info(f"Starting {self.max_concurrent_runs} worker processes...")
        for i in range(self.max_concurrent_runs):
            worker = multiprocessing.Process(
                target=_benchmark_worker,
                args=(stub_queue, gpu_pool, result_queue, error_queue, self.timeout_per_benchmark),
                name=f"benchmark_worker_{i}"
            )
            worker.daemon = True  # Ensure workers are terminated when main process exits
            worker.start()
            workers.append(worker)
            worker_pids.append(worker.pid)
            self.logger.debug(f"Started worker {i+1}/{self.max_concurrent_runs} (PID: {worker.pid})")
        
        self.logger.info(f"All {self.max_concurrent_runs} workers started successfully")
        
        # Step 4: Monitor and collect results in real-time
        individual_results: List[BenchmarkResult] = []
        failed_executions: List[BenchmarkFailure] = []
        active_workers = set(worker_pids)
        
        self.logger.info("Monitoring worker processes and collecting results...")
        
        try:
            # Monitor workers and collect results until all work is complete
            while active_workers:
                # Check for completed workers
                completed_workers = []
                for worker in workers:
                    if worker.is_alive():
                        # Worker is still running, check for results
                        pass
                    else:
                        # Worker has completed
                        if worker.pid in active_workers:
                            active_workers.remove(worker.pid)
                            completed_workers.append(worker)
                            
                            if worker.exitcode != 0:
                                self.logger.warning(f"Worker {worker.pid} exited with code {worker.exitcode}")
                            else:
                                self.logger.debug(f"Worker {worker.pid} completed successfully")
                
                # Collect any available results (non-blocking)
                self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
                
                # Log progress periodically
                if len(individual_results) + len(failed_executions) > 0 and (len(individual_results) + len(failed_executions)) % 5 == 0:
                    self.logger.info(f"Progress: {len(individual_results)} completed, {len(failed_executions)} failed, {len(active_workers)} workers active")
                
                # Small delay to prevent busy waiting
                time.sleep(0.1)
            
            # Step 5: Final result collection
            self.logger.info("All workers completed, collecting final results...")
            self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
            
        except KeyboardInterrupt:
            self.logger.warning("Received interrupt signal, initiating graceful shutdown...")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        except Exception as e:
            error_category = _categorize_error(e, "main execution")
            self.logger.error(f"Error during execution ({error_category}): {e}")
            self.logger.debug(f"Execution error details: {traceback.format_exc()}")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        finally:
            # Step 6: Cleanup resources
            self._cleanup_resources(workers, stub_queue, gpu_pool, result_queue, error_queue)
        
        # Step 7: Final summary
        total_processed = len(individual_results) + len(failed_executions)
        self.logger.info(f"Execution completed: {len(individual_results)} successful, {len(failed_executions)} failed")
        self.logger.info(f"Success rate: {len(individual_results)/total_processed*100:.1f}%" if total_processed > 0 else "No work processed")
        
        return ExecutionResults(
            execution_summary={},  # Will be filled by caller
            individual_results=individual_results,
            failed_executions=failed_executions,
            execution_time=0.0,  # Will be filled by caller
            progress=ExecutionProgress(
                total_stubs=len(pending_stubs),
                completed_stubs=len(individual_results),
                failed_stubs=len(failed_executions)
            )
        )
    
    def _collect_available_results(
        self,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue,
        individual_results: List[BenchmarkResult],
        failed_executions: List[BenchmarkFailure]
    ) -> None:
        """Collect all available results from queues without blocking.
        
        Args:
            result_queue: Queue containing successful benchmark results
            error_queue: Queue containing benchmark failures
            individual_results: List to append successful results to
            failed_executions: List to append failures to
        """
        # Collect all available successful results
        while True:
            try:
                result = result_queue.get_nowait()
                individual_results.append(result)
            except Empty:
                break
        
        # Collect all available errors
        while True:
            try:
                error = error_queue.get_nowait()
                if isinstance(error, BenchmarkFailure):
                    failed_executions.append(error)
                else:
                    # Handle worker-level errors
                    self.logger.error(f"Worker error: {error}")
            except Empty:
                break
    
    def _graceful_shutdown(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Perform graceful shutdown of worker processes and queues.
        
        Args:
            workers: List of worker processes to terminate
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.info("Initiating graceful shutdown...")
        
        try:
            # Signal workers to stop by adding sentinel values
            sentinel_count = 0
            for _ in range(len(workers)):
                try:
                    stub_queue.put_nowait(None)
                    sentinel_count += 1
                except Exception as e:
                    self.logger.debug(f"Could not add sentinel to queue: {e}")
                    break  # Queue is full, stop adding sentinels
            
            self.logger.info(f"Added {sentinel_count} shutdown signals to work queue")
            
            # Wait for workers to finish with timeout
            self.logger.info("Waiting for workers to finish...")
            for i, worker in enumerate(workers):
                try:
                    worker.join(timeout=30.0)  # 30 second timeout per worker
                    if worker.is_alive():
                        self.logger.warning(f"Force terminating worker {worker.pid}")
                        worker.terminate()
                        worker.join(timeout=5.0)
                        if worker.is_alive():
                            self.logger.error(f"Failed to terminate worker {worker.pid}, killing process")
                            worker.kill()
                            worker.join(timeout=2.0)
                    else:
                        self.logger.debug(f"Worker {worker.pid} terminated successfully")
                except Exception as e:
                    self.logger.error(f"Error during worker {worker.pid} shutdown: {e}")
            
            self.logger.info("Graceful shutdown completed")
            
        except Exception as e:
            self.logger.error(f"Error during graceful shutdown: {e}")
            # Force cleanup of remaining workers
            for worker in workers:
                if worker.is_alive():
                    try:
                        worker.kill()
                    except Exception:
                        pass
    
    def _cleanup_resources(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Clean up all multiprocessing resources.
        
        Args:
            workers: List of worker processes
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.debug("Cleaning up multiprocessing resources...")
        
        # Close all queues
        try:
            stub_queue.close()
            gpu_pool.close()
            result_queue.close()
            error_queue.close()
        except Exception as e:
            self.logger.warning(f"Error closing queues: {e}")
        
        # Join any remaining workers
        for worker in workers:
            if worker.is_alive():
                self.logger.warning(f"Force terminating remaining worker {worker.pid}")
                worker.terminate()
                worker.join(timeout=5.0)
                if worker.is_alive():
                    worker.kill()
        
        self.logger.debug("Resource cleanup completed")
    
    def _validate_gpu_ids(self, gpu_ids: List[int]) -> List[int]:
        """Validate and return GPU IDs."""
        if not gpu_ids:
            raise ValueError("gpu_ids cannot be empty")
        
        if not all(isinstance(gpu_id, int) and gpu_id >= 0 for gpu_id in gpu_ids):
            raise ValueError("All GPU IDs must be non-negative integers")
        
        # Remove duplicates while preserving order
        seen = set()
        unique_gpu_ids = []
        for gpu_id in gpu_ids:
            if gpu_id not in seen:
                seen.add(gpu_id)
                unique_gpu_ids.append(gpu_id)
        
        return unique_gpu_ids
    
    def _validate_max_concurrent_runs(self, max_concurrent_runs: int) -> int:
        """Validate and return max_concurrent_runs."""
        if not isinstance(max_concurrent_runs, int) or max_concurrent_runs <= 0:
            raise ValueError("max_concurrent_runs must be a positive integer")
        
        if max_concurrent_runs > len(self.gpu_ids) if hasattr(self, 'gpu_ids') else True:
            # This is just a warning - user might want more concurrent runs than GPUs
            # for benchmarks that don't require much GPU memory
            pass
        
        return max_concurrent_runs
    
    def _setup_logging(self) -> None:
        """Setup logging configuration."""
        self.logger = logging.getLogger(f"{__name__}.BenchmarkExecutor")
        
        if self.verbose and not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def _validate_gpu_setup(self) -> None:
        """Validate GPU availability."""
        try:
            validate_gpu_availability(self.gpu_ids)
            self.logger.info(f"GPU validation passed for devices: {self.gpu_ids}")
        except (RuntimeError, ValueError) as e:
            self.logger.error(f"GPU validation failed: {e}")
            raise
    
    def _validate_matrix_parameters(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig
    ) -> None:
        """Validate benchmark matrix parameters."""
        if not model_names:
            raise ValueError("model_names cannot be empty")
        
        if not sparse_attention_configs:
            raise ValueError("sparse_attention_configs cannot be empty")
        
        if not benchmark_configs:
            raise ValueError("benchmark_configs cannot be empty")
        
        # Validate model names
        for model_name in model_names:
            if not isinstance(model_name, str) or not model_name.strip():
                raise ValueError(f"Invalid model name: {model_name}")
        
        # Validate sparse attention configs
        for sparse_config in sparse_attention_configs:
            if not isinstance(sparse_config, tuple) or len(sparse_config) != 2:
                raise ValueError(f"sparse_attention_configs must be list of (name, config) tuples")
            
            config_name, config_obj = sparse_config
            if not isinstance(config_name, str) or not config_name.strip():
                raise ValueError(f"Sparse config name must be non-empty string: {config_name}")
        
        # Validate benchmark configs
        for benchmark_config in benchmark_configs:
            if not isinstance(benchmark_config, BenchmarkConfig):
                raise ValueError(f"benchmark_configs must contain BenchmarkConfig instances")
            # Additional validation could be added here if needed
        
        # Validate adapter config
        if not isinstance(adapter_config, AdapterConfig):
            raise ValueError("adapter_config must be an AdapterConfig instance")
            # Additional validation could be added here if needed
        
        self.logger.info("Matrix parameter validation passed")

```

```python
"""BenchmarkExecutor: Parallel GPU execution for benchmark matrix experiments.

This module contains the main BenchmarkExecutor class that orchestrates parallel 
execution of benchmark experiments across the model × sparse_attention_config × 
benchmark × subset matrix with dynamic GPU allocation and resumability.
"""

import logging
import multiprocessing
import os
import signal
import sys
import time
import traceback
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from queue import Empty
from contextlib import contextmanager
from sparse_attention_hub.metric_logging.logger import MicroMetricLogger

# Set multiprocessing start method to 'spawn' for CUDA compatibility
if multiprocessing.get_start_method(allow_none=True) != 'spawn':
    multiprocessing.set_start_method('spawn', force=True)

from sparse_attention_hub.sparse_attention.base import SparseAttentionConfig

from .executor_config import (
    AdapterConfig,
    BenchmarkConfig,
    BenchmarkStub,
    ExecutionResults,
    ExecutionProgress,
    BenchmarkResult,
    BenchmarkFailure,
    filter_existing_results,
    generate_benchmark_stubs,

)
from .benchmark_registry import (
    ensure_benchmarks_loaded,
    create_benchmark_instance
)

from .utils.gpu import (
    cleanup_gpu_memory,
    release_gpu_to_pool,
    acquire_gpu_from_pool,
    create_gpu_pool,
    validate_gpu_availability,
)

# Global flag for graceful shutdown
_shutdown_requested = False


@contextmanager
def set_gpu(gpu_id: int):
    """Context manager to set the current CUDA device.
    
    Args:
        gpu_id: The GPU device ID to set as current device
        
    Yields:
        None: The context manager yields control to the with block
    """
    import torch
    torch.cuda.set_device(gpu_id)
    yield


def _signal_handler(signum: int, frame: Any) -> None:
    """Handle shutdown signals gracefully."""
    global _shutdown_requested
    _shutdown_requested = True
    logging.getLogger(__name__).warning(f"Received signal {signum}, initiating graceful shutdown...")


def _categorize_error(error: Exception, context: str = "") -> str:
    """Categorize errors for better handling and reporting.
    
    Args:
        error: The exception that occurred
        context: Additional context about where the error occurred
        
    Returns:
        Error category string for classification
    """
    error_type = type(error).__name__
    error_msg = str(error).lower()
    
    # GPU-related errors
    if any(keyword in error_msg for keyword in ["cuda", "gpu", "out of memory", "oom"]):
        if "out of memory" in error_msg or "oom" in error_msg:
            return "GPU_OUT_OF_MEMORY"
        elif "cuda" in error_msg:
            return "GPU_CUDA_ERROR"
        else:
            return "GPU_ERROR"
    
    # Model loading errors
    elif any(keyword in error_msg for keyword in ["model", "huggingface", "transformers", "pretrained"]):
        if "not found" in error_msg or "doesn't exist" in error_msg:
            return "MODEL_NOT_FOUND"
        elif "tokenizer" in error_msg:
            return "TOKENIZER_ERROR"
        else:
            return "MODEL_LOADING_ERROR"
    
    # Dataset/benchmark errors
    elif any(keyword in error_msg for keyword in ["dataset", "benchmark", "subset"]):
        return "DATASET_ERROR"
    
    # Network/connection errors
    elif any(keyword in error_msg for keyword in ["connection", "timeout", "network", "http"]):
        return "NETWORK_ERROR"
    
    # File system errors
    elif any(keyword in error_msg for keyword in ["file", "directory", "permission", "disk"]):
        return "FILESYSTEM_ERROR"
    
    # Multiprocessing errors
    elif any(keyword in error_msg for keyword in ["queue", "process", "multiprocessing"]):
        return "MULTIPROCESSING_ERROR"
    
    # Default categories
    elif isinstance(error, TimeoutError):
        return "TIMEOUT_ERROR"
    elif isinstance(error, KeyboardInterrupt):
        return "INTERRUPT_ERROR"
    elif isinstance(error, MemoryError):
        return "SYSTEM_MEMORY_ERROR"
    else:
        return "UNKNOWN_ERROR"


def _benchmark_worker(
    stub_queue: multiprocessing.Queue,
    gpu_pool: multiprocessing.Queue,
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue,
    timeout_per_benchmark: float = 3600.0,
    max_records: Optional[int] = None,
    sampling_factor: float = 1.0
) -> None:
    """Worker function to execute benchmarks with dynamic GPU allocation.
    
    This function runs in a separate process and handles:
    1. Acquiring GPUs from the shared pool
    2. Creating model adapters with sparse attention configurations
    3. Executing benchmarks with proper error handling
    4. Releasing GPUs back to the pool
    5. Reporting results and errors
    
    Args:
        stub_queue: Queue containing BenchmarkStub instances to process
        gpu_pool: Shared queue containing available GPU IDs
        result_queue: Queue for reporting successful benchmark results
        error_queue: Queue for reporting benchmark failures
        timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
        max_records: Maximum number of metric events to log (None for unlimited)
        sampling_factor: Probability of logging each metric event (0.0-1.0)
    """
    worker_id = os.getpid()
    logger = logging.getLogger(f"{__name__}.worker_{worker_id}")
    
    # Set up signal handlers for graceful shutdown
    signal.signal(signal.SIGTERM, _signal_handler)
    signal.signal(signal.SIGINT, _signal_handler)
    
    # Track resources for cleanup
    current_gpu_id: Optional[int] = None
    adapter = None

    try:
        while True:
            # Check for shutdown signal
            if _shutdown_requested:
                logger.info(f"Worker {worker_id}: Shutdown requested, exiting")
                break
            
            # Step 1: Get next benchmark stub from queue
            try:
                stub: BenchmarkStub = stub_queue.get(timeout=1.0)  # 1 second timeout
            except Empty:
                # No more work to do
                logger.info(f"Worker {worker_id}: No more work, shutting down")
                break
            
            # Check for sentinel value (shutdown signal)
            if stub is None:
                logger.info(f"Worker {worker_id}: Received shutdown signal, exiting")
                break
            
            # Step 2: Acquire GPU from pool
            try:
                current_gpu_id = acquire_gpu_from_pool(gpu_pool, timeout=30.0)
                logger.info(f"Worker {worker_id}: Acquired GPU {current_gpu_id} for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}")
            except Exception as e:
                error_category = _categorize_error(e, "GPU acquisition")
                error_msg = f"Failed to acquire GPU for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                error_queue.put(BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=0.0
                ))
                continue

            # Step 3: Run the benchmark on the current GPU
            start_time = time.time()
            execution_time = 0.0  # Initialize execution_time
            
            try:
                with set_gpu(current_gpu_id):
                    logger.debug(f"Worker {worker_id}: Set CUDA device to {current_gpu_id}")
                    execution_success = False

                    # Create model adapter with sparse attention config
                    logger.info(f"Worker {worker_id}: Creating model adapter for {stub.model_name}")
                    
                    # Import here to avoid issues with multiprocessing
                    from sparse_attention_hub.adapters.huggingface import ModelAdapterHF
                    
                    adapter = ModelAdapterHF(
                        model_name=stub.model_name,
                        sparse_attention_config=stub.sparse_attention_config,
                        model_kwargs=stub.adapter_config.model_kwargs,
                        tokenizer_kwargs=stub.adapter_config.tokenizer_kwargs
                    )
                    
                    # Create benchmark instance
                    logger.info(f"Worker {worker_id}: Creating benchmark instance for {stub.benchmark_name}")
                    benchmark = create_benchmark_instance(
                        benchmark_name=stub.benchmark_name,
                        subsets=[stub.subset] if stub.subset else None
                    )
                    
                    # Execute benchmark
                    logger.info(f"Worker {worker_id}: Executing benchmark {stub.benchmark_name} on GPU {current_gpu_id}")
                    metric_logger = MicroMetricLogger()
                    metric_logger.configure_logging(
                        log_path=stub.result_dir, 
                        enabled_metrics=["research_attention_density", "research_attention_output_error"],
                        max_records=max_records,
                        sampling_factor=sampling_factor
                    )
                    metrics = benchmark.run_benchmark(
                        adapter=adapter,
                        result_dir=stub.result_dir,
                        generation_kwargs=stub.generation_kwargs,
                        request_kwargs=stub.request_kwargs
                    )
                    metric_logger.flush()
                    
                    execution_time = time.time() - start_time
                    execution_success = True
                    
                    # Report successful result
                    result = BenchmarkResult(
                        stub=stub,
                        metrics=metrics,
                        execution_time=execution_time,
                        success=True
                    )
                    result_queue.put(result)
                    
                    logger.info(f"Worker {worker_id}: Successfully completed {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name} in {execution_time:.2f}s")

            except Exception as e:
                execution_time = time.time() - start_time  # Calculate execution time even on failure
                error_category = _categorize_error(e, "benchmark execution")
                error_msg = f"Benchmark execution failed: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                
                # Log detailed error information for debugging
                logger.debug(f"Worker {worker_id}: Error details: {traceback.format_exc()}")
                
                # Report failure
                failure = BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=execution_time
                )
                error_queue.put(failure)
                continue
            
            finally:
                # Step 4: Cleanup and release GPU
                _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
                current_gpu_id = None
                adapter = None
    
    except Exception as worker_error:
        logger.error(f"Worker {worker_id}: Critical worker error: {worker_error}")
        logger.debug(f"Worker {worker_id}: Critical error details: {traceback.format_exc()}")
        
        # Cleanup any remaining resources
        _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
        
        # Report worker failure to error queue
        error_queue.put({
            "worker_id": worker_id,
            "error_type": "WORKER_CRITICAL_ERROR",
            "error_message": str(worker_error),
            "error_traceback": traceback.format_exc()
        })


def _cleanup_worker_resources(
    gpu_pool: multiprocessing.Queue,
    gpu_id: Optional[int],
    adapter: Any,
    logger: logging.Logger,
    worker_id: int
) -> None:
    """Clean up worker resources including GPU memory and adapter.
    
    Args:
        gpu_pool: GPU pool queue to return GPU to
        gpu_id: GPU ID to release (can be None) - this is the actual GPU ID from the pool
        adapter: Model adapter to cleanup (can be None)
        logger: Logger instance for the worker
        worker_id: Worker process ID for logging
    """
    try:
        # Clean up adapter if it exists
        if adapter is not None:
            try:
                # Force garbage collection of adapter
                del adapter
                import gc
                gc.collect()
                logger.debug(f"Worker {worker_id}: Cleaned up model adapter")
            except Exception as e:
                logger.warning(f"Worker {worker_id}: Failed to cleanup adapter: {e}")
        
        # Clean up GPU memory and release GPU
        if gpu_id is not None:
            try:
                cleanup_gpu_memory(gpu_id)
                logger.debug(f"Worker {worker_id}: Cleaned up GPU memory for GPU {gpu_id}")
            
                # Release GPU back to pool (always use the original GPU ID for the pool)
                release_gpu_to_pool(gpu_pool, gpu_id)
                logger.debug(f"Worker {worker_id}: Released GPU {gpu_id} back to pool")
                
            except Exception as cleanup_error:
                logger.warning(f"Worker {worker_id}: GPU cleanup failed for GPU {gpu_id}: {cleanup_error}")
                # Try to release GPU even if cleanup failed
                try:
                    release_gpu_to_pool(gpu_pool, gpu_id)
                except Exception:
                    logger.error(f"Worker {worker_id}: Failed to release GPU {gpu_id} to pool")
    
    except Exception as e:
        logger.error(f"Worker {worker_id}: Error during resource cleanup: {e}")


class BenchmarkExecutor:
    """Parallel benchmark executor with dynamic GPU allocation and resumability.
    
    Executes benchmark experiments across a model × sparse_attention_config × 
    benchmark × subset matrix with parallel GPU execution and automatic resumability.
    
    Features:
    - Dynamic GPU allocation from specified gpu_ids
    - Subset-level resumability (skip completed experiments)
    - Parallel execution with configurable concurrency
    - Comprehensive error handling and logging
    - Result aggregation and summary generation
    
    Example:
        >>> executor = BenchmarkExecutor(
        ...     gpu_ids=[0, 1, 2, 3],
        ...     max_concurrent_runs=4,
        ...     base_result_dir="./benchmark_results"
        ... )
        >>> 
        >>> results = executor.run_benchmark_matrix(
        ...     model_names=["model1", "model2"],
        ...     sparse_attention_configs=[("dense", None), ("sparse", config)],
        ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
        ...     adapter_config=AdapterConfig()
        ... )
    """
    
    def __init__(
        self,
        gpu_ids: List[int],
        max_concurrent_runs: int,
        base_result_dir: str = "./benchmark_results",
        enable_resumability: bool = True,
        result_file_validation: bool = True,
        required_result_files: Optional[List[str]] = None,
        timeout_per_benchmark: float = 360000.0,  # 100 hours default
        verbose: bool = True,
        max_records: Optional[int] = None,  # Maximum number of metric events to log
        sampling_factor: float = 1.0,  # Probability of logging each metric event (0.0-1.0)
    ):
        """Initialize the BenchmarkExecutor.
        
        Args:
            gpu_ids: List of GPU device IDs to use for parallel execution
            max_concurrent_runs: Maximum number of concurrent benchmark runs
            base_result_dir: Base directory for storing benchmark results
            enable_resumability: Whether to skip already completed experiments
            result_file_validation: Whether to check for specific result files
            required_result_files: List of files required for completion check
            timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
            verbose: Whether to enable verbose logging
            max_records: Maximum number of metric events to log (None for unlimited)
            sampling_factor: Probability of logging each metric event (0.0-1.0)
            
        Raises:
            ValueError: If configuration parameters are invalid
            RuntimeError: If GPU validation fails
        """
        # Validate and store configuration
        self.gpu_ids = self._validate_gpu_ids(gpu_ids)
        self.max_concurrent_runs = self._validate_max_concurrent_runs(max_concurrent_runs)
        self.base_result_dir = Path(base_result_dir).resolve()
        self.enable_resumability = enable_resumability
        self.result_file_validation = result_file_validation
        self.required_result_files = required_result_files or ["raw_results.csv"]
        self.timeout_per_benchmark = timeout_per_benchmark
        self.verbose = verbose
        
        # Metric logging configuration
        self.max_records = max_records
        self.sampling_factor = max(0.0, min(1.0, sampling_factor))  # Clamp to [0.0, 1.0]
        
        # Initialize logging
        self._setup_logging()
        
        # Set up signal handlers for graceful shutdown
        self._setup_signal_handlers()
        
        # Validate GPU availability
        self._validate_gpu_setup()
        
        # Load all available benchmarks
        ensure_benchmarks_loaded()
        

        
        self.logger.info(f"BenchmarkExecutor initialized:")
        self.logger.info(f"  GPU IDs: {self.gpu_ids}")
        self.logger.info(f"  Max concurrent runs: {self.max_concurrent_runs}")
        self.logger.info(f"  Base result directory: {self.base_result_dir}")
        self.logger.info(f"  Resumability: {'enabled' if self.enable_resumability else 'disabled'}")
    
    def _setup_signal_handlers(self) -> None:
        """Set up signal handlers for graceful shutdown."""
        signal.signal(signal.SIGTERM, _signal_handler)
        signal.signal(signal.SIGINT, _signal_handler)
        
        # Register cleanup function to run at exit
        import atexit
        atexit.register(self._cleanup_on_exit)
    
    def _cleanup_on_exit(self) -> None:
        """Cleanup function called when the process exits."""
        if hasattr(self, 'logger'):
            self.logger.info("BenchmarkExecutor cleanup on exit")
        
        # Reset global shutdown flag
        global _shutdown_requested
        _shutdown_requested = False

    def run_benchmark_matrix(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig,
        generation_kwargs: Optional[Dict[str, Any]] = None,
        request_kwargs: Optional[Dict[str, Any]] = None
    ) -> ExecutionResults:
        """Execute benchmark matrix with parallel GPU execution.
        
        Runs all combinations of model_names × sparse_attention_configs × 
        benchmark_configs × subsets in parallel across available GPUs.
        
        Args:
            model_names: List of model names to benchmark
            sparse_attention_configs: List of (name, config) tuples for sparse attention
            benchmark_configs: List of benchmark configurations
            adapter_config: Configuration for model adapter
            generation_kwargs: Optional generation parameters
            request_kwargs: Optional request processing parameters
            
        Returns:
            ExecutionResults containing comprehensive execution information
            
        Example:
            >>> results = executor.run_benchmark_matrix(
            ...     model_names=["meta-llama/Llama-3.1-8B-Instruct"],
            ...     sparse_attention_configs=[("dense", None)],
            ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
            ...     adapter_config=AdapterConfig()
            ... )
            >>> print(f"Completed: {results.completed_count}/{results.total_count}")
        """
        # Initialize execution
        self.logger.info("Starting benchmark matrix execution")
        
        try:
            # Step 1: Validate input parameters
            self._validate_matrix_parameters(
                model_names, sparse_attention_configs, benchmark_configs, adapter_config
            )
            
            # Step 2: Generate benchmark stubs (matrix expansion)
            self.logger.info("Generating benchmark matrix...")
            all_stubs = generate_benchmark_stubs(
                model_names=model_names,
                sparse_attention_configs=sparse_attention_configs,
                benchmark_configs=benchmark_configs,
                adapter_config=adapter_config,
                generation_kwargs=generation_kwargs,
                request_kwargs=request_kwargs,
                base_result_dir=str(self.base_result_dir)
            )
            
            total_combinations = len(all_stubs)
            self.logger.info(f"Generated {total_combinations} benchmark combinations:")
            self.logger.info(f"  {len(model_names)} models × {len(sparse_attention_configs)} sparse configs × "
                           f"{sum(len(b.subsets) if b.subsets else 1 for b in benchmark_configs)} benchmark-subsets")
            
            # Step 3: Filter for resumability (if enabled)
            if self.enable_resumability:
                self.logger.info("Checking for existing results (resumability)...")
                pending_stubs, completed_stubs = filter_existing_results(
                    all_stubs,
                    check_result_files=self.result_file_validation,
                    required_files=self.required_result_files,
                    verbose=self.verbose
                )
                
                self.logger.info(f"Resumability check: {len(completed_stubs)} completed, {len(pending_stubs)} pending")
            else:
                pending_stubs = all_stubs
                completed_stubs = []
                self.logger.info("Resumability disabled - will execute all combinations")
            
            # Step 4: Early exit if nothing to do
            if not pending_stubs:
                self.logger.info("No pending benchmarks to execute. All experiments already completed!")
                
                execution_summary = {
                    "total_combinations": total_combinations,
                    "models": model_names,
                    "sparse_configs": [name for name, _ in sparse_attention_configs],
                    "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs]
                }
                
                progress = ExecutionProgress(
                    total_stubs=total_combinations,
                    skipped_stubs=len(completed_stubs),
                    completed_stubs=0,
                    failed_stubs=0
                )
                
                return ExecutionResults(
                    execution_summary=execution_summary,
                    individual_results=[],
                    failed_executions=[],
                    execution_time=0.0,
                    progress=progress
                )
            

            
            # Step 6: Execute pending benchmarks with worker processes
            self.logger.info(f"Executing {len(pending_stubs)} pending benchmarks with {self.max_concurrent_runs} workers...")
            
            start_time = time.time()
            results = self._execute_with_workers(pending_stubs)
            execution_time = time.time() - start_time
            
            # Step 7: Aggregate results
            execution_summary = {
                "total_combinations": total_combinations,
                "models": model_names,
                "sparse_configs": [name for name, _ in sparse_attention_configs],
                "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs],
                "execution_time_seconds": execution_time,
                "gpu_ids_used": self.gpu_ids,
                "max_concurrent_runs": self.max_concurrent_runs
            }
            
            progress = ExecutionProgress(
                total_stubs=total_combinations,
                skipped_stubs=len(completed_stubs),
                completed_stubs=len(results.individual_results),
                failed_stubs=len(results.failed_executions)
            )
            
            final_results = ExecutionResults(
                execution_summary=execution_summary,
                individual_results=results.individual_results,
                failed_executions=results.failed_executions,
                execution_time=execution_time,
                progress=progress
            )
            
            self.logger.info(f"Benchmark matrix execution completed:")
            self.logger.info(f"  Total time: {execution_time:.2f}s")
            self.logger.info(f"  Successful: {len(results.individual_results)}")
            self.logger.info(f"  Failed: {len(results.failed_executions)}")
            self.logger.info(f"  Skipped: {len(completed_stubs)}")
            
            return final_results
            
        except Exception as e:
            self.logger.error(f"Benchmark execution failed: {e}")
            raise
    
    def _execute_with_workers(self, pending_stubs: List[BenchmarkStub]) -> ExecutionResults:
        """Execute benchmark stubs using worker processes.
        
        This method implements comprehensive process pool management with:
        - Dynamic work distribution through shared queues
        - Graceful process startup and shutdown
        - Real-time result collection and error handling
        - Process health monitoring and recovery
        
        Args:
            pending_stubs: List of benchmark stubs to execute
            
        Returns:
            ExecutionResults containing all results and failures
        """
        self.logger.info(f"Starting execution with {len(pending_stubs)} stubs using {self.max_concurrent_runs} workers")
        
        # Step 1: Create and configure shared queues
        stub_queue = multiprocessing.Queue(maxsize=len(pending_stubs) + 100)  # Buffer for queue operations
        gpu_pool = create_gpu_pool(self.gpu_ids)
        result_queue = multiprocessing.Queue()
        error_queue = multiprocessing.Queue()
        
        # Step 2: Populate work queue with all stubs
        self.logger.info("Populating work queue...")
        for stub in pending_stubs:
            stub_queue.put(stub)
        
        # Add sentinel values to signal workers to stop
        for _ in range(self.max_concurrent_runs):
            stub_queue.put(None)  # Sentinel value
        
        # Step 3: Create and start worker processes
        workers = []
        worker_pids = []
        
        self.logger.info(f"Starting {self.max_concurrent_runs} worker processes...")
        for i in range(self.max_concurrent_runs):
            worker = multiprocessing.Process(
                target=_benchmark_worker,
                args=(stub_queue, gpu_pool, result_queue, error_queue, self.timeout_per_benchmark, self.max_records, self.sampling_factor),
                name=f"benchmark_worker_{i}"
            )
            worker.daemon = True  # Ensure workers are terminated when main process exits
            worker.start()
            workers.append(worker)
            worker_pids.append(worker.pid)
            self.logger.debug(f"Started worker {i+1}/{self.max_concurrent_runs} (PID: {worker.pid})")
        
        self.logger.info(f"All {self.max_concurrent_runs} workers started successfully")
        
        # Step 4: Monitor and collect results in real-time
        individual_results: List[BenchmarkResult] = []
        failed_executions: List[BenchmarkFailure] = []
        active_workers = set(worker_pids)
        
        self.logger.info("Monitoring worker processes and collecting results...")
        
        try:
            # Monitor workers and collect results until all work is complete
            while active_workers:
                # Check for completed workers
                completed_workers = []
                for worker in workers:
                    if worker.is_alive():
                        # Worker is still running, check for results
                        pass
                    else:
                        # Worker has completed
                        if worker.pid in active_workers:
                            active_workers.remove(worker.pid)
                            completed_workers.append(worker)
                            
                            if worker.exitcode != 0:
                                self.logger.warning(f"Worker {worker.pid} exited with code {worker.exitcode}")
                            else:
                                self.logger.debug(f"Worker {worker.pid} completed successfully")
                
                # Collect any available results (non-blocking)
                self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
                
                # Log progress periodically
                if len(individual_results) + len(failed_executions) > 0 and (len(individual_results) + len(failed_executions)) % 5 == 0:
                    self.logger.info(f"Progress: {len(individual_results)} completed, {len(failed_executions)} failed, {len(active_workers)} workers active")
                
                # Small delay to prevent busy waiting
                time.sleep(0.1)
            
            # Step 5: Final result collection
            self.logger.info("All workers completed, collecting final results...")
            self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
            
        except KeyboardInterrupt:
            self.logger.warning("Received interrupt signal, initiating graceful shutdown...")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        except Exception as e:
            error_category = _categorize_error(e, "main execution")
            self.logger.error(f"Error during execution ({error_category}): {e}")
            self.logger.debug(f"Execution error details: {traceback.format_exc()}")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        finally:
            # Step 6: Cleanup resources
            self._cleanup_resources(workers, stub_queue, gpu_pool, result_queue, error_queue)
        
        # Step 7: Final summary
        total_processed = len(individual_results) + len(failed_executions)
        self.logger.info(f"Execution completed: {len(individual_results)} successful, {len(failed_executions)} failed")
        self.logger.info(f"Success rate: {len(individual_results)/total_processed*100:.1f}%" if total_processed > 0 else "No work processed")
        
        return ExecutionResults(
            execution_summary={},  # Will be filled by caller
            individual_results=individual_results,
            failed_executions=failed_executions,
            execution_time=0.0,  # Will be filled by caller
            progress=ExecutionProgress(
                total_stubs=len(pending_stubs),
                completed_stubs=len(individual_results),
                failed_stubs=len(failed_executions)
            )
        )
    
    def _collect_available_results(
        self,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue,
        individual_results: List[BenchmarkResult],
        failed_executions: List[BenchmarkFailure]
    ) -> None:
        """Collect all available results from queues without blocking.
        
        Args:
            result_queue: Queue containing successful benchmark results
            error_queue: Queue containing benchmark failures
            individual_results: List to append successful results to
            failed_executions: List to append failures to
        """
        # Collect all available successful results
        while True:
            try:
                result = result_queue.get_nowait()
                individual_results.append(result)
            except Empty:
                break
        
        # Collect all available errors
        while True:
            try:
                error = error_queue.get_nowait()
                if isinstance(error, BenchmarkFailure):
                    failed_executions.append(error)
                else:
                    # Handle worker-level errors
                    self.logger.error(f"Worker error: {error}")
            except Empty:
                break
    
    def _graceful_shutdown(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Perform graceful shutdown of worker processes and queues.
        
        Args:
            workers: List of worker processes to terminate
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.info("Initiating graceful shutdown...")
        
        try:
            # Signal workers to stop by adding sentinel values
            sentinel_count = 0
            for _ in range(len(workers)):
                try:
                    stub_queue.put_nowait(None)
                    sentinel_count += 1
                except Exception as e:
                    self.logger.debug(f"Could not add sentinel to queue: {e}")
                    break  # Queue is full, stop adding sentinels
            
            self.logger.info(f"Added {sentinel_count} shutdown signals to work queue")
            
            # Wait for workers to finish with timeout
            self.logger.info("Waiting for workers to finish...")
            for i, worker in enumerate(workers):
                try:
                    worker.join(timeout=30.0)  # 30 second timeout per worker
                    if worker.is_alive():
                        self.logger.warning(f"Force terminating worker {worker.pid}")
                        worker.terminate()
                        worker.join(timeout=5.0)
                        if worker.is_alive():
                            self.logger.error(f"Failed to terminate worker {worker.pid}, killing process")
                            worker.kill()
                            worker.join(timeout=2.0)
                    else:
                        self.logger.debug(f"Worker {worker.pid} terminated successfully")
                except Exception as e:
                    self.logger.error(f"Error during worker {worker.pid} shutdown: {e}")
            
            self.logger.info("Graceful shutdown completed")
            
        except Exception as e:
            self.logger.error(f"Error during graceful shutdown: {e}")
            # Force cleanup of remaining workers
            for worker in workers:
                if worker.is_alive():
                    try:
                        worker.kill()
                    except Exception:
                        pass
    
    def _cleanup_resources(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Clean up all multiprocessing resources.
        
        Args:
            workers: List of worker processes
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.debug("Cleaning up multiprocessing resources...")
        
        # Close all queues
        try:
            stub_queue.close()
            gpu_pool.close()
            result_queue.close()
            error_queue.close()
        except Exception as e:
            self.logger.warning(f"Error closing queues: {e}")
        
        # Join any remaining workers
        for worker in workers:
            if worker.is_alive():
                self.logger.warning(f"Force terminating remaining worker {worker.pid}")
                worker.terminate()
                worker.join(timeout=5.0)
                if worker.is_alive():
                    worker.kill()
        
        self.logger.debug("Resource cleanup completed")
    
    def _validate_gpu_ids(self, gpu_ids: List[int]) -> List[int]:
        """Validate and return GPU IDs."""
        if not gpu_ids:
            raise ValueError("gpu_ids cannot be empty")
        
        if not all(isinstance(gpu_id, int) and gpu_id >= 0 for gpu_id in gpu_ids):
            raise ValueError("All GPU IDs must be non-negative integers")
        
        # Remove duplicates while preserving order
        seen = set()
        unique_gpu_ids = []
        for gpu_id in gpu_ids:
            if gpu_id not in seen:
                seen.add(gpu_id)
                unique_gpu_ids.append(gpu_id)
        
        return unique_gpu_ids
    
    def _validate_max_concurrent_runs(self, max_concurrent_runs: int) -> int:
        """Validate and return max_concurrent_runs."""
        if not isinstance(max_concurrent_runs, int) or max_concurrent_runs <= 0:
            raise ValueError("max_concurrent_runs must be a positive integer")
        
        if max_concurrent_runs > len(self.gpu_ids) if hasattr(self, 'gpu_ids') else True:
            # This is just a warning - user might want more concurrent runs than GPUs
            # for benchmarks that don't require much GPU memory
            pass
        
        return max_concurrent_runs
    
    def _setup_logging(self) -> None:
        """Setup logging configuration."""
        self.logger = logging.getLogger(f"{__name__}.BenchmarkExecutor")
        
        if self.verbose and not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def _validate_gpu_setup(self) -> None:
        """Validate GPU availability."""
        try:
            validate_gpu_availability(self.gpu_ids)
            self.logger.info(f"GPU validation passed for devices: {self.gpu_ids}")
        except (RuntimeError, ValueError) as e:
            self.logger.error(f"GPU validation failed: {e}")
            raise
    
    def _validate_matrix_parameters(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig
    ) -> None:
        """Validate benchmark matrix parameters."""
        if not model_names:
            raise ValueError("model_names cannot be empty")
        
        if not sparse_attention_configs:
            raise ValueError("sparse_attention_configs cannot be empty")
        
        if not benchmark_configs:
            raise ValueError("benchmark_configs cannot be empty")
        
        # Validate model names
        for model_name in model_names:
            if not isinstance(model_name, str) or not model_name.strip():
                raise ValueError(f"Invalid model name: {model_name}")
        
        # Validate sparse attention configs
        for sparse_config in sparse_attention_configs:
            if not isinstance(sparse_config, tuple) or len(sparse_config) != 2:
                raise ValueError(f"sparse_attention_configs must be list of (name, config) tuples")
            
            config_name, config_obj = sparse_config
            if not isinstance(config_name, str) or not config_name.strip():
                raise ValueError(f"Sparse config name must be non-empty string: {config_name}")
        
        # Validate benchmark configs
        for benchmark_config in benchmark_configs:
            if not isinstance(benchmark_config, BenchmarkConfig):
                raise ValueError(f"benchmark_configs must contain BenchmarkConfig instances")
            # Additional validation could be added here if needed
        
        # Validate adapter config
        if not isinstance(adapter_config, AdapterConfig):
            raise ValueError("adapter_config must be an AdapterConfig instance")
            # Additional validation could be added here if needed
        
        self.logger.info("Matrix parameter validation passed")

```

```python
"""BenchmarkExecutor: Parallel GPU execution for benchmark matrix experiments.

This module contains the main BenchmarkExecutor class that orchestrates parallel 
execution of benchmark experiments across the model × sparse_attention_config × 
benchmark × subset matrix with dynamic GPU allocation and resumability.
"""

import logging
import multiprocessing
import os
import signal
import sys
import time
import traceback
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from queue import Empty
from contextlib import contextmanager
from sparse_attention_hub.metric_logging.logger import MicroMetricLogger

# Set multiprocessing start method to 'spawn' for CUDA compatibility
if multiprocessing.get_start_method(allow_none=True) != 'spawn':
    multiprocessing.set_start_method('spawn', force=True)

from sparse_attention_hub.sparse_attention.base import SparseAttentionConfig

from .executor_config import (
    AdapterConfig,
    BenchmarkConfig,
    BenchmarkStub,
    ExecutionResults,
    ExecutionProgress,
    BenchmarkResult,
    BenchmarkFailure,
    filter_existing_results,
    generate_benchmark_stubs,

)
from .benchmark_registry import (
    ensure_benchmarks_loaded,
    create_benchmark_instance
)

from .utils.gpu import (
    cleanup_gpu_memory,
    release_gpu_to_pool,
    acquire_gpu_from_pool,
    create_gpu_pool,
    validate_gpu_availability,
)

# Global flag for graceful shutdown
_shutdown_requested = False


@contextmanager
def set_gpu(gpu_id: int):
    """Context manager to set the current CUDA device.
    
    Args:
        gpu_id: The GPU device ID to set as current device
        
    Yields:
        None: The context manager yields control to the with block
    """
    import torch
    torch.cuda.set_device(gpu_id)
    yield


def _signal_handler(signum: int, frame: Any) -> None:
    """Handle shutdown signals gracefully."""
    global _shutdown_requested
    _shutdown_requested = True
    logging.getLogger(__name__).warning(f"Received signal {signum}, initiating graceful shutdown...")


def _categorize_error(error: Exception, context: str = "") -> str:
    """Categorize errors for better handling and reporting.
    
    Args:
        error: The exception that occurred
        context: Additional context about where the error occurred
        
    Returns:
        Error category string for classification
    """
    error_type = type(error).__name__
    error_msg = str(error).lower()
    
    # GPU-related errors
    if any(keyword in error_msg for keyword in ["cuda", "gpu", "out of memory", "oom"]):
        if "out of memory" in error_msg or "oom" in error_msg:
            return "GPU_OUT_OF_MEMORY"
        elif "cuda" in error_msg:
            return "GPU_CUDA_ERROR"
        else:
            return "GPU_ERROR"
    
    # Model loading errors
    elif any(keyword in error_msg for keyword in ["model", "huggingface", "transformers", "pretrained"]):
        if "not found" in error_msg or "doesn't exist" in error_msg:
            return "MODEL_NOT_FOUND"
        elif "tokenizer" in error_msg:
            return "TOKENIZER_ERROR"
        else:
            return "MODEL_LOADING_ERROR"
    
    # Dataset/benchmark errors
    elif any(keyword in error_msg for keyword in ["dataset", "benchmark", "subset"]):
        return "DATASET_ERROR"
    
    # Network/connection errors
    elif any(keyword in error_msg for keyword in ["connection", "timeout", "network", "http"]):
        return "NETWORK_ERROR"
    
    # File system errors
    elif any(keyword in error_msg for keyword in ["file", "directory", "permission", "disk"]):
        return "FILESYSTEM_ERROR"
    
    # Multiprocessing errors
    elif any(keyword in error_msg for keyword in ["queue", "process", "multiprocessing"]):
        return "MULTIPROCESSING_ERROR"
    
    # Default categories
    elif isinstance(error, TimeoutError):
        return "TIMEOUT_ERROR"
    elif isinstance(error, KeyboardInterrupt):
        return "INTERRUPT_ERROR"
    elif isinstance(error, MemoryError):
        return "SYSTEM_MEMORY_ERROR"
    else:
        return "UNKNOWN_ERROR"


def _benchmark_worker(
    stub_queue: multiprocessing.Queue,
    gpu_pool: multiprocessing.Queue,
    result_queue: multiprocessing.Queue,
    error_queue: multiprocessing.Queue,
    timeout_per_benchmark: float = 3600.0,
    max_records: Optional[int] = None,
    sampling_factor: float = 1.0
) -> None:
    """Worker function to execute benchmarks with dynamic GPU allocation.
    
    This function runs in a separate process and handles:
    1. Acquiring GPUs from the shared pool
    2. Creating model adapters with sparse attention configurations
    3. Executing benchmarks with proper error handling
    4. Releasing GPUs back to the pool
    5. Reporting results and errors
    
    Args:
        stub_queue: Queue containing BenchmarkStub instances to process
        gpu_pool: Shared queue containing available GPU IDs
        result_queue: Queue for reporting successful benchmark results
        error_queue: Queue for reporting benchmark failures
        timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
        max_records: Maximum number of metric events to log (None for unlimited)
        sampling_factor: Probability of logging each metric event (0.0-1.0)
    """
    worker_id = os.getpid()
    logger = logging.getLogger(f"{__name__}.worker_{worker_id}")
    
    # Set up signal handlers for graceful shutdown
    signal.signal(signal.SIGTERM, _signal_handler)
    signal.signal(signal.SIGINT, _signal_handler)
    
    # Track resources for cleanup
    current_gpu_id: Optional[int] = None
    adapter = None

    try:
        while True:
            # Check for shutdown signal
            if _shutdown_requested:
                logger.info(f"Worker {worker_id}: Shutdown requested, exiting")
                break
            
            # Step 1: Get next benchmark stub from queue
            try:
                stub: BenchmarkStub = stub_queue.get(timeout=1.0)  # 1 second timeout
            except Empty:
                # No more work to do
                logger.info(f"Worker {worker_id}: No more work, shutting down")
                break
            
            # Check for sentinel value (shutdown signal)
            if stub is None:
                logger.info(f"Worker {worker_id}: Received shutdown signal, exiting")
                break
            
            # Step 2: Acquire GPU from pool
            try:
                current_gpu_id = acquire_gpu_from_pool(gpu_pool, timeout=30.0)
                logger.info(f"Worker {worker_id}: Acquired GPU {current_gpu_id} for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}")
            except Exception as e:
                error_category = _categorize_error(e, "GPU acquisition")
                error_msg = f"Failed to acquire GPU for {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name}: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                error_queue.put(BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=0.0
                ))
                continue

            # Step 3: Run the benchmark on the current GPU
            start_time = time.time()
            execution_time = 0.0  # Initialize execution_time
            
            try:
                with set_gpu(current_gpu_id):
                    logger.debug(f"Worker {worker_id}: Set CUDA device to {current_gpu_id}")
                    execution_success = False

                    # Create model adapter with sparse attention config
                    logger.info(f"Worker {worker_id}: Creating model adapter for {stub.model_name}")
                    
                    # Import here to avoid issues with multiprocessing
                    from sparse_attention_hub.adapters.huggingface import ModelAdapterHF
                    
                    adapter = ModelAdapterHF(
                        model_name=stub.model_name,
                        sparse_attention_config=stub.sparse_attention_config,
                        model_kwargs=stub.adapter_config.model_kwargs,
                        tokenizer_kwargs=stub.adapter_config.tokenizer_kwargs
                    )
                    
                    # Create benchmark instance
                    logger.info(f"Worker {worker_id}: Creating benchmark instance for {stub.benchmark_name}")
                    benchmark = create_benchmark_instance(
                        benchmark_name=stub.benchmark_name,
                        subsets=[stub.subset] if stub.subset else None
                    )
                    
                    # Execute benchmark
                    logger.info(f"Worker {worker_id}: Executing benchmark {stub.benchmark_name} on GPU {current_gpu_id}")
                    metric_logger = MicroMetricLogger()
                    metric_logger.configure_logging(
                        log_path=stub.result_dir, 
                        enabled_metrics=["research_attention_density", "research_attention_output_error"],
                        max_records=max_records,
                        sampling_factor=sampling_factor
                    )
                    metrics = benchmark.run_benchmark(
                        adapter=adapter,
                        result_dir=stub.result_dir,
                        generation_kwargs=stub.generation_kwargs,
                        request_kwargs=stub.request_kwargs
                    )
                    metric_logger.flush()
                    
                    execution_time = time.time() - start_time
                    execution_success = True
                    
                    # Report successful result
                    result = BenchmarkResult(
                        stub=stub,
                        metrics=metrics,
                        execution_time=execution_time,
                        success=True
                    )
                    result_queue.put(result)
                    
                    logger.info(f"Worker {worker_id}: Successfully completed {stub.model_name}/{stub.sparse_config_name}/{stub.benchmark_name} in {execution_time:.2f}s")

            except Exception as e:
                execution_time = time.time() - start_time  # Calculate execution time even on failure
                error_category = _categorize_error(e, "benchmark execution")
                error_msg = f"Benchmark execution failed: {e}"
                logger.error(f"Worker {worker_id}: {error_msg}")
                
                # Log detailed error information for debugging
                logger.debug(f"Worker {worker_id}: Error details: {traceback.format_exc()}")
                
                # Report failure
                failure = BenchmarkFailure(
                    stub=stub,
                    error_message=error_msg,
                    error_type=error_category,
                    execution_time=execution_time
                )
                error_queue.put(failure)
                continue
            
            finally:
                # Step 4: Cleanup and release GPU
                _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
                current_gpu_id = None
                adapter = None
    
    except Exception as worker_error:
        logger.error(f"Worker {worker_id}: Critical worker error: {worker_error}")
        logger.debug(f"Worker {worker_id}: Critical error details: {traceback.format_exc()}")
        
        # Cleanup any remaining resources
        _cleanup_worker_resources(gpu_pool, current_gpu_id, adapter, logger, worker_id)
        
        # Report worker failure to error queue
        error_queue.put({
            "worker_id": worker_id,
            "error_type": "WORKER_CRITICAL_ERROR",
            "error_message": str(worker_error),
            "error_traceback": traceback.format_exc()
        })


def _cleanup_worker_resources(
    gpu_pool: multiprocessing.Queue,
    gpu_id: Optional[int],
    adapter: Any,
    logger: logging.Logger,
    worker_id: int
) -> None:
    """Clean up worker resources including GPU memory and adapter.
    
    Args:
        gpu_pool: GPU pool queue to return GPU to
        gpu_id: GPU ID to release (can be None) - this is the actual GPU ID from the pool
        adapter: Model adapter to cleanup (can be None)
        logger: Logger instance for the worker
        worker_id: Worker process ID for logging
    """
    try:
        # Clean up adapter if it exists
        if adapter is not None:
            try:
                # Force garbage collection of adapter
                del adapter
                import gc
                gc.collect()
                logger.debug(f"Worker {worker_id}: Cleaned up model adapter")
            except Exception as e:
                logger.warning(f"Worker {worker_id}: Failed to cleanup adapter: {e}")
        
        # Clean up GPU memory and release GPU
        if gpu_id is not None:
            try:
                cleanup_gpu_memory(gpu_id)
                logger.debug(f"Worker {worker_id}: Cleaned up GPU memory for GPU {gpu_id}")
            
                # Release GPU back to pool (always use the original GPU ID for the pool)
                release_gpu_to_pool(gpu_pool, gpu_id)
                logger.debug(f"Worker {worker_id}: Released GPU {gpu_id} back to pool")
                
            except Exception as cleanup_error:
                logger.warning(f"Worker {worker_id}: GPU cleanup failed for GPU {gpu_id}: {cleanup_error}")
                # Try to release GPU even if cleanup failed
                try:
                    release_gpu_to_pool(gpu_pool, gpu_id)
                except Exception:
                    logger.error(f"Worker {worker_id}: Failed to release GPU {gpu_id} to pool")
    
    except Exception as e:
        logger.error(f"Worker {worker_id}: Error during resource cleanup: {e}")


class BenchmarkExecutor:
    """Parallel benchmark executor with dynamic GPU allocation and resumability.
    
    Executes benchmark experiments across a model × sparse_attention_config × 
    benchmark × subset matrix with parallel GPU execution and automatic resumability.
    
    Features:
    - Dynamic GPU allocation from specified gpu_ids
    - Subset-level resumability (skip completed experiments)
    - Parallel execution with configurable concurrency
    - Comprehensive error handling and logging
    - Result aggregation and summary generation
    
    Example:
        >>> executor = BenchmarkExecutor(
        ...     gpu_ids=[0, 1, 2, 3],
        ...     max_concurrent_runs=4,
        ...     base_result_dir="./benchmark_results"
        ... )
        >>> 
        >>> results = executor.run_benchmark_matrix(
        ...     model_names=["model1", "model2"],
        ...     sparse_attention_configs=[("dense", None), ("sparse", config)],
        ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
        ...     adapter_config=AdapterConfig()
        ... )
    """
    
    def __init__(
        self,
        gpu_ids: List[int],
        max_concurrent_runs: int,
        base_result_dir: str = "./benchmark_results",
        enable_resumability: bool = True,
        result_file_validation: bool = True,
        required_result_files: Optional[List[str]] = None,
        timeout_per_benchmark: float = 360000.0,  # 100 hours default
        verbose: bool = True,
        max_records: Optional[int] = None,  # Maximum number of metric events to log
        sampling_factor: float = 1.0,  # Probability of logging each metric event (0.0-1.0)
    ):
        """Initialize the BenchmarkExecutor.
        
        Args:
            gpu_ids: List of GPU device IDs to use for parallel execution
            max_concurrent_runs: Maximum number of concurrent benchmark runs
            base_result_dir: Base directory for storing benchmark results
            enable_resumability: Whether to skip already completed experiments
            result_file_validation: Whether to check for specific result files
            required_result_files: List of files required for completion check
            timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
            verbose: Whether to enable verbose logging
            max_records: Maximum number of metric events to log (None for unlimited)
            sampling_factor: Probability of logging each metric event (0.0-1.0)
            
        Raises:
            ValueError: If configuration parameters are invalid
            RuntimeError: If GPU validation fails
        """
        # Validate and store configuration
        self.gpu_ids = self._validate_gpu_ids(gpu_ids)
        self.max_concurrent_runs = self._validate_max_concurrent_runs(max_concurrent_runs)
        self.base_result_dir = Path(base_result_dir).resolve()
        self.enable_resumability = enable_resumability
        self.result_file_validation = result_file_validation
        self.required_result_files = required_result_files or ["raw_results.csv"]
        self.timeout_per_benchmark = timeout_per_benchmark
        self.verbose = verbose
        
        # Metric logging configuration
        self.max_records = max_records
        self.sampling_factor = max(0.0, min(1.0, sampling_factor))  # Clamp to [0.0, 1.0]
        
        # Initialize logging
        self._setup_logging()
        
        # Set up signal handlers for graceful shutdown
        self._setup_signal_handlers()
        
        # Validate GPU availability
        self._validate_gpu_setup()
        
        # Load all available benchmarks
        ensure_benchmarks_loaded()
        

        
        self.logger.info(f"BenchmarkExecutor initialized:")
        self.logger.info(f"  GPU IDs: {self.gpu_ids}")
        self.logger.info(f"  Max concurrent runs: {self.max_concurrent_runs}")
        self.logger.info(f"  Base result directory: {self.base_result_dir}")
        self.logger.info(f"  Resumability: {'enabled' if self.enable_resumability else 'disabled'}")
        self.logger.info(f"  Metric logging - Max records: {self.max_records if self.max_records else 'unlimited'}")
        self.logger.info(f"  Metric logging - Sampling factor: {self.sampling_factor}")
    
    def _setup_signal_handlers(self) -> None:
        """Set up signal handlers for graceful shutdown."""
        signal.signal(signal.SIGTERM, _signal_handler)
        signal.signal(signal.SIGINT, _signal_handler)
        
        # Register cleanup function to run at exit
        import atexit
        atexit.register(self._cleanup_on_exit)
    
    def _cleanup_on_exit(self) -> None:
        """Cleanup function called when the process exits."""
        if hasattr(self, 'logger'):
            self.logger.info("BenchmarkExecutor cleanup on exit")
        
        # Reset global shutdown flag
        global _shutdown_requested
        _shutdown_requested = False

    def run_benchmark_matrix(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig,
        generation_kwargs: Optional[Dict[str, Any]] = None,
        request_kwargs: Optional[Dict[str, Any]] = None
    ) -> ExecutionResults:
        """Execute benchmark matrix with parallel GPU execution.
        
        Runs all combinations of model_names × sparse_attention_configs × 
        benchmark_configs × subsets in parallel across available GPUs.
        
        Args:
            model_names: List of model names to benchmark
            sparse_attention_configs: List of (name, config) tuples for sparse attention
            benchmark_configs: List of benchmark configurations
            adapter_config: Configuration for model adapter
            generation_kwargs: Optional generation parameters
            request_kwargs: Optional request processing parameters
            
        Returns:
            ExecutionResults containing comprehensive execution information
            
        Example:
            >>> results = executor.run_benchmark_matrix(
            ...     model_names=["meta-llama/Llama-3.1-8B-Instruct"],
            ...     sparse_attention_configs=[("dense", None)],
            ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
            ...     adapter_config=AdapterConfig()
            ... )
            >>> print(f"Completed: {results.completed_count}/{results.total_count}")
        """
        # Initialize execution
        self.logger.info("Starting benchmark matrix execution")
        
        try:
            # Step 1: Validate input parameters
            self._validate_matrix_parameters(
                model_names, sparse_attention_configs, benchmark_configs, adapter_config
            )
            
            # Step 2: Generate benchmark stubs (matrix expansion)
            self.logger.info("Generating benchmark matrix...")
            all_stubs = generate_benchmark_stubs(
                model_names=model_names,
                sparse_attention_configs=sparse_attention_configs,
                benchmark_configs=benchmark_configs,
                adapter_config=adapter_config,
                generation_kwargs=generation_kwargs,
                request_kwargs=request_kwargs,
                base_result_dir=str(self.base_result_dir)
            )
            
            total_combinations = len(all_stubs)
            self.logger.info(f"Generated {total_combinations} benchmark combinations:")
            self.logger.info(f"  {len(model_names)} models × {len(sparse_attention_configs)} sparse configs × "
                           f"{sum(len(b.subsets) if b.subsets else 1 for b in benchmark_configs)} benchmark-subsets")
            
            # Step 3: Filter for resumability (if enabled)
            if self.enable_resumability:
                self.logger.info("Checking for existing results (resumability)...")
                pending_stubs, completed_stubs = filter_existing_results(
                    all_stubs,
                    check_result_files=self.result_file_validation,
                    required_files=self.required_result_files,
                    verbose=self.verbose
                )
                
                self.logger.info(f"Resumability check: {len(completed_stubs)} completed, {len(pending_stubs)} pending")
            else:
                pending_stubs = all_stubs
                completed_stubs = []
                self.logger.info("Resumability disabled - will execute all combinations")
            
            # Step 4: Early exit if nothing to do
            if not pending_stubs:
                self.logger.info("No pending benchmarks to execute. All experiments already completed!")
                
                execution_summary = {
                    "total_combinations": total_combinations,
                    "models": model_names,
                    "sparse_configs": [name for name, _ in sparse_attention_configs],
                    "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs]
                }
                
                progress = ExecutionProgress(
                    total_stubs=total_combinations,
                    skipped_stubs=len(completed_stubs),
                    completed_stubs=0,
                    failed_stubs=0
                )
                
                return ExecutionResults(
                    execution_summary=execution_summary,
                    individual_results=[],
                    failed_executions=[],
                    execution_time=0.0,
                    progress=progress
                )
            

            
            # Step 6: Execute pending benchmarks with worker processes
            self.logger.info(f"Executing {len(pending_stubs)} pending benchmarks with {self.max_concurrent_runs} workers...")
            
            start_time = time.time()
            results = self._execute_with_workers(pending_stubs)
            execution_time = time.time() - start_time
            
            # Step 7: Aggregate results
            execution_summary = {
                "total_combinations": total_combinations,
                "models": model_names,
                "sparse_configs": [name for name, _ in sparse_attention_configs],
                "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs],
                "execution_time_seconds": execution_time,
                "gpu_ids_used": self.gpu_ids,
                "max_concurrent_runs": self.max_concurrent_runs
            }
            
            progress = ExecutionProgress(
                total_stubs=total_combinations,
                skipped_stubs=len(completed_stubs),
                completed_stubs=len(results.individual_results),
                failed_stubs=len(results.failed_executions)
            )
            
            final_results = ExecutionResults(
                execution_summary=execution_summary,
                individual_results=results.individual_results,
                failed_executions=results.failed_executions,
                execution_time=execution_time,
                progress=progress
            )
            
            self.logger.info(f"Benchmark matrix execution completed:")
            self.logger.info(f"  Total time: {execution_time:.2f}s")
            self.logger.info(f"  Successful: {len(results.individual_results)}")
            self.logger.info(f"  Failed: {len(results.failed_executions)}")
            self.logger.info(f"  Skipped: {len(completed_stubs)}")
            
            return final_results
            
        except Exception as e:
            self.logger.error(f"Benchmark execution failed: {e}")
            raise
    
    def _execute_with_workers(self, pending_stubs: List[BenchmarkStub]) -> ExecutionResults:
        """Execute benchmark stubs using worker processes.
        
        This method implements comprehensive process pool management with:
        - Dynamic work distribution through shared queues
        - Graceful process startup and shutdown
        - Real-time result collection and error handling
        - Process health monitoring and recovery
        
        Args:
            pending_stubs: List of benchmark stubs to execute
            
        Returns:
            ExecutionResults containing all results and failures
        """
        self.logger.info(f"Starting execution with {len(pending_stubs)} stubs using {self.max_concurrent_runs} workers")
        
        # Step 1: Create and configure shared queues
        stub_queue = multiprocessing.Queue(maxsize=len(pending_stubs) + 100)  # Buffer for queue operations
        gpu_pool = create_gpu_pool(self.gpu_ids)
        result_queue = multiprocessing.Queue()
        error_queue = multiprocessing.Queue()
        
        # Step 2: Populate work queue with all stubs
        self.logger.info("Populating work queue...")
        for stub in pending_stubs:
            stub_queue.put(stub)
        
        # Add sentinel values to signal workers to stop
        for _ in range(self.max_concurrent_runs):
            stub_queue.put(None)  # Sentinel value
        
        # Step 3: Create and start worker processes
        workers = []
        worker_pids = []
        
        self.logger.info(f"Starting {self.max_concurrent_runs} worker processes...")
        for i in range(self.max_concurrent_runs):
            worker = multiprocessing.Process(
                target=_benchmark_worker,
                args=(stub_queue, gpu_pool, result_queue, error_queue, self.timeout_per_benchmark, self.max_records, self.sampling_factor),
                name=f"benchmark_worker_{i}"
            )
            worker.daemon = True  # Ensure workers are terminated when main process exits
            worker.start()
            workers.append(worker)
            worker_pids.append(worker.pid)
            self.logger.debug(f"Started worker {i+1}/{self.max_concurrent_runs} (PID: {worker.pid})")
        
        self.logger.info(f"All {self.max_concurrent_runs} workers started successfully")
        
        # Step 4: Monitor and collect results in real-time
        individual_results: List[BenchmarkResult] = []
        failed_executions: List[BenchmarkFailure] = []
        active_workers = set(worker_pids)
        
        self.logger.info("Monitoring worker processes and collecting results...")
        
        try:
            # Monitor workers and collect results until all work is complete
            while active_workers:
                # Check for completed workers
                completed_workers = []
                for worker in workers:
                    if worker.is_alive():
                        # Worker is still running, check for results
                        pass
                    else:
                        # Worker has completed
                        if worker.pid in active_workers:
                            active_workers.remove(worker.pid)
                            completed_workers.append(worker)
                            
                            if worker.exitcode != 0:
                                self.logger.warning(f"Worker {worker.pid} exited with code {worker.exitcode}")
                            else:
                                self.logger.debug(f"Worker {worker.pid} completed successfully")
                
                # Collect any available results (non-blocking)
                self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
                
                # Log progress periodically
                if len(individual_results) + len(failed_executions) > 0 and (len(individual_results) + len(failed_executions)) % 5 == 0:
                    self.logger.info(f"Progress: {len(individual_results)} completed, {len(failed_executions)} failed, {len(active_workers)} workers active")
                
                # Small delay to prevent busy waiting
                time.sleep(0.1)
            
            # Step 5: Final result collection
            self.logger.info("All workers completed, collecting final results...")
            self._collect_available_results(result_queue, error_queue, individual_results, failed_executions)
            
        except KeyboardInterrupt:
            self.logger.warning("Received interrupt signal, initiating graceful shutdown...")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        except Exception as e:
            error_category = _categorize_error(e, "main execution")
            self.logger.error(f"Error during execution ({error_category}): {e}")
            self.logger.debug(f"Execution error details: {traceback.format_exc()}")
            self._graceful_shutdown(workers, stub_queue, gpu_pool, result_queue, error_queue)
            raise
        
        finally:
            # Step 6: Cleanup resources
            self._cleanup_resources(workers, stub_queue, gpu_pool, result_queue, error_queue)
        
        # Step 7: Final summary
        total_processed = len(individual_results) + len(failed_executions)
        self.logger.info(f"Execution completed: {len(individual_results)} successful, {len(failed_executions)} failed")
        self.logger.info(f"Success rate: {len(individual_results)/total_processed*100:.1f}%" if total_processed > 0 else "No work processed")
        
        return ExecutionResults(
            execution_summary={},  # Will be filled by caller
            individual_results=individual_results,
            failed_executions=failed_executions,
            execution_time=0.0,  # Will be filled by caller
            progress=ExecutionProgress(
                total_stubs=len(pending_stubs),
                completed_stubs=len(individual_results),
                failed_stubs=len(failed_executions)
            )
        )
    
    def _collect_available_results(
        self,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue,
        individual_results: List[BenchmarkResult],
        failed_executions: List[BenchmarkFailure]
    ) -> None:
        """Collect all available results from queues without blocking.
        
        Args:
            result_queue: Queue containing successful benchmark results
            error_queue: Queue containing benchmark failures
            individual_results: List to append successful results to
            failed_executions: List to append failures to
        """
        # Collect all available successful results
        while True:
            try:
                result = result_queue.get_nowait()
                individual_results.append(result)
            except Empty:
                break
        
        # Collect all available errors
        while True:
            try:
                error = error_queue.get_nowait()
                if isinstance(error, BenchmarkFailure):
                    failed_executions.append(error)
                else:
                    # Handle worker-level errors
                    self.logger.error(f"Worker error: {error}")
            except Empty:
                break
    
    def _graceful_shutdown(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Perform graceful shutdown of worker processes and queues.
        
        Args:
            workers: List of worker processes to terminate
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.info("Initiating graceful shutdown...")
        
        try:
            # Signal workers to stop by adding sentinel values
            sentinel_count = 0
            for _ in range(len(workers)):
                try:
                    stub_queue.put_nowait(None)
                    sentinel_count += 1
                except Exception as e:
                    self.logger.debug(f"Could not add sentinel to queue: {e}")
                    break  # Queue is full, stop adding sentinels
            
            self.logger.info(f"Added {sentinel_count} shutdown signals to work queue")
            
            # Wait for workers to finish with timeout
            self.logger.info("Waiting for workers to finish...")
            for i, worker in enumerate(workers):
                try:
                    worker.join(timeout=30.0)  # 30 second timeout per worker
                    if worker.is_alive():
                        self.logger.warning(f"Force terminating worker {worker.pid}")
                        worker.terminate()
                        worker.join(timeout=5.0)
                        if worker.is_alive():
                            self.logger.error(f"Failed to terminate worker {worker.pid}, killing process")
                            worker.kill()
                            worker.join(timeout=2.0)
                    else:
                        self.logger.debug(f"Worker {worker.pid} terminated successfully")
                except Exception as e:
                    self.logger.error(f"Error during worker {worker.pid} shutdown: {e}")
            
            self.logger.info("Graceful shutdown completed")
            
        except Exception as e:
            self.logger.error(f"Error during graceful shutdown: {e}")
            # Force cleanup of remaining workers
            for worker in workers:
                if worker.is_alive():
                    try:
                        worker.kill()
                    except Exception:
                        pass
    
    def _cleanup_resources(
        self,
        workers: List[multiprocessing.Process],
        stub_queue: multiprocessing.Queue,
        gpu_pool: multiprocessing.Queue,
        result_queue: multiprocessing.Queue,
        error_queue: multiprocessing.Queue
    ) -> None:
        """Clean up all multiprocessing resources.
        
        Args:
            workers: List of worker processes
            stub_queue: Work queue to close
            gpu_pool: GPU pool queue to close
            result_queue: Result queue to close
            error_queue: Error queue to close
        """
        self.logger.debug("Cleaning up multiprocessing resources...")
        
        # Close all queues
        try:
            stub_queue.close()
            gpu_pool.close()
            result_queue.close()
            error_queue.close()
        except Exception as e:
            self.logger.warning(f"Error closing queues: {e}")
        
        # Join any remaining workers
        for worker in workers:
            if worker.is_alive():
                self.logger.warning(f"Force terminating remaining worker {worker.pid}")
                worker.terminate()
                worker.join(timeout=5.0)
                if worker.is_alive():
                    worker.kill()
        
        self.logger.debug("Resource cleanup completed")
    
    def _validate_gpu_ids(self, gpu_ids: List[int]) -> List[int]:
        """Validate and return GPU IDs."""
        if not gpu_ids:
            raise ValueError("gpu_ids cannot be empty")
        
        if not all(isinstance(gpu_id, int) and gpu_id >= 0 for gpu_id in gpu_ids):
            raise ValueError("All GPU IDs must be non-negative integers")
        
        # Remove duplicates while preserving order
        seen = set()
        unique_gpu_ids = []
        for gpu_id in gpu_ids:
            if gpu_id not in seen:
                seen.add(gpu_id)
                unique_gpu_ids.append(gpu_id)
        
        return unique_gpu_ids
    
    def _validate_max_concurrent_runs(self, max_concurrent_runs: int) -> int:
        """Validate and return max_concurrent_runs."""
        if not isinstance(max_concurrent_runs, int) or max_concurrent_runs <= 0:
            raise ValueError("max_concurrent_runs must be a positive integer")
        
        if max_concurrent_runs > len(self.gpu_ids) if hasattr(self, 'gpu_ids') else True:
            # This is just a warning - user might want more concurrent runs than GPUs
            # for benchmarks that don't require much GPU memory
            pass
        
        return max_concurrent_runs
    
    def _setup_logging(self) -> None:
        """Setup logging configuration."""
        self.logger = logging.getLogger(f"{__name__}.BenchmarkExecutor")
        
        if self.verbose and not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def _validate_gpu_setup(self) -> None:
        """Validate GPU availability."""
        try:
            validate_gpu_availability(self.gpu_ids)
            self.logger.info(f"GPU validation passed for devices: {self.gpu_ids}")
        except (RuntimeError, ValueError) as e:
            self.logger.error(f"GPU validation failed: {e}")
            raise
    
    def _validate_matrix_parameters(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig
    ) -> None:
        """Validate benchmark matrix parameters."""
        if not model_names:
            raise ValueError("model_names cannot be empty")
        
        if not sparse_attention_configs:
            raise ValueError("sparse_attention_configs cannot be empty")
        
        if not benchmark_configs:
            raise ValueError("benchmark_configs cannot be empty")
        
        # Validate model names
        for model_name in model_names:
            if not isinstance(model_name, str) or not model_name.strip():
                raise ValueError(f"Invalid model name: {model_name}")
        
        # Validate sparse attention configs
        for sparse_config in sparse_attention_configs:
            if not isinstance(sparse_config, tuple) or len(sparse_config) != 2:
                raise ValueError(f"sparse_attention_configs must be list of (name, config) tuples")
            
            config_name, config_obj = sparse_config
            if not isinstance(config_name, str) or not config_name.strip():
                raise ValueError(f"Sparse config name must be non-empty string: {config_name}")
        
        # Validate benchmark configs
        for benchmark_config in benchmark_configs:
            if not isinstance(benchmark_config, BenchmarkConfig):
                raise ValueError(f"benchmark_configs must contain BenchmarkConfig instances")
            # Additional validation could be added here if needed
        
        # Validate adapter config
        if not isinstance(adapter_config, AdapterConfig):
            raise ValueError("adapter_config must be an AdapterConfig instance")
            # Additional validation could be added here if needed
        
        self.logger.info("Matrix parameter validation passed")

```

```python
#!/usr/bin/env python3
"""Demo script showing how to use the new MicroMetricLogger parameters in BenchmarkExecutor."""

from benchmark.executor import BenchmarkExecutor

def demo_executor_with_metrics():
    """Demonstrate the new metric logging parameters in BenchmarkExecutor."""
    
    print("=== Demo: BenchmarkExecutor with Metric Logging Parameters ===")
    
    # Example 1: Default settings (unlimited records, 100% sampling)
    print("\n1. Creating executor with default metric settings:")
    executor1 = BenchmarkExecutor(
        gpu_ids=[0],
        max_concurrent_runs=1,
        base_result_dir="./demo_results_1",
        verbose=False  # Reduce log noise for demo
    )
    
    # Example 2: Limited records
    print("\n2. Creating executor with max_records=1000:")
    executor2 = BenchmarkExecutor(
        gpu_ids=[0], 
        max_concurrent_runs=1,
        base_result_dir="./demo_results_2",
        max_records=1000,  # Limit to 1000 metric events
        verbose=False
    )
    
    # Example 3: Sampling factor
    print("\n3. Creating executor with sampling_factor=0.1 (10% sampling):")
    executor3 = BenchmarkExecutor(
        gpu_ids=[0],
        max_concurrent_runs=1, 
        base_result_dir="./demo_results_3",
        sampling_factor=0.1,  # Only log 10% of events
        verbose=False
    )
    
    # Example 4: Both limits
    print("\n4. Creating executor with both max_records=500 and sampling_factor=0.5:")
    executor4 = BenchmarkExecutor(
        gpu_ids=[0],
        max_concurrent_runs=1,
        base_result_dir="./demo_results_4", 
        max_records=500,      # Max 500 events
        sampling_factor=0.5,  # 50% sampling
        verbose=False
    )
    
    print("\n=== Parameter Summary ===")
    executors = [
        ("Default", executor1),
        ("Max Records", executor2), 
        ("Sampling", executor3),
        ("Combined", executor4)
    ]
    
    for name, executor in executors:
        print(f"\n{name} Executor:")
        print(f"  max_records: {executor.max_records if executor.max_records else 'unlimited'}")
        print(f"  sampling_factor: {executor.sampling_factor}")
        
    print("\n=== Demo Complete ===")
    print("These parameters will be automatically passed to MicroMetricLogger")
    print("when running benchmarks through these executors.")

if __name__ == "__main__":
    demo_executor_with_metrics()
```

## ✅ Successfully Propagated MicroMetricLogger Parameters from BenchmarkExecutor

I've successfully implemented the requested changes to make the `max_records` and `sampling_factor` parameters propagate from the `BenchmarkExecutor` object instantiation down to the `MicroMetricLogger` configuration.

## 🔧 **Changes Made:**

### **1. Updated BenchmarkExecutor `__init__` method:**
```python
def __init__(
    self,
    # ... existing parameters ...
    max_records: Optional[int] = None,  # Maximum number of metric events to log
    sampling_factor: float = 1.0,  # Probability of logging each metric event (0.0-1.0)
):
```

### **2. Added parameter documentation:**
- Updated the docstring to describe the new parameters
- Added parameter validation (sampling_factor is clamped to [0.0, 1.0])
- Stored as instance variables: `self.max_records` and `self.sampling_factor`

### **3. Updated worker function signature:**
```python
def _benchmark_worker(
    # ... existing parameters ...
    max_records: Optional[int] = None,
    sampling_factor: float = 1.0
) -> None:
```

### **4. Updated worker process creation:**
```python
worker = multiprocessing.Process(
    target=_benchmark_worker,
    args=(stub_queue, gpu_pool, result_queue, error_queue, 
          self.timeout_per_benchmark, self.max_records, self.sampling_factor),
    name=f"benchmark_worker_{i}"
)
```

### **5. Updated MicroMetricLogger configuration:**
```python
metric_logger.configure_logging(
    log_path=stub.result_dir, 
    enabled_metrics=["research_attention_density", "research_attention_output_error"],
    max_records=max_records,        # Now uses executor parameter
    sampling_factor=sampling_factor # Now uses executor parameter
)
```

### **6. Added initialization logging:**
```python
self.logger.info(f"  Metric logging - Max records: {self.max_records if self.max_records else 'unlimited'}")
self.logger.info(f"  Metric logging - Sampling factor: {self.sampling_factor}")
```

## 🎯 **Usage Examples:**

### **Default Behavior (No Limits):**
```python
executor = BenchmarkExecutor(
    gpu_ids=[0, 1],
    max_concurrent_runs=2,
    base_result_dir="./results"
    # max_records=None (unlimited), sampling_factor=1.0 (100%)
)
```

### **With Record Limit:**
```python
executor = BenchmarkExecutor(
    gpu_ids=[0, 1],
    max_concurrent_runs=2,
    base_result_dir="./results",
    max_records=5000  # Limit to 5000 events per benchmark
)
```

### **With Sampling:**
```python
executor = BenchmarkExecutor(
    gpu_ids=[0, 1], 
    max_concurrent_runs=2,
    base_result_dir="./results",
    sampling_factor=0.1  # Log only 10% of events
)
```

### **Combined Limits:**
```python
executor = BenchmarkExecutor(
    gpu_ids=[0, 1],
    max_concurrent_runs=2,
    base_result_dir="./results", 
    max_records=1000,     # Max 1000 events
    sampling_factor=0.5   # 50% sampling probability
)
```

## 🔄 **Parameter Flow:**

1. **User sets parameters** in `BenchmarkExecutor()` instantiation
2. **Executor stores** parameters as instance variables
3. **Parameters passed** to worker processes via multiprocessing args
4. **Worker processes** use parameters in `MicroMetricLogger.configure_logging()`
5. **All benchmarks** executed by that executor use the same metric logging configuration

## ✨ **Benefits:**

- **Centralized Configuration**: Set metric limits once at the executor level
- **Consistent Behavior**: All benchmarks use the same metric logging settings
- **Resource Control**: Prevent metric logging from consuming too much disk space or CPU
- **Flexible Sampling**: Reduce metric overhead for large-scale experiments
- **Backward Compatible**: Default values maintain existing behavior

The parameters now properly propagate from the executor instantiation down to the actual metric logging, giving you fine-grained control over metric collection behavior across all your benchmark runs!

```python
def __init__(
    self,
    # ... existing parameters ...
    max_records: Optional[int] = None,  # Maximum number of metric events to log
    sampling_factor: float = 1.0,  # Probability of logging each metric event (0.0-1.0)
):
```

```python
def _benchmark_worker(
    # ... existing parameters ...
    max_records: Optional[int] = None,
    sampling_factor: float = 1.0
) -> None:
```

```python
worker = multiprocessing.Process(
    target=_benchmark_worker,
    args=(stub_queue, gpu_pool, result_queue, error_queue, 
          self.timeout_per_benchmark, self.max_records, self.sampling_factor),
    name=f"benchmark_worker_{i}"
)
```

```python
metric_logger.configure_logging(
    log_path=stub.result_dir, 
    enabled_metrics=["research_attention_density", "research_attention_output_error"],
    max_records=max_records,        # Now uses executor parameter
    sampling_factor=sampling_factor # Now uses executor parameter
)
```

```python
self.logger.info(f"  Metric logging - Max records: {self.max_records if self.max_records else 'unlimited'}")
self.logger.info(f"  Metric logging - Sampling factor: {self.sampling_factor}")
```

```python
executor = BenchmarkExecutor(
    gpu_ids=[0, 1],
    max_concurrent_runs=2,
    base_result_dir="./results"
    # max_records=None (unlimited), sampling_factor=1.0 (100%)
)
```

```python
executor = BenchmarkExecutor(
    gpu_ids=[0, 1],
    max_concurrent_runs=2,
    base_result_dir="./results",
    max_records=5000  # Limit to 5000 events per benchmark
)
```

```python
executor = BenchmarkExecutor(
    gpu_ids=[0, 1], 
    max_concurrent_runs=2,
    base_result_dir="./results",
    sampling_factor=0.1  # Log only 10% of events
)
```

```python
executor = BenchmarkExecutor(
    gpu_ids=[0, 1],
    max_concurrent_runs=2,
    base_result_dir="./results", 
    max_records=1000,     # Max 1000 events
    sampling_factor=0.5   # 50% sampling probability
)
```

