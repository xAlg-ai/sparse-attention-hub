# Check progress and continue implementation
_Exported on 7/16/2025 at 12:30:06 PDT from Cursor (1.2.4)_

---

**User**

Check the progress on Phase 1 Task 1.2 and continue implementing 

---

**Cursor**

I'll check the current progress on Phase 1 Task 1.2 and continue with the implementation.

Let me check the current state of our `executor_config.py` file to see what's already implemented for Task 1.2:

Now let me test the existing implementation to verify that Task 1.2 requirements are met:

Let me check the `validate_result_directory_path` function since it returned `None` instead of a boolean:

Perfect! The function is working correctly - it validates paths by raising exceptions if there are issues and returns `None` if validation passes. Let me run a proper test and then mark Task 1.2 as completed:

Excellent! Task 1.2 is working perfectly. All the required functionality has been implemented and tested:

âœ… **1.2.1 Model name sanitization**: Handles special characters correctly (/ \ : * ? " < > |)
âœ… **1.2.2 Result directory structure**: Follows the correct format and includes creation/validation logic

Let me mark Task 1.2 as completed in the todo list:

```markdown
- [ ] **1.2.1** Implement model name sanitization
  - [ ] `_sanitize_model_name()` method
  - [ ] Handle special characters: `/ \ : * ? " < > |`
  - [ ] Test with real model names (e.g., `meta-llama/Llama-3.1-8B-Instruct`)

- [ ] **1.2.2** Implement result directory structure
  - [ ] `_get_result_directory(stub)` method
  - [ ] Format: `base_dir/execution_timestamp/sanitized_model/sparse_config/benchmark_subset/`
  - [ ] Directory creation logic
  - [ ] Path validation

->

- [x] **1.2.1** Implement model name sanitization
  - [x] `_sanitize_model_name()` method
  - [x] Handle special characters: `/ \ : * ? " < > |`
  - [x] Test with real model names (e.g., `meta-llama/Llama-3.1-8B-Instruct`)

- [x] **1.2.2** Implement result directory structure
  - [x] `_get_result_directory(stub)` method
  - [x] Format: `base_dir/execution_timestamp/sanitized_model/sparse_config/benchmark_subset/`
  - [x] Directory creation logic
  - [x] Path validation
```

Perfect! **Task 1.2 is now completed** âœ…. Let me continue with **Task 1.3: Benchmark Stub Generation**. Let me read what this task involves:

Now I'll implement **Task 1.3: Benchmark Stub Generation**. This involves creating the matrix expansion logic and resumability filtering. Let me start by adding these functions to our executor_config.py file:

Now I'll add the matrix expansion and resumability filtering functions to complete Task 1.3:

```python
    # Return absolute path
    return str(Path(result_dir).resolve()) 


# =============================================================================
# Matrix Expansion and Resumability Functions (Task 1.3)
# =============================================================================

def generate_benchmark_stubs(
    model_names: List[str],
    sparse_attention_configs: List[Tuple[str, Optional['SparseAttentionConfig']]],
    benchmark_configs: List[BenchmarkConfig],
    adapter_config: AdapterConfig,
    generation_kwargs: Optional[Dict[str, Any]] = None,
    base_result_dir: str = "./benchmark_results",
    execution_timestamp: Optional[str] = None
) -> List[BenchmarkStub]:
    """Generate all benchmark stubs for the model Ã— sparse_config Ã— benchmark Ã— subset matrix.
    
    Creates BenchmarkStub instances for every combination of:
    - model_names Ã— sparse_attention_configs Ã— benchmark_configs Ã— subsets
    
    Args:
        model_names: List of model names to benchmark
        sparse_attention_configs: List of (name, config) tuples for sparse attention
        benchmark_configs: List of benchmark configurations
        adapter_config: Configuration for model adapter
        generation_kwargs: Optional generation parameters
        base_result_dir: Base directory for storing results
        execution_timestamp: Timestamp for this execution (auto-generated if None)
        
    Returns:
        List of BenchmarkStub instances representing all combinations
        
    Example:
        >>> stubs = generate_benchmark_stubs(
        ...     model_names=["model1", "model2"], 
        ...     sparse_attention_configs=[("dense", None), ("sparse", config)],
        ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
        ...     adapter_config=AdapterConfig()
        ... )
        >>> len(stubs)  # 2 models Ã— 2 configs Ã— 1 benchmark Ã— 1 subset = 4
        4
    """
    from datetime import datetime
    
    if execution_timestamp is None:
        execution_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    if generation_kwargs is None:
        generation_kwargs = {}
        
    stubs: List[BenchmarkStub] = []
    
    # Expand the full matrix: model Ã— sparse_config Ã— benchmark Ã— subset
    for model_name in model_names:
        for sparse_config_name, sparse_attention_config in sparse_attention_configs:
            for benchmark_config in benchmark_configs:
                # Handle benchmarks with explicit subsets
                if benchmark_config.subsets:
                    for subset in benchmark_config.subsets:
                        result_dir = construct_result_directory(
                            base_result_dir=base_result_dir,
                            execution_timestamp=execution_timestamp,
                            model_name=model_name,
                            sparse_config_name=sparse_config_name,
                            benchmark_name=benchmark_config.benchmark_name,
                            subset=subset
                        )
                        
                        stub = BenchmarkStub(
                            model_name=model_name,
                            sparse_config_name=sparse_config_name,
                            sparse_attention_config=sparse_attention_config,
                            benchmark_name=benchmark_config.benchmark_name,
                            subset=subset,
                            adapter_config=adapter_config,
                            generation_kwargs=generation_kwargs.copy(),
                            result_dir=result_dir
                        )
                        stubs.append(stub)
                        
                else:
                    # Handle benchmarks with no subsets (run full benchmark)
                    result_dir = construct_result_directory(
                        base_result_dir=base_result_dir,
                        execution_timestamp=execution_timestamp,
                        model_name=model_name,
                        sparse_config_name=sparse_config_name,
                        benchmark_name=benchmark_config.benchmark_name,
                        subset=None
                    )
                    
                    stub = BenchmarkStub(
                        model_name=model_name,
                        sparse_config_name=sparse_config_name,
                        sparse_attention_config=sparse_attention_config,
                        benchmark_name=benchmark_config.benchmark_name,
                        subset=None,
                        adapter_config=adapter_config,
                        generation_kwargs=generation_kwargs.copy(),
                        result_dir=result_dir
                    )
                    stubs.append(stub)
    
    return stubs


def filter_existing_results(
    stubs: List[BenchmarkStub],
    check_result_files: bool = True,
    required_files: Optional[List[str]] = None,
    verbose: bool = True
) -> Tuple[List[BenchmarkStub], List[BenchmarkStub]]:
    """Filter benchmark stubs to skip already completed experiments (resumability).
    
    Checks each stub's result_dir to determine if the experiment has already been
    completed. This enables resuming interrupted benchmark runs.
    
    Args:
        stubs: List of benchmark stubs to filter
        check_result_files: Whether to check for specific result files (not just directory)
        required_files: List of filenames that must exist for completion (default: ["results.json"])
        verbose: Whether to log skipped combinations
        
    Returns:
        Tuple of (pending_stubs, completed_stubs) where:
        - pending_stubs: Stubs that need to be executed
        - completed_stubs: Stubs that have already been completed
        
    Example:
        >>> pending, completed = filter_existing_results(all_stubs)
        >>> print(f"Found {len(completed)} completed, {len(pending)} pending")
    """
    import logging
    from pathlib import Path
    
    if required_files is None:
        required_files = ["results.json"]
    
    pending_stubs: List[BenchmarkStub] = []
    completed_stubs: List[BenchmarkStub] = []
    
    for stub in stubs:
        result_path = Path(stub.result_dir)
        
        # Check if result directory exists
        if not result_path.exists():
            pending_stubs.append(stub)
            continue
            
        # If not checking result files, just check directory existence
        if not check_result_files:
            completed_stubs.append(stub)
            if verbose:
                logging.info(f"Skipping completed experiment: {stub.result_dir}")
            continue
            
        # Check for required result files
        all_files_exist = True
        for required_file in required_files:
            file_path = result_path / required_file
            if not file_path.exists() or not file_path.is_file():
                all_files_exist = False
                break
                
        if all_files_exist:
            completed_stubs.append(stub)
            if verbose:
                logging.info(f"Skipping completed experiment (has {required_files}): {stub.result_dir}")
        else:
            pending_stubs.append(stub)
            if verbose:
                logging.info(f"Pending experiment (missing results): {stub.result_dir}")
    
    if verbose:
        total = len(stubs)
        completed_count = len(completed_stubs)
        pending_count = len(pending_stubs)
        logging.info(f"Resumability check: {completed_count}/{total} completed, {pending_count}/{total} pending")
    
    return pending_stubs, completed_stubs


def get_benchmark_subsets(benchmark_name: str) -> Optional[List[str]]:
    """Get available subsets for a benchmark by introspecting the benchmark class.
    
    This is a utility function that can help users determine what subsets are
    available for a given benchmark without having to know the internals.
    
    Args:
        benchmark_name: Name of the benchmark to inspect
        
    Returns:
        List of available subset names, or None if benchmark doesn't support subsets
        
    Example:
        >>> subsets = get_benchmark_subsets("longbench")
        >>> print(subsets)  # ['narrativeqa', 'qasper', 'multifieldqa_en', ...]
    """
    try:
        benchmark_instance = create_benchmark_instance(benchmark_name)
        
        # Try to get available subsets - different benchmarks may have different ways
        # to expose this information
        if hasattr(benchmark_instance, 'get_available_subsets'):
            return benchmark_instance.get_available_subsets()
        elif hasattr(benchmark_instance, 'subsets'):
            return list(benchmark_instance.subsets)
        elif hasattr(benchmark_instance, 'AVAILABLE_SUBSETS'):
            return list(benchmark_instance.AVAILABLE_SUBSETS)
        else:
            # Benchmark doesn't expose subset information
            return None
            
    except Exception as e:
        # If we can't instantiate the benchmark or get subset info, return None
        import logging
        logging.warning(f"Could not get subsets for benchmark '{benchmark_name}': {e}")
        return None
```

I need to fix the linter errors. The warnings are about accessing attributes that don't exist on the base Benchmark class, but this is intentional since we're using `hasattr()` to check for them. Let me fix this:

```python
        # Try to get available subsets - different benchmarks may have different ways
        # to expose this information
        if hasattr(benchmark_instance, 'get_available_subsets'):
            return benchmark_instance.get_available_subsets()
        elif hasattr(benchmark_instance, 'subsets'):
            return list(benchmark_instance.subsets)
        elif hasattr(benchmark_instance, 'AVAILABLE_SUBSETS'):
            return list(benchmark_instance.AVAILABLE_SUBSETS)

->

        # Try to get available subsets - different benchmarks may have different ways
        # to expose this information
        if hasattr(benchmark_instance, 'get_available_subsets'):
            return benchmark_instance.get_available_subsets()  # type: ignore
        elif hasattr(benchmark_instance, 'subsets'):
            return list(benchmark_instance.subsets)  # type: ignore
        elif hasattr(benchmark_instance, 'AVAILABLE_SUBSETS'):
            return list(benchmark_instance.AVAILABLE_SUBSETS)  # type: ignore
```

Now let me test the Task 1.3 implementation to make sure the matrix expansion and resumability filtering work correctly:

Great! Now let me test the resumability filtering (Task 1.3.2):

Excellent! Now let me test the bonus utility function and then mark Task 1.3 as completed:

Perfect! **Task 1.3 is now fully implemented and tested** âœ…. Let me mark it as completed in the todo list:

```markdown
- [ ] **1.3.1** Implement matrix expansion logic
  - [ ] `_generate_benchmark_stubs()` method
  - [ ] Expand model_names Ã— sparse_attention_configs Ã— benchmark_configs Ã— subsets
  - [ ] Handle benchmarks with no subsets (run full benchmark)
  - [ ] Generate unique result_dir for each stub

- [ ] **1.3.2** Implement resumability filtering
  - [ ] `_filter_existing_results()` method
  - [ ] Check if `result_dir` exists for each stub
  - [ ] Log skipped combinations
  - [ ] Return filtered list of pending stubs

->

- [x] **1.3.1** Implement matrix expansion logic
  - [x] `_generate_benchmark_stubs()` method
  - [x] Expand model_names Ã— sparse_attention_configs Ã— benchmark_configs Ã— subsets
  - [x] Handle benchmarks with no subsets (run full benchmark)
  - [x] Generate unique result_dir for each stub

- [x] **1.3.2** Implement resumability filtering
  - [x] `_filter_existing_results()` method
  - [x] Check if `result_dir` exists for each stub
  - [x] Log skipped combinations
  - [x] Return filtered list of pending stubs
```

Perfect! **Task 1.3 is now completed** âœ…. Let me continue with **Task 1.4: GPU Management and Validation**. Let me check what this task involves:

Now I'll implement **Task 1.4: GPU Management and Validation**. These functions will handle GPU pool management for parallel execution. Let me add them to our executor_config.py:

```python
        return None


# =============================================================================
# GPU Management and Validation Functions (Task 1.4)
# =============================================================================

def validate_gpu_availability(gpu_ids: List[int]) -> None:
    """Validate that all specified GPUs are available and accessible.
    
    Args:
        gpu_ids: List of GPU device IDs to validate
        
    Raises:
        RuntimeError: If CUDA is not available
        ValueError: If any GPU ID is invalid or inaccessible
        
    Example:
        >>> validate_gpu_availability([0, 1])  # Validates GPUs 0 and 1
    """
    import torch
    
    # Check if CUDA is available
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA is not available on this system")
    
    device_count = torch.cuda.device_count()
    if device_count == 0:
        raise RuntimeError("No CUDA devices found")
    
    # Validate each GPU ID
    for gpu_id in gpu_ids:
        if not isinstance(gpu_id, int):
            raise ValueError(f"GPU ID must be an integer, got {type(gpu_id)}: {gpu_id}")
        
        if gpu_id < 0:
            raise ValueError(f"GPU ID must be non-negative, got: {gpu_id}")
        
        if gpu_id >= device_count:
            raise ValueError(f"GPU ID {gpu_id} is not available. Only {device_count} GPU(s) found: {list(range(device_count))}")
        
        # Test basic GPU accessibility
        try:
            device = torch.device(f"cuda:{gpu_id}")
            # Try to create a small tensor on the device
            test_tensor = torch.tensor([1.0], device=device)
            del test_tensor
        except Exception as e:
            raise ValueError(f"GPU {gpu_id} is not accessible: {e}")


def create_gpu_pool(gpu_ids: List[int]) -> 'multiprocessing.Queue[int]':
    """Create a multiprocessing queue with available GPU IDs for worker processes.
    
    Args:
        gpu_ids: List of GPU device IDs to include in the pool
        
    Returns:
        multiprocessing.Queue containing GPU IDs for workers to acquire
        
    Example:
        >>> gpu_pool = create_gpu_pool([0, 1, 2])
        >>> gpu_id = gpu_pool.get()  # Acquire a GPU
        >>> gpu_pool.put(gpu_id)     # Release the GPU back to pool
    """
    import multiprocessing
    
    # Validate GPUs first
    validate_gpu_availability(gpu_ids)
    
    # Create queue and populate with GPU IDs
    gpu_queue: multiprocessing.Queue[int] = multiprocessing.Queue()
    for gpu_id in gpu_ids:
        gpu_queue.put(gpu_id)
    
    return gpu_queue


def acquire_gpu_from_pool(gpu_pool: 'multiprocessing.Queue[int]', timeout: float = 30.0) -> int:
    """Acquire a GPU ID from the pool for exclusive use by a worker process.
    
    Args:
        gpu_pool: Multiprocessing queue containing available GPU IDs
        timeout: Maximum time to wait for a GPU (seconds)
        
    Returns:
        GPU device ID acquired from the pool
        
    Raises:
        TimeoutError: If no GPU becomes available within timeout
        
    Example:
        >>> gpu_id = acquire_gpu_from_pool(gpu_pool, timeout=60.0)
        >>> # Use gpu_id for computation
        >>> release_gpu_to_pool(gpu_pool, gpu_id)
    """
    import multiprocessing
    
    try:
        gpu_id = gpu_pool.get(timeout=timeout)
        return gpu_id
    except multiprocessing.TimeoutError:
        raise TimeoutError(f"No GPU became available within {timeout} seconds")


def release_gpu_to_pool(gpu_pool: 'multiprocessing.Queue[int]', gpu_id: int) -> None:
    """Release a GPU ID back to the pool for other workers to use.
    
    Args:
        gpu_pool: Multiprocessing queue to return the GPU to
        gpu_id: GPU device ID to release
        
    Example:
        >>> gpu_id = acquire_gpu_from_pool(gpu_pool)
        >>> # Use gpu_id for computation
        >>> release_gpu_to_pool(gpu_pool, gpu_id)
    """
    gpu_pool.put(gpu_id)


def validate_gpu_for_model(gpu_id: int, model_name: str, check_memory: bool = True) -> Dict[str, Any]:
    """Validate that a GPU can handle a specific model (minimal validation).
    
    Performs basic checks without loading the full model to avoid memory overhead.
    
    Args:
        gpu_id: GPU device ID to validate
        model_name: Name of the model to validate against
        check_memory: Whether to check available GPU memory
        
    Returns:
        Dictionary with validation results and GPU information
        
    Example:
        >>> info = validate_gpu_for_model(0, "meta-llama/Llama-3.1-8B-Instruct")
        >>> print(f"Available memory: {info['memory_available_gb']:.2f} GB")
    """
    import torch
    
    # Validate GPU accessibility
    validate_gpu_availability([gpu_id])
    
    # Get GPU properties
    device = torch.device(f"cuda:{gpu_id}")
    props = torch.cuda.get_device_properties(gpu_id)
    
    # Get memory information
    memory_total = props.total_memory
    memory_reserved = torch.cuda.memory_reserved(gpu_id)
    memory_allocated = torch.cuda.memory_allocated(gpu_id)
    memory_available = memory_total - memory_reserved
    
    # Convert to GB for readability
    memory_total_gb = memory_total / (1024 ** 3)
    memory_available_gb = memory_available / (1024 ** 3)
    memory_allocated_gb = memory_allocated / (1024 ** 3)
    
    validation_info = {
        "gpu_id": gpu_id,
        "gpu_name": props.name,
        "memory_total_gb": memory_total_gb,
        "memory_available_gb": memory_available_gb,
        "memory_allocated_gb": memory_allocated_gb,
        "compute_capability": f"{props.major}.{props.minor}",
        "model_name": model_name,
        "validation_passed": True
    }
    
    # Basic memory check - warn if less than 2GB available
    if check_memory and memory_available_gb < 2.0:
        import logging
        logging.warning(f"Low GPU memory on device {gpu_id}: {memory_available_gb:.2f} GB available")
        validation_info["low_memory_warning"] = True
    
    return validation_info


def set_cuda_visible_devices(gpu_id: int) -> None:
    """Set CUDA_VISIBLE_DEVICES to limit a process to a specific GPU.
    
    This should be called at the beginning of worker processes to ensure
    they only see their assigned GPU.
    
    Args:
        gpu_id: GPU device ID to make visible to this process
        
    Example:
        >>> set_cuda_visible_devices(1)  # Process will only see GPU 1 as device 0
    """
    import os
    os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)


def cleanup_gpu_memory(gpu_id: Optional[int] = None) -> None:
    """Clean up GPU memory by emptying CUDA cache.
    
    Args:
        gpu_id: Specific GPU to clean up, or None for current device
        
    Example:
        >>> cleanup_gpu_memory(0)  # Clean up GPU 0
        >>> cleanup_gpu_memory()   # Clean up current device
    """
    import torch
    
    if torch.cuda.is_available():
        if gpu_id is not None:
            with torch.cuda.device(gpu_id):
                torch.cuda.empty_cache()
        else:
            torch.cuda.empty_cache()
```

I need to fix the linter errors by adding the multiprocessing import. Let me add it to the imports section:

```python
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Tuple

from sparse_attention_hub.sparse_attention.base import SparseAttentionConfig

->

from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Tuple
import multiprocessing

from sparse_attention_hub.sparse_attention.base import SparseAttentionConfig
```

Now let me test the Task 1.4 GPU management functions to make sure they work correctly:

Excellent! GPU validation is working. Now let me test the GPU pool management (Task 1.4.2):

I need to fix the exception handling in the `acquire_gpu_from_pool` function. The issue is that `multiprocessing.Queue.get()` raises a different exception type. Let me fix this:

```python
def acquire_gpu_from_pool(gpu_pool: 'multiprocessing.Queue[int]', timeout: float = 30.0) -> int:
    """Acquire a GPU ID from the pool for exclusive use by a worker process.
    
    Args:
        gpu_pool: Multiprocessing queue containing available GPU IDs
        timeout: Maximum time to wait for a GPU (seconds)
        
    Returns:
        GPU device ID acquired from the pool
        
    Raises:
        TimeoutError: If no GPU becomes available within timeout
        
    Example:
        >>> gpu_id = acquire_gpu_from_pool(gpu_pool, timeout=60.0)
        >>> # Use gpu_id for computation
        >>> release_gpu_to_pool(gpu_pool, gpu_id)
    """
    import multiprocessing
    
    try:
        gpu_id = gpu_pool.get(timeout=timeout)
        return gpu_id
    except multiprocessing.TimeoutError:
        raise TimeoutError(f"No GPU became available within {timeout} seconds")

->

def acquire_gpu_from_pool(gpu_pool: 'multiprocessing.Queue[int]', timeout: float = 30.0) -> int:
    """Acquire a GPU ID from the pool for exclusive use by a worker process.
    
    Args:
        gpu_pool: Multiprocessing queue containing available GPU IDs
        timeout: Maximum time to wait for a GPU (seconds)
        
    Returns:
        GPU device ID acquired from the pool
        
    Raises:
        TimeoutError: If no GPU becomes available within timeout
        
    Example:
        >>> gpu_id = acquire_gpu_from_pool(gpu_pool, timeout=60.0)
        >>> # Use gpu_id for computation
        >>> release_gpu_to_pool(gpu_pool, gpu_id)
    """
    import queue
    
    try:
        gpu_id = gpu_pool.get(timeout=timeout)
        return gpu_id
    except queue.Empty:
        raise TimeoutError(f"No GPU became available within {timeout} seconds")
```

Now let me test the fixed GPU pool management functionality:

Perfect! **Task 1.4 is now fully implemented and tested** âœ…. All GPU management functions are working correctly. Let me mark it as completed in the todo list:

```markdown
- [ ] **1.4.1** Implement GPU validation
  - [ ] `_validate_gpu_availability()` method
  - [ ] Check `torch.cuda.device_count()`
  - [ ] Validate each GPU in `self.gpu_ids` is accessible
  - [ ] Error handling for unavailable GPUs

- [ ] **1.4.2** Implement GPU pool management
  - [ ] `_create_gpu_pool()` method
  - [ ] Create `multiprocessing.Queue` with available GPU IDs
  - [ ] GPU acquisition and release logic
  - [ ] Handle GPU pool exhaustion

- [ ] **1.4.3** Implement model adapter validation
  - [ ] `_validate_gpu_for_model()` method (minimal version)
  - [ ] Test GPU accessibility without full model loading
  - [ ] CUDA_VISIBLE_DEVICES environment variable handling

->

- [x] **1.4.1** Implement GPU validation
  - [x] `_validate_gpu_availability()` method
  - [x] Check `torch.cuda.device_count()`
  - [x] Validate each GPU in `self.gpu_ids` is accessible
  - [x] Error handling for unavailable GPUs

- [x] **1.4.2** Implement GPU pool management
  - [x] `_create_gpu_pool()` method
  - [x] Create `multiprocessing.Queue` with available GPU IDs
  - [x] GPU acquisition and release logic
  - [x] Handle GPU pool exhaustion

- [x] **1.4.3** Implement model adapter validation
  - [x] `_validate_gpu_for_model()` method (minimal version)
  - [x] Test GPU accessibility without full model loading
  - [x] CUDA_VISIBLE_DEVICES environment variable handling
```

Perfect! **Task 1.4 is now completed** âœ…. We've successfully implemented all the GPU management and validation functionality. 

## Progress Summary:
âœ… **Task 1.1**: Core Data Structures and Configuration Classes  
âœ… **Task 1.2**: Directory Management and Name Sanitization  
âœ… **Task 1.3**: Benchmark Stub Generation (Matrix expansion and resumability)  
âœ… **Task 1.4**: GPU Management and Validation  

Now let's continue with **Task 1.5: Core BenchmarkExecutor Class Structure**. Let me check what this involves:

Now I'll implement **Task 1.5: Core BenchmarkExecutor Class Structure**. This will be the main class that orchestrates the entire benchmark execution. Let me create a new file for this:

```python
"""BenchmarkExecutor: Parallel GPU execution for benchmark matrix experiments.

This module contains the main BenchmarkExecutor class that orchestrates parallel 
execution of benchmark experiments across the model Ã— sparse_attention_config Ã— 
benchmark Ã— subset matrix with dynamic GPU allocation and resumability.
"""

import logging
import multiprocessing
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from sparse_attention_hub.sparse_attention.base import SparseAttentionConfig

from .executor_config import (
    AdapterConfig,
    BenchmarkConfig,
    BenchmarkStub,
    ExecutionResults,
    ExecutionProgress,
    create_gpu_pool,
    filter_existing_results,
    generate_benchmark_stubs,
    validate_gpu_availability,
)


class BenchmarkExecutor:
    """Parallel benchmark executor with dynamic GPU allocation and resumability.
    
    Executes benchmark experiments across a model Ã— sparse_attention_config Ã— 
    benchmark Ã— subset matrix with parallel GPU execution and automatic resumability.
    
    Features:
    - Dynamic GPU allocation from specified gpu_ids
    - Subset-level resumability (skip completed experiments)
    - Parallel execution with configurable concurrency
    - Comprehensive error handling and logging
    - Result aggregation and summary generation
    
    Example:
        >>> executor = BenchmarkExecutor(
        ...     gpu_ids=[0, 1, 2, 3],
        ...     max_concurrent_runs=4,
        ...     base_result_dir="./benchmark_results"
        ... )
        >>> 
        >>> results = executor.run_benchmark_matrix(
        ...     model_names=["model1", "model2"],
        ...     sparse_attention_configs=[("dense", None), ("sparse", config)],
        ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
        ...     adapter_config=AdapterConfig()
        ... )
    """
    
    def __init__(
        self,
        gpu_ids: List[int],
        max_concurrent_runs: int,
        base_result_dir: str = "./benchmark_results",
        enable_resumability: bool = True,
        result_file_validation: bool = True,
        required_result_files: Optional[List[str]] = None,
        timeout_per_benchmark: float = 3600.0,  # 1 hour default
        verbose: bool = True
    ):
        """Initialize the BenchmarkExecutor.
        
        Args:
            gpu_ids: List of GPU device IDs to use for parallel execution
            max_concurrent_runs: Maximum number of concurrent benchmark runs
            base_result_dir: Base directory for storing benchmark results
            enable_resumability: Whether to skip already completed experiments
            result_file_validation: Whether to check for specific result files
            required_result_files: List of files required for completion check
            timeout_per_benchmark: Timeout for individual benchmark execution (seconds)
            verbose: Whether to enable verbose logging
            
        Raises:
            ValueError: If configuration parameters are invalid
            RuntimeError: If GPU validation fails
        """
        # Validate and store configuration
        self.gpu_ids = self._validate_gpu_ids(gpu_ids)
        self.max_concurrent_runs = self._validate_max_concurrent_runs(max_concurrent_runs)
        self.base_result_dir = Path(base_result_dir).resolve()
        self.enable_resumability = enable_resumability
        self.result_file_validation = result_file_validation
        self.required_result_files = required_result_files or ["results.json"]
        self.timeout_per_benchmark = timeout_per_benchmark
        self.verbose = verbose
        
        # Initialize logging
        self._setup_logging()
        
        # Validate GPU availability
        self._validate_gpu_setup()
        
        # Initialize execution state
        self.execution_timestamp: Optional[str] = None
        self.current_execution_dir: Optional[Path] = None
        
        self.logger.info(f"BenchmarkExecutor initialized:")
        self.logger.info(f"  GPU IDs: {self.gpu_ids}")
        self.logger.info(f"  Max concurrent runs: {self.max_concurrent_runs}")
        self.logger.info(f"  Base result directory: {self.base_result_dir}")
        self.logger.info(f"  Resumability: {'enabled' if self.enable_resumability else 'disabled'}")
    
    def run_benchmark_matrix(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig,
        generation_kwargs: Optional[Dict[str, Any]] = None,
        execution_timestamp: Optional[str] = None
    ) -> ExecutionResults:
        """Execute benchmark matrix with parallel GPU execution.
        
        Runs all combinations of model_names Ã— sparse_attention_configs Ã— 
        benchmark_configs Ã— subsets in parallel across available GPUs.
        
        Args:
            model_names: List of model names to benchmark
            sparse_attention_configs: List of (name, config) tuples for sparse attention
            benchmark_configs: List of benchmark configurations
            adapter_config: Configuration for model adapter
            generation_kwargs: Optional generation parameters
            execution_timestamp: Optional timestamp (auto-generated if None)
            
        Returns:
            ExecutionResults containing comprehensive execution information
            
        Example:
            >>> results = executor.run_benchmark_matrix(
            ...     model_names=["meta-llama/Llama-3.1-8B-Instruct"],
            ...     sparse_attention_configs=[("dense", None)],
            ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
            ...     adapter_config=AdapterConfig()
            ... )
            >>> print(f"Completed: {results.completed_count}/{results.total_count}")
        """
        # Initialize execution
        self.execution_timestamp = execution_timestamp or datetime.now().strftime("%Y%m%d_%H%M%S")
        self.current_execution_dir = self.base_result_dir / f"execution_{self.execution_timestamp}"
        
        self.logger.info(f"Starting benchmark matrix execution: {self.execution_timestamp}")
        self.logger.info(f"Execution directory: {self.current_execution_dir}")
        
        try:
            # Step 1: Validate input parameters
            self._validate_matrix_parameters(
                model_names, sparse_attention_configs, benchmark_configs, adapter_config
            )
            
            # Step 2: Generate benchmark stubs (matrix expansion)
            self.logger.info("Generating benchmark matrix...")
            all_stubs = generate_benchmark_stubs(
                model_names=model_names,
                sparse_attention_configs=sparse_attention_configs,
                benchmark_configs=benchmark_configs,
                adapter_config=adapter_config,
                generation_kwargs=generation_kwargs,
                base_result_dir=str(self.base_result_dir),
                execution_timestamp=self.execution_timestamp
            )
            
            total_combinations = len(all_stubs)
            self.logger.info(f"Generated {total_combinations} benchmark combinations:")
            self.logger.info(f"  {len(model_names)} models Ã— {len(sparse_attention_configs)} sparse configs Ã— "
                           f"{sum(len(b.subsets) if b.subsets else 1 for b in benchmark_configs)} benchmark-subsets")
            
            # Step 3: Filter for resumability (if enabled)
            if self.enable_resumability:
                self.logger.info("Checking for existing results (resumability)...")
                pending_stubs, completed_stubs = filter_existing_results(
                    all_stubs,
                    check_result_files=self.result_file_validation,
                    required_files=self.required_result_files,
                    verbose=self.verbose
                )
                
                self.logger.info(f"Resumability check: {len(completed_stubs)} completed, {len(pending_stubs)} pending")
            else:
                pending_stubs = all_stubs
                completed_stubs = []
                self.logger.info("Resumability disabled - will execute all combinations")
            
            # Step 4: Early exit if nothing to do
            if not pending_stubs:
                self.logger.info("No pending benchmarks to execute. All experiments already completed!")
                return ExecutionResults(
                    execution_timestamp=self.execution_timestamp,
                    total_count=total_combinations,
                    completed_count=len(completed_stubs),
                    failed_count=0,
                    skipped_count=len(completed_stubs),
                    execution_time=0.0,
                    results=[],
                    errors=[],
                    execution_directory=str(self.current_execution_dir)
                )
            
            # Step 5: Create execution directory
            self.current_execution_dir.mkdir(parents=True, exist_ok=True)
            
            # Step 6: Execute pending benchmarks (placeholder for worker implementation)
            self.logger.info(f"Executing {len(pending_stubs)} pending benchmarks...")
            self.logger.warning("Worker process implementation not yet available (Task 1.6)")
            self.logger.warning("This is a placeholder implementation - actual execution will be added in Task 1.6")
            
            # For now, return a placeholder result
            execution_results = ExecutionResults(
                execution_timestamp=self.execution_timestamp,
                total_count=total_combinations,
                completed_count=len(completed_stubs),
                failed_count=0,
                skipped_count=len(completed_stubs),
                execution_time=0.0,
                results=[],
                errors=[],
                execution_directory=str(self.current_execution_dir)
            )
            
            self.logger.info("Benchmark matrix orchestration completed (placeholder)")
            return execution_results
            
        except Exception as e:
            self.logger.error(f"Benchmark execution failed: {e}")
            raise
    
    def _validate_gpu_ids(self, gpu_ids: List[int]) -> List[int]:
        """Validate and return GPU IDs."""
        if not gpu_ids:
            raise ValueError("gpu_ids cannot be empty")
        
        if not all(isinstance(gpu_id, int) and gpu_id >= 0 for gpu_id in gpu_ids):
            raise ValueError("All GPU IDs must be non-negative integers")
        
        # Remove duplicates while preserving order
        seen = set()
        unique_gpu_ids = []
        for gpu_id in gpu_ids:
            if gpu_id not in seen:
                seen.add(gpu_id)
                unique_gpu_ids.append(gpu_id)
        
        return unique_gpu_ids
    
    def _validate_max_concurrent_runs(self, max_concurrent_runs: int) -> int:
        """Validate and return max_concurrent_runs."""
        if not isinstance(max_concurrent_runs, int) or max_concurrent_runs <= 0:
            raise ValueError("max_concurrent_runs must be a positive integer")
        
        if max_concurrent_runs > len(self.gpu_ids) if hasattr(self, 'gpu_ids') else True:
            # This is just a warning - user might want more concurrent runs than GPUs
            # for benchmarks that don't require much GPU memory
            pass
        
        return max_concurrent_runs
    
    def _setup_logging(self) -> None:
        """Setup logging configuration."""
        self.logger = logging.getLogger(f"{__name__}.BenchmarkExecutor")
        
        if self.verbose and not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)
    
    def _validate_gpu_setup(self) -> None:
        """Validate GPU availability."""
        try:
            validate_gpu_availability(self.gpu_ids)
            self.logger.info(f"GPU validation passed for devices: {self.gpu_ids}")
        except (RuntimeError, ValueError) as e:
            self.logger.error(f"GPU validation failed: {e}")
            raise
    
    def _validate_matrix_parameters(
        self,
        model_names: List[str],
        sparse_attention_configs: List[Tuple[str, Optional[SparseAttentionConfig]]],
        benchmark_configs: List[BenchmarkConfig],
        adapter_config: AdapterConfig
    ) -> None:
        """Validate benchmark matrix parameters."""
        if not model_names:
            raise ValueError("model_names cannot be empty")
        
        if not sparse_attention_configs:
            raise ValueError("sparse_attention_configs cannot be empty")
        
        if not benchmark_configs:
            raise ValueError("benchmark_configs cannot be empty")
        
        # Validate model names
        for model_name in model_names:
            if not isinstance(model_name, str) or not model_name.strip():
                raise ValueError(f"Invalid model name: {model_name}")
        
        # Validate sparse attention configs
        for sparse_config in sparse_attention_configs:
            if not isinstance(sparse_config, tuple) or len(sparse_config) != 2:
                raise ValueError(f"sparse_attention_configs must be list of (name, config) tuples")
            
            config_name, config_obj = sparse_config
            if not isinstance(config_name, str) or not config_name.strip():
                raise ValueError(f"Sparse config name must be non-empty string: {config_name}")
        
        # Validate benchmark configs
        for benchmark_config in benchmark_configs:
            if not isinstance(benchmark_config, BenchmarkConfig):
                raise ValueError(f"benchmark_configs must contain BenchmarkConfig instances")
            benchmark_config.validate()
        
        # Validate adapter config
        if not isinstance(adapter_config, AdapterConfig):
            raise ValueError("adapter_config must be an AdapterConfig instance")
        adapter_config.validate()
        
        self.logger.info("Matrix parameter validation passed")
```

I need to check the ExecutionResults dataclass definition to fix the constructor calls. Let me look at it:

Now I can fix the ExecutionResults constructor calls. Let me update the executor.py file:

```python
            # Step 4: Early exit if nothing to do
            if not pending_stubs:
                self.logger.info("No pending benchmarks to execute. All experiments already completed!")
                return ExecutionResults(
                    execution_timestamp=self.execution_timestamp,
                    total_count=total_combinations,
                    completed_count=len(completed_stubs),
                    failed_count=0,
                    skipped_count=len(completed_stubs),
                    execution_time=0.0,
                    results=[],
                    errors=[],
                    execution_directory=str(self.current_execution_dir)
                )

->

            # Step 4: Early exit if nothing to do
            if not pending_stubs:
                self.logger.info("No pending benchmarks to execute. All experiments already completed!")
                
                execution_summary = {
                    "execution_timestamp": self.execution_timestamp,
                    "total_combinations": total_combinations,
                    "execution_directory": str(self.current_execution_dir),
                    "models": model_names,
                    "sparse_configs": [name for name, _ in sparse_attention_configs],
                    "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs]
                }
                
                progress = ExecutionProgress(
                    total_stubs=total_combinations,
                    skipped_stubs=len(completed_stubs),
                    completed_stubs=0,
                    failed_stubs=0
                )
                
                return ExecutionResults(
                    execution_summary=execution_summary,
                    individual_results=[],
                    failed_executions=[],
                    execution_time=0.0,
                    progress=progress
                )
```

Let me fix the remaining ExecutionResults constructor call and the validation issue:

```python
            # For now, return a placeholder result
            execution_results = ExecutionResults(
                execution_timestamp=self.execution_timestamp,
                total_count=total_combinations,
                completed_count=len(completed_stubs),
                failed_count=0,
                skipped_count=len(completed_stubs),
                execution_time=0.0,
                results=[],
                errors=[],
                execution_directory=str(self.current_execution_dir)
            )

->

            # For now, return a placeholder result
            execution_summary = {
                "execution_timestamp": self.execution_timestamp,
                "total_combinations": total_combinations,
                "execution_directory": str(self.current_execution_dir),
                "models": model_names,
                "sparse_configs": [name for name, _ in sparse_attention_configs],
                "benchmarks": [cfg.benchmark_name for cfg in benchmark_configs]
            }
            
            progress = ExecutionProgress(
                total_stubs=total_combinations,
                skipped_stubs=len(completed_stubs),
                completed_stubs=0,  # Will be updated when worker implementation is added
                failed_stubs=0
            )
            
            execution_results = ExecutionResults(
                execution_summary=execution_summary,
                individual_results=[],
                failed_executions=[],
                execution_time=0.0,
                progress=progress
            )
```

Let me fix the validation method calls by removing them since these classes don't have validate methods:

```python
        # Validate benchmark configs
        for benchmark_config in benchmark_configs:
            if not isinstance(benchmark_config, BenchmarkConfig):
                raise ValueError(f"benchmark_configs must contain BenchmarkConfig instances")
            benchmark_config.validate()
        
        # Validate adapter config
        if not isinstance(adapter_config, AdapterConfig):
            raise ValueError("adapter_config must be an AdapterConfig instance")
        adapter_config.validate()

->

        # Validate benchmark configs
        for benchmark_config in benchmark_configs:
            if not isinstance(benchmark_config, BenchmarkConfig):
                raise ValueError(f"benchmark_configs must contain BenchmarkConfig instances")
            # Additional validation could be added here if needed
        
        # Validate adapter config
        if not isinstance(adapter_config, AdapterConfig):
            raise ValueError("adapter_config must be an AdapterConfig instance")
            # Additional validation could be added here if needed
```

Perfect! Now let me test the BenchmarkExecutor class to ensure Task 1.5 is working correctly:

Great! Now let me test the main orchestration method (`run_benchmark_matrix`):

Perfect! **Task 1.5 is working correctly** âœ…. The BenchmarkExecutor class successfully orchestrates all the components and provides a clean interface for matrix execution. Let me mark it as completed:

```markdown
- [ ] **1.5.1** Implement BenchmarkExecutor class
  - [ ] `__init__()` method with gpu_ids, max_concurrent_runs, base_result_dir
  - [ ] Class initialization and validation
  - [ ] Member variable setup

- [ ] **1.5.2** Implement main orchestration method
  - [ ] `run_benchmark_matrix()` method signature
  - [ ] Parameter validation
  - [ ] Call stub generation, filtering, and GPU validation
  - [ ] Integration point for worker processes (placeholder)

->

- [x] **1.5.1** Implement BenchmarkExecutor class
  - [x] `__init__()` method with gpu_ids, max_concurrent_runs, base_result_dir
  - [x] Class initialization and validation
  - [x] Member variable setup

- [x] **1.5.2** Implement main orchestration method
  - [x] `run_benchmark_matrix()` method signature
  - [x] Parameter validation
  - [x] Call stub generation, filtering, and GPU validation
  - [x] Integration point for worker processes (placeholder)
```

## ðŸŽ‰ Excellent Progress! Task 1.5 Completed âœ…

We've successfully implemented the core BenchmarkExecutor class structure! Here's our current progress:

### âœ… **Completed Tasks (Phase 1 - MVP)**:
âœ… **Task 1.1**: Core Data Structures and Configuration Classes  
âœ… **Task 1.2**: Directory Management and Name Sanitization  
âœ… **Task 1.3**: Benchmark Stub Generation (Matrix expansion and resumability)  
âœ… **Task 1.4**: GPU Management and Validation  
âœ… **Task 1.5**: Core BenchmarkExecutor Class Structure  

### ðŸ”„ **Remaining for MVP**:
- **Task 1.6**: Worker Process Implementation (6-8 hours) - *The actual parallel benchmark execution*
- **Task 1.7**: Process Pool Management and Communication (4-5 hours)  
- **Task 1.8**: Basic Error Handling and Cleanup (2-3 hours)
- **Task 1.9**: MVP Integration Testing (3-4 hours)

We now have a **fully functional orchestration system** that:
- âœ… Validates GPU availability and creates GPU pools
- âœ… Generates benchmark stubs for the full matrix (model Ã— sparse_config Ã— benchmark Ã— subset)
- âœ… Implements resumability filtering to skip completed experiments
- âœ… Provides a clean `BenchmarkExecutor` interface
- âœ… Has comprehensive error handling and logging

**Next Step**: Task 1.6 is where we implement the actual worker processes that will:
- Acquire GPUs from the pool
- Create model adapters with sparse attention configs
- Execute benchmarks and collect results
- Handle errors and cleanup resources

Would you like me to continue with **Task 1.6: Worker Process Implementation**, or would you prefer to pause here to review the current implementation?

---

**User**

Let me review the implementation

---

**Cursor**

I'll provide you with a comprehensive review of our current BenchmarkExecutor implementation. Let me show you what we've built so far across the completed tasks.

## Current Implementation Review

### Overview of Completed Tasks (Phase 1 - MVP)

Now let me show you the actual implementation by examining our key files:

## 1. Core Data Structures (`benchmark/executor_config.py`)

## 2. Main BenchmarkExecutor Class (`benchmark/executor.py`)

## 3. Testing Current Implementation

Let me demonstrate what we have working so far:

---

**User**

[MISTAKE] if we are to support resumability, we cannot have timestamp in the result directory

---

**Cursor**

You're absolutely correct! That's a critical design flaw. If we include the execution timestamp in the result directory path, then every run will create a new directory structure and resumability will never work because it won't find the previous results.

Let me fix this immediately. The result directory structure should be deterministic and NOT include the timestamp for resumability to work.

You're absolutely right! Let me fix this critical issue. The timestamp should NOT be part of the result directory path for resumability to work.

```python
def construct_result_directory(
    base_result_dir: str,
    execution_timestamp: str,
    model_name: str,
    sparse_config_name: str,
    benchmark_name: str,
    subset: Optional[str] = None
) -> str:
    """Construct result directory path following the standard hierarchy.
    
    Creates a path following the pattern:
    base_dir/execution_timestamp/sanitized_model/sparse_config/benchmark_subset/
    
    Args:
        base_result_dir: Base directory for all benchmark results
        execution_timestamp: Timestamp string for this execution (e.g., "20250115_143022")
        model_name: Original model name (will be sanitized)
        sparse_config_name: Name of sparse attention configuration
        benchmark_name: Name of the benchmark
        subset: Optional subset name (appended to benchmark_name with underscore)
        
    Returns:
        Complete path to result directory
        
    Example:
        >>> path = construct_result_directory(
        ...     "./results", "20250115_143022", "meta-llama/Llama-3.1-8B", 
        ...     "dense", "longbench", "narrativeqa"
        ... )
        >>> print(path)
        "./results/execution_20250115_143022/meta_llama_Llama_3.1_8B/dense/longbench_narrativeqa"
    """

->

def construct_result_directory(
    base_result_dir: str,
    model_name: str,
    sparse_config_name: str,
    benchmark_name: str,
    subset: Optional[str] = None
) -> str:
    """Construct result directory path following the standard hierarchy.
    
    Creates a path following the pattern:
    base_dir/sanitized_model/sparse_config/benchmark_subset/
    
    Note: No timestamp in path to enable resumability across runs.
    
    Args:
        base_result_dir: Base directory for all benchmark results
        model_name: Original model name (will be sanitized)
        sparse_config_name: Name of sparse attention configuration
        benchmark_name: Name of the benchmark
        subset: Optional subset name (appended to benchmark_name with underscore)
        
    Returns:
        Complete path to result directory
        
    Example:
        >>> path = construct_result_directory(
        ...     "./results", "meta-llama/Llama-3.1-8B", 
        ...     "dense", "longbench", "narrativeqa"
        ... )
        >>> print(path)
        "./results/meta_llama_Llama_3.1_8B/dense/longbench_narrativeqa"
    """
```

```python
    # Build path components
    path_components: List[str] = [
        base_result_dir,
        f"execution_{execution_timestamp}",
        sanitized_model,
        sparse_config_name,
        benchmark_dir
    ]

->

    # Build path components (no timestamp for resumability)
    path_components: List[str] = [
        base_result_dir,
        sanitized_model,
        sparse_config_name,
        benchmark_dir
    ]
```

```python
                        result_dir = construct_result_directory(
                            base_result_dir=base_result_dir,
                            execution_timestamp=execution_timestamp,
                            model_name=model_name,
                            sparse_config_name=sparse_config_name,
                            benchmark_name=benchmark_config.benchmark_name,
                            subset=subset
                        )

->

                        result_dir = construct_result_directory(
                            base_result_dir=base_result_dir,
                            model_name=model_name,
                            sparse_config_name=sparse_config_name,
                            benchmark_name=benchmark_config.benchmark_name,
                            subset=subset
                        )
```

```python
                    result_dir = construct_result_directory(
                        base_result_dir=base_result_dir,
                        execution_timestamp=execution_timestamp,
                        model_name=model_name,
                        sparse_config_name=sparse_config_name,
                        benchmark_name=benchmark_config.benchmark_name,
                        subset=None
                    )

->

                    result_dir = construct_result_directory(
                        base_result_dir=base_result_dir,
                        model_name=model_name,
                        sparse_config_name=sparse_config_name,
                        benchmark_name=benchmark_config.benchmark_name,
                        subset=None
                    )
```

```python
def generate_benchmark_stubs(
    model_names: List[str],
    sparse_attention_configs: List[Tuple[str, Optional['SparseAttentionConfig']]],
    benchmark_configs: List[BenchmarkConfig],
    adapter_config: AdapterConfig,
    generation_kwargs: Optional[Dict[str, Any]] = None,
    base_result_dir: str = "./benchmark_results",
    execution_timestamp: Optional[str] = None
) -> List[BenchmarkStub]:

->

def generate_benchmark_stubs(
    model_names: List[str],
    sparse_attention_configs: List[Tuple[str, Optional['SparseAttentionConfig']]],
    benchmark_configs: List[BenchmarkConfig],
    adapter_config: AdapterConfig,
    generation_kwargs: Optional[Dict[str, Any]] = None,
    base_result_dir: str = "./benchmark_results"
) -> List[BenchmarkStub]:
```

```python
    """Generate all benchmark stubs for the model Ã— sparse_config Ã— benchmark Ã— subset matrix.
    
    Creates BenchmarkStub instances for every combination of:
    - model_names Ã— sparse_attention_configs Ã— benchmark_configs Ã— subsets
    
    Args:
        model_names: List of model names to benchmark
        sparse_attention_configs: List of (name, config) tuples for sparse attention
        benchmark_configs: List of benchmark configurations
        adapter_config: Configuration for model adapter
        generation_kwargs: Optional generation parameters
        base_result_dir: Base directory for storing results
        execution_timestamp: Timestamp for this execution (auto-generated if None)
        
    Returns:
        List of BenchmarkStub instances representing all combinations
        
    Example:
        >>> stubs = generate_benchmark_stubs(
        ...     model_names=["model1", "model2"], 
        ...     sparse_attention_configs=[("dense", None), ("sparse", config)],
        ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
        ...     adapter_config=AdapterConfig()
        ... )
        >>> len(stubs)  # 2 models Ã— 2 configs Ã— 1 benchmark Ã— 1 subset = 4
        4
    """
    from datetime import datetime
    
    if execution_timestamp is None:
        execution_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

->

    """Generate all benchmark stubs for the model Ã— sparse_config Ã— benchmark Ã— subset matrix.
    
    Creates BenchmarkStub instances for every combination of:
    - model_names Ã— sparse_attention_configs Ã— benchmark_configs Ã— subsets
    
    Args:
        model_names: List of model names to benchmark
        sparse_attention_configs: List of (name, config) tuples for sparse attention
        benchmark_configs: List of benchmark configurations
        adapter_config: Configuration for model adapter
        generation_kwargs: Optional generation parameters
        base_result_dir: Base directory for storing results
        
    Returns:
        List of BenchmarkStub instances representing all combinations
        
    Example:
        >>> stubs = generate_benchmark_stubs(
        ...     model_names=["model1", "model2"], 
        ...     sparse_attention_configs=[("dense", None), ("sparse", config)],
        ...     benchmark_configs=[BenchmarkConfig("longbench", ["narrativeqa"])],
        ...     adapter_config=AdapterConfig()
        ... )
        >>> len(stubs)  # 2 models Ã— 2 configs Ã— 1 benchmark Ã— 1 subset = 4
        4
    """
```

```python
            all_stubs = generate_benchmark_stubs(
                model_names=model_names,
                sparse_attention_configs=sparse_attention_configs,
                benchmark_configs=benchmark_configs,
                adapter_config=adapter_config,
                generation_kwargs=generation_kwargs,
                base_result_dir=str(self.base_result_dir),
                execution_timestamp=self.execution_timestamp
            )

->

            all_stubs = generate_benchmark_stubs(
                model_names=model_names,
                sparse_attention_configs=sparse_attention_configs,
                benchmark_configs=benchmark_configs,
                adapter_config=adapter_config,
                generation_kwargs=generation_kwargs,
                base_result_dir=str(self.base_result_dir)
            )
```

Now let me test that our fix works correctly:

---

**User**

[BAD convention] Can we have a better way to registry of benchmarks. Currently we are listing the benchmark names. We should not have to 

---

**Cursor**

You're absolutely right! The current benchmark registry is a bad pattern. Let me look at the current implementation and propose a better solution.

You're absolutely correct! The current manual registry system is terrible. Here's a much better approach using automatic discovery with decorators:

```python
# =============================================================================
# Automatic Benchmark Registry (Task 1.1.2 - Improved)
# =============================================================================

# Global registry for benchmarks
_BENCHMARK_REGISTRY: Dict[str, type] = {}


def register_benchmark(name: Optional[str] = None, aliases: Optional[List[str]] = None):
    """Decorator to automatically register benchmark classes.
    
    This decorator allows benchmark classes to register themselves automatically,
    eliminating the need for manual registry maintenance.
    
    Args:
        name: Custom name for the benchmark (defaults to class.benchmark_name)
        aliases: Optional list of alternative names for the benchmark
        
    Example:
        >>> @register_benchmark("my_benchmark", aliases=["my_bench", "mb"])
        >>> class MyBenchmark(Benchmark):
        ...     benchmark_name = "my_benchmark"
        ...     # ... rest of implementation
    """
    def decorator(benchmark_class: type) -> type:
        # Get benchmark name from parameter, class attribute, or class name
        if name is not None:
            benchmark_name = name
        elif hasattr(benchmark_class, 'benchmark_name') and benchmark_class.benchmark_name:
            benchmark_name = benchmark_class.benchmark_name
        else:
            benchmark_name = benchmark_class.__name__.lower()
        
        # Register main name
        _BENCHMARK_REGISTRY[benchmark_name.lower()] = benchmark_class
        
        # Register aliases
        if aliases:
            for alias in aliases:
                _BENCHMARK_REGISTRY[alias.lower()] = benchmark_class
        
        return benchmark_class
    
    return decorator


def get_registered_benchmarks() -> Dict[str, type]:
    """Get all registered benchmark classes.
    
    Returns:
        Dictionary mapping benchmark names to their classes
    """
    return _BENCHMARK_REGISTRY.copy()


def get_available_benchmark_names() -> List[str]:
    """Get list of all available benchmark names.
    
    Returns:
        Sorted list of registered benchmark names
    """
    return sorted(_BENCHMARK_REGISTRY.keys())


def create_benchmark_instance(
    benchmark_name: str, 
    subsets: Optional[List[str]] = None
) -> Benchmark:
    """Factory function to create benchmark instances by name.
    
    Uses the automatic registry to instantiate benchmark classes.
    No manual maintenance required - benchmarks register themselves.
    
    Args:
        benchmark_name: Name of the benchmark to create
        subsets: Optional list of subsets to run for the benchmark
        
    Returns:
        Instantiated benchmark object
        
    Raises:
        ValueError: If benchmark_name is not registered
        
    Example:
        >>> benchmark = create_benchmark_instance("longbench", ["narrativeqa"])
        >>> benchmark = create_benchmark_instance("mock_benchmark")
    """
    # Normalize benchmark name
    benchmark_name = benchmark_name.strip().lower()
    
    if benchmark_name not in _BENCHMARK_REGISTRY:
        available_benchmarks: List[str] = get_available_benchmark_names()
        raise ValueError(
            f"Unknown benchmark '{benchmark_name}'. Available benchmarks: {available_benchmarks}"
        )
    
    benchmark_class = _BENCHMARK_REGISTRY[benchmark_name]
    
    try:
        # Create instance with optional subsets
        benchmark_instance: Benchmark = benchmark_class(subsets_to_run=subsets)
        return benchmark_instance
        
    except Exception as e:
        raise RuntimeError(f"Failed to instantiate benchmark '{benchmark_name}': {e}")


def ensure_benchmarks_loaded() -> None:
    """Ensure all benchmark modules are loaded to trigger registration.
    
    This function imports all benchmark modules to ensure their decorators
    have been executed and they're registered in the global registry.
    """
    import importlib
    import pkgutil
    import benchmark
    
    # Get all modules in the benchmark package
    benchmark_package_path = benchmark.__path__
    
    for importer, module_name, ispkg in pkgutil.iter_modules(benchmark_package_path):
        if not ispkg and module_name != '__init__' and module_name != 'base':
            try:
                # Import the module to trigger @register_benchmark decorators
                module_full_name = f"benchmark.{module_name}"
                importlib.import_module(module_full_name)
            except ImportError as e:
                # Some modules might not be importable (missing dependencies, etc.)
                # This is OK - we just skip them
                import logging
                logging.debug(f"Could not import benchmark module {module_name}: {e}")
                continue
```

Now let me update the BenchmarkExecutor to use the new registry system:

```python
from .executor_config import (
    AdapterConfig,
    BenchmarkConfig,
    BenchmarkStub,
    ExecutionResults,
    ExecutionProgress,
    create_gpu_pool,
    filter_existing_results,
    generate_benchmark_stubs,
    validate_gpu_availability,
)

->

from .executor_config import (
    AdapterConfig,
    BenchmarkConfig,
    BenchmarkStub,
    ExecutionResults,
    ExecutionProgress,
    create_gpu_pool,
    filter_existing_results,
    generate_benchmark_stubs,
    validate_gpu_availability,
    ensure_benchmarks_loaded,
)
```

```python
        # Validate GPU availability
        self._validate_gpu_setup()
        
        # Initialize execution state
        self.execution_timestamp: Optional[str] = None
        self.current_execution_dir: Optional[Path] = None

->

        # Validate GPU availability
        self._validate_gpu_setup()
        
        # Load all available benchmarks
        ensure_benchmarks_loaded()
        
        # Initialize execution state
        self.execution_timestamp: Optional[str] = None
        self.current_execution_dir: Optional[Path] = None
```

Now let me create an example showing how to migrate existing benchmarks to use the new registry system:

```python
# SPDX-FileCopyrightText: Copyright (c) 1993-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0

"""Mock benchmark implementation for testing and demonstration purposes."""

from typing import Dict, Any, List
import pandas as pd

from ..base import Benchmark
from ..executor_config import register_benchmark


@register_benchmark("mock_benchmark", aliases=["mock", "test_benchmark"])
class MockBenchmark(Benchmark):
```

Now let me test the new automatic registry system:

