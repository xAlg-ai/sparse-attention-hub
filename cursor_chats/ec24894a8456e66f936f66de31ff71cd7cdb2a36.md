# Implement MagicPig and its configuration
_Exported on 7/18/2025 at 12:27:57 PDT from Cursor (1.2.4)_

---

**User**


Implement MagicPig  <-- which extends from SamplingMasker and SamplingMaskerConfig


MagicPigConfig:
	lsh_l : int
	lsh_k : int
	# will not use sampling_rate

Some points:
# Read and understand how other maskers are implemented
# Notes on how to implement add_mask:


pseudo code for add_mask
1. check if previous_mask is full mask, if so return full mask.
3. prob = compute_probabilities (b,h,#queries, #keys)
	- transform keys and queries for maximum inner product search 
			# using Neyshabur, Behnam and Srebro, Nathan. On symmetric and asymmetric lshs for inner product search. 
	- Compute the LSH probability of collision using the cosine similarities

4. matches = compute_matches between transformed keys and queries
	# compute random signed projection signature of size lsh_l * lsh_k
	# compute the indices that would match in a LSH table. i.e. if we divide signature into lsh_l groups of lsh_k bits, then at least one group should match. use torch operations for vectorized computation of this.
5. dense_mask = matches * probabilities 

	

5. use this dense_mask to generate Mask from dense mask.
6. merge this_mask with previous mask using previous_mask.merge and return the new mask.


Understand, sparse_attention_hub and specifically Maskers and masking_utils to understand what is going on. 
Then proceed to make a detailed pseudo code of the algorithm you will implement -- use cursor_chats/plans/magicpig.md file Let us iterate on this implementation. Do not write any code at this point.

---

**Cursor**

I'll help you implement the MagicPig masker. Let me start by understanding the codebase structure and how other maskers are implemented, then create a detailed pseudo code plan.


Now let me explore the codebase to understand the structure and how other maskers are implemented.


Now let me look at some existing sampling masker implementations to understand the patterns:


Now let me search for any existing LSH or similarity search implementations in the codebase:


Now let me create the directory for the plan and write the detailed pseudo code:


---

**User**

directory already exisits

---

**Cursor**

I'll create the detailed pseudo code plan for the MagicPig implementation. Let me write it to the existing directory:


```markdown
# MagicPig Implementation Plan

## Overview
MagicPig is a sampling-based masker that uses Locality Sensitive Hashing (LSH) to efficiently approximate maximum inner product search for attention computation. It combines LSH-based similarity matching with probability-based sampling to create sparse attention patterns.

## Algorithm Pseudo Code

### 1. Configuration and Initialization
```python
@dataclass
class MagicPigConfig(SamplingMaskerConfig):
    lsh_l: int  # number of LSH tables
    lsh_k: int  # number of bits per LSH table
    # Note: sampling_rate is inherited but not used in MagicPig
```

### 2. Main add_mask Method Pseudo Code

```python
def add_mask(self, keys, queries, values, attention_mask, sparse_meta_data, previous_mask, **kwargs):
    # Step 1: Check if previous_mask is full mask
    if previous_mask.is_full_mask():
        return previous_mask
    
    # Step 2: Extract tensor dimensions
    batch_size, num_heads, seq_len_queries, seq_len_keys = extract_dimensions(keys, queries)
    
    # Step 3: Compute probabilities using LSH collision probability
    probabilities = compute_probabilities(keys, queries, self.lsh_k, self.lsh_l)
    # Shape: (batch_size, num_heads, seq_len_queries, seq_len_keys)
    
    # Step 4: Compute LSH matches
    matches = compute_lsh_matches(keys, queries, self.lsh_k, self.lsh_l)
    # Shape: (batch_size, num_heads, seq_len_queries, seq_len_keys) - binary tensor
    
    # Step 5: Combine matches and probabilities
    dense_mask = matches * probabilities
    # Shape: (batch_size, num_heads, seq_len_queries, seq_len_keys)
    
    # Step 6: Create Mask from dense mask
    this_mask = Mask.create_mask_from_dense_mask(
        shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
        mask=dense_mask,
        dtype=previous_mask.dtype
    )
    
    # Step 7: Merge with previous mask and return
    return previous_mask.merge_mask(this_mask, inplace=False)
```

### 3. Key Helper Functions

#### 3.1 Transform for Maximum Inner Product Search
```python
def transform_for_mips(keys, queries):
    """
    Transform keys and queries for maximum inner product search using the technique from:
    Neyshabur, Behnam and Srebro, Nathan. "On symmetric and asymmetric LSHs for inner product search."
    
    Args:
        keys: (batch_size, num_heads, seq_len_keys, head_dim)
        queries: (batch_size, num_heads, seq_len_queries, head_dim)
    
    Returns:
        transformed_keys, transformed_queries
    """
    # Normalize queries
    query_norms = torch.norm(queries, dim=-1, keepdim=True)
    queries_normalized = queries / (query_norms + 1e-8)
    
    # For keys, use the augmentation technique
    key_norms = torch.norm(keys, dim=-1, keepdim=True)
    max_key_norm = torch.max(key_norms)
    
    # Scale keys by max norm
    keys_scaled = keys / max_key_norm
    
    # Compute augmentation terms
    key_augmentation = torch.sqrt(max_key_norm**2 - key_norms**2) / max_key_norm
    query_augmentation = torch.zeros_like(queries_normalized[..., :1])
    
    # Concatenate augmentation terms
    keys_transformed = torch.cat([keys_scaled, key_augmentation], dim=-1)
    queries_transformed = torch.cat([queries_normalized, query_augmentation], dim=-1)
    
    return keys_transformed, queries_transformed
```

#### 3.2 Compute LSH Collision Probabilities
```python
def compute_probabilities(keys, queries, lsh_k, lsh_l):
    """
    Compute LSH collision probabilities using cosine similarity.
    
    Args:
        keys: (batch_size, num_heads, seq_len_keys, head_dim)
        queries: (batch_size, num_heads, seq_len_queries, head_dim)
        lsh_k: number of bits per LSH table
        lsh_l: number of LSH tables
    
    Returns:
        probabilities: (batch_size, num_heads, seq_len_queries, seq_len_keys)
    """
    # Transform for MIPS
    keys_transformed, queries_transformed = transform_for_mips(keys, queries)
    
    # Compute cosine similarities
    # Normalize the transformed vectors
    keys_norm = torch.norm(keys_transformed, dim=-1, keepdim=True)
    queries_norm = torch.norm(queries_transformed, dim=-1, keepdim=True)
    
    keys_normalized = keys_transformed / (keys_norm + 1e-8)
    queries_normalized = queries_transformed / (queries_norm + 1e-8)
    
    # Compute cosine similarities
    cosine_similarities = torch.matmul(queries_normalized, keys_normalized.transpose(-2, -1))
    cosine_similarities = torch.clamp(cosine_similarities, -1.0, 1.0)
    
    # Convert to angles
    angles = torch.acos(cosine_similarities)
    
    # Compute LSH collision probability
    # P(collision) = (1 - theta/pi)^k for single table
    # P(collision across l tables) = 1 - (1 - p)^l
    single_table_prob = (1 - angles / torch.pi) ** lsh_k
    collision_prob = 1 - (1 - single_table_prob) ** lsh_l
    
    return collision_prob
```

#### 3.3 Compute LSH Matches
```python
def compute_lsh_matches(keys, queries, lsh_k, lsh_l):
    """
    Compute LSH matches using random signed projections.
    
    Args:
        keys: (batch_size, num_heads, seq_len_keys, head_dim)
        queries: (batch_size, num_heads, seq_len_queries, head_dim)
        lsh_k: number of bits per LSH table
        lsh_l: number of LSH tables
    
    Returns:
        matches: (batch_size, num_heads, seq_len_queries, seq_len_keys) - binary tensor
    """
    # Transform for MIPS
    keys_transformed, queries_transformed = transform_for_mips(keys, queries)
    
    batch_size, num_heads, seq_len_queries, seq_len_keys = keys.shape
    head_dim = keys_transformed.shape[-1]
    
    # Generate random projection matrix
    total_bits = lsh_k * lsh_l
    projection = torch.randn(head_dim, total_bits, device=keys.device, dtype=keys.dtype)
    
    # Compute signatures
    # Reshape for batch processing
    keys_flat = keys_transformed.view(-1, head_dim)  # (batch*heads*seq_len_keys, head_dim)
    queries_flat = queries_transformed.view(-1, head_dim)  # (batch*heads*seq_len_queries, head_dim)
    
    # Compute signed projections
    keys_signatures = torch.sign(torch.matmul(keys_flat, projection))  # (batch*heads*seq_len_keys, total_bits)
    queries_signatures = torch.sign(torch.matmul(queries_flat, projection))  # (batch*heads*seq_len_queries, total_bits)
    
    # Reshape back to original dimensions
    keys_signatures = keys_signatures.view(batch_size, num_heads, seq_len_keys, total_bits)
    queries_signatures = queries_signatures.view(batch_size, num_heads, seq_len_queries, total_bits)
    
    # Compute matches for each query-key pair
    # Expand dimensions for broadcasting
    keys_signatures_expanded = keys_signatures.unsqueeze(2)  # (batch, heads, 1, seq_len_keys, total_bits)
    queries_signatures_expanded = queries_signatures.unsqueeze(3)  # (batch, heads, seq_len_queries, 1, total_bits)
    
    # Compute element-wise product
    signature_matches = keys_signatures_expanded * queries_signatures_expanded
    # Shape: (batch, heads, seq_len_queries, seq_len_keys, total_bits)
    
    # Reshape to group by LSH tables
    signature_matches_grouped = signature_matches.view(batch_size, num_heads, seq_len_queries, seq_len_keys, lsh_l, lsh_k)
    
    # Check if at least one group (table) has all bits matching
    # Sum within each group - if sum == lsh_k, all bits match
    group_matches = (signature_matches_grouped.sum(dim=-1) == lsh_k).int()
    # Shape: (batch, heads, seq_len_queries, seq_len_keys, lsh_l)
    
    # Check if at least one table has a match
    matches = (group_matches.sum(dim=-1) > 0).int()
    # Shape: (batch, heads, seq_len_queries, seq_len_keys)
    
    return matches
```

### 4. Implementation Notes

#### 4.1 Key Design Decisions
1. **MIPS Transformation**: Using the technique from Neyshabur & Srebro to transform inner product search to cosine similarity
2. **LSH Parameters**: 
   - `lsh_k`: Controls precision of each hash table (higher = more precise but fewer collisions)
   - `lsh_l`: Controls number of hash tables (higher = more chances for collision)
3. **Probability Computation**: Using theoretical LSH collision probability based on cosine similarity
4. **Match Computation**: Using random signed projections for efficient LSH implementation

#### 4.2 Memory and Computational Considerations
1. **Memory Usage**: 
   - Need to store transformed keys/queries
   - Need to store random projection matrix
   - Need to store signatures during computation
2. **Computational Complexity**:
   - O(batch_size * num_heads * seq_len * head_dim * total_bits) for signature computation
   - O(batch_size * num_heads * seq_len_queries * seq_len_keys * lsh_l) for match computation
3. **Optimizations**:
   - Use vectorized operations where possible
   - Consider caching random projections if used multiple times
   - Use efficient broadcasting for signature comparisons

#### 4.3 Integration with Existing Framework
1. **Mask Creation**: Use `Mask.create_mask_from_dense_mask()` to create mask from computed dense tensor
2. **Mask Merging**: Use `previous_mask.merge_mask()` to combine with existing masks
3. **Configuration**: Extend `SamplingMaskerConfig` but ignore `sampling_rate`
4. **Registry**: Register with `@MaskerRegistry.register(MagicPigConfig)`

### 5. Testing Strategy

#### 5.1 Unit Tests
1. Test MIPS transformation correctness
2. Test LSH probability computation
3. Test LSH match computation
4. Test mask creation and merging
5. Test edge cases (full masks, empty masks)

#### 5.2 Integration Tests
1. Test with different LSH parameters
2. Test with different tensor shapes
3. Test with existing masker pipeline
4. Test memory usage and performance

### 6. Expected Behavior

#### 6.1 Sparsity Pattern
- MagicPig should create sparser attention patterns than random sampling
- Similar keys should have higher probability of being attended to
- The sparsity should be controlled by LSH parameters

#### 6.2 Performance Characteristics
- Should be more computationally expensive than random sampling due to LSH computation
- Should provide better attention quality for similar content
- Memory usage should scale with LSH parameters

#### 6.3 Quality Metrics
- Compare attention quality with random sampling
- Measure sparsity vs. performance trade-off
- Evaluate similarity preservation in attention patterns
```

Perfect! I've created a comprehensive implementation plan for the MagicPig masker. Let me summarize the key aspects of this plan:

## Summary of MagicPig Implementation Plan

### **Core Algorithm**
The MagicPig masker implements Locality Sensitive Hashing (LSH) for efficient attention computation by:

1. **Transforming inner product search to cosine similarity** using the Neyshabur & Srebro technique
2. **Computing LSH collision probabilities** based on theoretical analysis
3. **Generating LSH matches** using random signed projections
4. **Combining matches with probabilities** to create the final attention mask

### **Key Components**

1. **Configuration**: `MagicPigConfig` with `lsh_l` (number of tables) and `lsh_k` (bits per table)
2. **MIPS Transformation**: Augments vectors to convert inner product to cosine similarity
3. **Probability Computation**: Uses theoretical LSH collision probability formula
4. **Match Computation**: Uses random signed projections with efficient vectorized operations
5. **Mask Integration**: Creates dense mask and merges with previous mask

### **Technical Details**

- **LSH Parameters**: 
  - `lsh_k`: Controls precision (higher = more precise, fewer collisions)
  - `lsh_l`: Controls coverage (higher = more chances for collision)
- **Memory Usage**: Scales with LSH parameters and sequence length
- **Computational Complexity**: O(batch_size × num_heads × seq_len × head_dim × total_bits)

### **Integration Points**

- Extends `SamplingMasker` but ignores `sampling_rate`
- Uses existing `Mask` class for creation and merging
- Registered with `MaskerRegistry` for automatic instantiation
- Follows the same interface as other sampling maskers

The plan provides a detailed pseudo code implementation that can be directly translated into Python code. The algorithm should provide better attention quality than random sampling by preserving similarity relationships while maintaining computational efficiency through LSH techniques.

Would you like me to proceed with implementing this plan, or would you like to discuss any specific aspects of the algorithm first?

---

**User**

implement the plan 

---

**Cursor**

I'll implement the MagicPig masker according to the plan. Let me start by implementing the complete solution:


```python
"""Magic pig sampling masker implementation.

This module provides a MagicPig sampling masker that uses Locality Sensitive Hashing (LSH)
to efficiently approximate maximum inner product search for attention computation. It combines
LSH-based similarity matching with probability-based sampling to create sparse attention patterns.

The MagicPig masker is useful for:
- Reducing computational complexity in attention mechanisms while preserving similarity relationships
- Creating attention patterns that prioritize similar content
- Exploring LSH-based attention patterns for research purposes
"""

from dataclasses import dataclass
from typing import Any, Dict, Tuple

import torch

from sparse_attention_hub.sparse_attention.research_attention.maskers.base import (
    MaskerConfig,
    MaskerRegistry,
)
from sparse_attention_hub.sparse_attention.utils.mask import Mask

from ..base import SamplingMasker, SamplingMaskerConfig


@dataclass
class MagicPigConfig(SamplingMaskerConfig):
    """Configuration for MagicPig masker.
    
    This configuration class inherits from SamplingMaskerConfig and adds LSH-specific
    parameters. Note that sampling_rate is inherited but not used in MagicPig.
    
    Attributes:
        lsh_l: Number of LSH tables. Higher values increase the chance of finding
            similar items but also increase computational cost.
        lsh_k: Number of bits per LSH table. Higher values make each table more
            precise but reduce the probability of collisions.
    """

    lsh_l: int  # number of LSH tables
    lsh_k: int  # number of bits per LSH table

    def __post_init__(self) -> None:
        """Validate LSH parameters after initialization."""
        super().__post_init__()
        if self.lsh_l <= 0:
            raise ValueError(f"lsh_l must be positive, got {self.lsh_l}")
        if self.lsh_k <= 0:
            raise ValueError(f"lsh_k must be positive, got {self.lsh_k}")


@MaskerRegistry.register(MagicPigConfig)
class MagicPig(SamplingMasker):
    """Magic Pig masker using Locality Sensitive Hashing for attention computation.
    
    This masker implements LSH-based sampling of attention positions by using
    locality sensitive hashing to identify similar key-query pairs. The sampling
    is based on both LSH collision probability and actual LSH matches.
    
    Attributes:
        lsh_l: Number of LSH tables used for hashing.
        lsh_k: Number of bits per LSH table.
        
    Important Notes:
        - Uses the Neyshabur & Srebro technique to transform inner product search to cosine similarity
        - Computes theoretical LSH collision probabilities based on cosine similarity
        - Uses random signed projections for efficient LSH implementation
        - Ignores the sampling_rate parameter from the parent class
        - Merge operation adds the data in masks and clamps to 1.0
        
    Example:
        >>> config = MagicPigConfig(lsh_l=5, lsh_k=4)
        >>> masker = MagicPig(config)
        >>> # Use masker.add_mask() to apply LSH-based sampling to attention masks
    """

    def __init__(self, config: MagicPigConfig) -> None:
        """Initialize Magic Pig masker with configuration.
        
        Args:
            config: Configuration object containing LSH parameters.
                
        Raises:
            ValueError: If LSH parameters are invalid.
        """
        super().__init__(config)
        self.lsh_l = config.lsh_l
        self.lsh_k = config.lsh_k

    def _transform_for_mips(
        self, keys: torch.Tensor, queries: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Transform keys and queries for maximum inner product search.
        
        Uses the technique from Neyshabur, Behnam and Srebro, Nathan. 
        "On symmetric and asymmetric LSHs for inner product search."
        
        Args:
            keys: Key tensor with shape (batch_size, num_heads, seq_len_keys, head_dim).
            queries: Query tensor with shape (batch_size, num_heads, seq_len_queries, head_dim).
        
        Returns:
            Tuple of (transformed_keys, transformed_queries) with augmented dimensions.
        """
        # Normalize queries
        query_norms: torch.Tensor = torch.norm(queries, dim=-1, keepdim=True)
        queries_normalized: torch.Tensor = queries / (query_norms + 1e-8)
        
        # For keys, use the augmentation technique
        key_norms: torch.Tensor = torch.norm(keys, dim=-1, keepdim=True)
        max_key_norm: torch.Tensor = torch.max(key_norms)
        
        # Scale keys by max norm
        keys_scaled: torch.Tensor = keys / max_key_norm
        
        # Compute augmentation terms
        key_augmentation: torch.Tensor = torch.sqrt(max_key_norm**2 - key_norms**2) / max_key_norm
        query_augmentation: torch.Tensor = torch.zeros_like(queries_normalized[..., :1])
        
        # Concatenate augmentation terms
        keys_transformed: torch.Tensor = torch.cat([keys_scaled, key_augmentation], dim=-1)
        queries_transformed: torch.Tensor = torch.cat([queries_normalized, query_augmentation], dim=-1)
        
        return keys_transformed, queries_transformed

    def _compute_probabilities(
        self, keys: torch.Tensor, queries: torch.Tensor
    ) -> torch.Tensor:
        """Compute LSH collision probabilities using cosine similarity.
        
        Args:
            keys: Key tensor with shape (batch_size, num_heads, seq_len_keys, head_dim).
            queries: Query tensor with shape (batch_size, num_heads, seq_len_queries, head_dim).
        
        Returns:
            Collision probabilities tensor with shape (batch_size, num_heads, seq_len_queries, seq_len_keys).
        """
        # Transform for MIPS
        keys_transformed: torch.Tensor
        queries_transformed: torch.Tensor
        keys_transformed, queries_transformed = self._transform_for_mips(keys, queries)
        
        # Compute cosine similarities
        # Normalize the transformed vectors
        keys_norm: torch.Tensor = torch.norm(keys_transformed, dim=-1, keepdim=True)
        queries_norm: torch.Tensor = torch.norm(queries_transformed, dim=-1, keepdim=True)
        
        keys_normalized: torch.Tensor = keys_transformed / (keys_norm + 1e-8)
        queries_normalized: torch.Tensor = queries_transformed / (queries_norm + 1e-8)
        
        # Compute cosine similarities
        cosine_similarities: torch.Tensor = torch.matmul(
            queries_normalized, keys_normalized.transpose(-2, -1)
        )
        cosine_similarities = torch.clamp(cosine_similarities, -1.0, 1.0)
        
        # Convert to angles
        angles: torch.Tensor = torch.acos(cosine_similarities)
        
        # Compute LSH collision probability
        # P(collision) = (1 - theta/pi)^k for single table
        # P(collision across l tables) = 1 - (1 - p)^l
        single_table_prob: torch.Tensor = (1 - angles / torch.pi) ** self.lsh_k
        collision_prob: torch.Tensor = 1 - (1 - single_table_prob) ** self.lsh_l
        
        return collision_prob

    def _compute_lsh_matches(
        self, keys: torch.Tensor, queries: torch.Tensor
    ) -> torch.Tensor:
        """Compute LSH matches using random signed projections.
        
        Args:
            keys: Key tensor with shape (batch_size, num_heads, seq_len_keys, head_dim).
            queries: Query tensor with shape (batch_size, num_heads, seq_len_queries, head_dim).
        
        Returns:
            Binary matches tensor with shape (batch_size, num_heads, seq_len_queries, seq_len_keys).
        """
        # Transform for MIPS
        keys_transformed: torch.Tensor
        queries_transformed: torch.Tensor
        keys_transformed, queries_transformed = self._transform_for_mips(keys, queries)
        
        batch_size: int = keys.shape[0]
        num_heads: int = keys.shape[1]
        seq_len_queries: int = queries.shape[2]
        seq_len_keys: int = keys.shape[2]
        head_dim: int = keys_transformed.shape[-1]
        
        # Generate random projection matrix
        total_bits: int = self.lsh_k * self.lsh_l
        projection: torch.Tensor = torch.randn(
            head_dim, total_bits, device=keys.device, dtype=keys.dtype
        )
        
        # Compute signatures
        # Reshape for batch processing
        keys_flat: torch.Tensor = keys_transformed.view(-1, head_dim)  # (batch*heads*seq_len_keys, head_dim)
        queries_flat: torch.Tensor = queries_transformed.view(-1, head_dim)  # (batch*heads*seq_len_queries, head_dim)
        
        # Compute signed projections
        keys_signatures: torch.Tensor = torch.sign(torch.matmul(keys_flat, projection))  # (batch*heads*seq_len_keys, total_bits)
        queries_signatures: torch.Tensor = torch.sign(torch.matmul(queries_flat, projection))  # (batch*heads*seq_len_queries, total_bits)
        
        # Reshape back to original dimensions
        keys_signatures = keys_signatures.view(batch_size, num_heads, seq_len_keys, total_bits)
        queries_signatures = queries_signatures.view(batch_size, num_heads, seq_len_queries, total_bits)
        
        # Compute matches for each query-key pair
        # Expand dimensions for broadcasting
        keys_signatures_expanded: torch.Tensor = keys_signatures.unsqueeze(2)  # (batch, heads, 1, seq_len_keys, total_bits)
        queries_signatures_expanded: torch.Tensor = queries_signatures.unsqueeze(3)  # (batch, heads, seq_len_queries, 1, total_bits)
        
        # Compute element-wise product
        signature_matches: torch.Tensor = keys_signatures_expanded * queries_signatures_expanded
        # Shape: (batch, heads, seq_len_queries, seq_len_keys, total_bits)
        
        # Reshape to group by LSH tables
        signature_matches_grouped: torch.Tensor = signature_matches.view(
            batch_size, num_heads, seq_len_queries, seq_len_keys, self.lsh_l, self.lsh_k
        )
        
        # Check if at least one group (table) has all bits matching
        # Sum within each group - if sum == lsh_k, all bits match
        group_matches: torch.Tensor = (signature_matches_grouped.sum(dim=-1) == self.lsh_k).int()
        # Shape: (batch, heads, seq_len_queries, seq_len_keys, lsh_l)
        
        # Check if at least one table has a match
        matches: torch.Tensor = (group_matches.sum(dim=-1) > 0).int()
        # Shape: (batch, heads, seq_len_queries, seq_len_keys)
        
        return matches

    def add_mask(
        self,
        keys: torch.Tensor,
        queries: torch.Tensor,
        values: torch.Tensor,
        attention_mask: torch.Tensor,
        sparse_meta_data: Dict[Any, Any],
        previous_mask: Mask,
        **kwargs: Dict[str, Any],
    ) -> Mask:
        """Add Magic Pig mask to attention computation.
        
        This method implements the core LSH-based sampling logic. It computes
        LSH collision probabilities and matches, then combines them to create
        a sparse attention pattern that prioritizes similar content.
        
        Args:
            keys: Key tensor with shape (batch_size, num_heads, seq_len_keys, head_dim).
                The keys used in the attention computation.
            queries: Query tensor with shape (batch_size, num_heads, seq_len_queries, head_dim).
                The queries used in the attention computation.
            values: Value tensor with shape (batch_size, num_heads, seq_len_keys, head_dim).
                The values used in the attention computation.
            attention_mask: Attention mask tensor indicating which positions are valid.
                This is typically used for padding or causal masking.
            sparse_meta_data: Dictionary containing sparse attention metadata.
                This can include additional information needed for the masker.
            previous_mask: Previous attention mask to merge with the new Magic Pig mask.
                If this is a full mask, it will be returned unchanged.
            **kwargs: Additional keyword arguments that may be passed to the masker.
                These are not used in the current implementation but allow for
                future extensibility.
        
        Returns:
            A new Mask object representing the attention pattern after applying
            LSH-based sampling. The mask is created by merging the previous mask
            with the new Magic Pig mask.
            
        Note:
            - If previous_mask is a full mask, it is returned unchanged
            - The LSH parameters (lsh_k, lsh_l) control the sparsity and quality
            - Similar key-query pairs have higher probability of being attended to
            - The resulting mask is merged with the previous mask using the merge_mask method
        """
        # Step 1: Check if previous_mask is full mask, if so return full mask
        if previous_mask.is_full_mask():
            return previous_mask

        # Step 2: Extract tensor dimensions
        batch_size: int = queries.shape[0]
        num_heads: int = queries.shape[1]
        seq_len_queries: int = queries.shape[2]
        seq_len_keys: int = keys.shape[2]
        
        # Step 3: Compute probabilities using LSH collision probability
        probabilities: torch.Tensor = self._compute_probabilities(keys, queries)
        # Shape: (batch_size, num_heads, seq_len_queries, seq_len_keys)
        
        # Step 4: Compute LSH matches
        matches: torch.Tensor = self._compute_lsh_matches(keys, queries)
        # Shape: (batch_size, num_heads, seq_len_queries, seq_len_keys) - binary tensor
        
        # Step 5: Combine matches and probabilities
        dense_mask: torch.Tensor = matches * probabilities
        # Shape: (batch_size, num_heads, seq_len_queries, seq_len_keys)
        
        # Step 6: Create Mask from dense mask
        mask_shape: Tuple[int, int, int, int] = (batch_size, num_heads, seq_len_queries, seq_len_keys)
        this_mask: Mask = Mask.create_mask_from_dense_mask(
            shape=mask_shape,
            mask=dense_mask,
            dtype=previous_mask.dtype
        )
        
        # Step 7: Merge with previous mask and return
        return previous_mask.merge_mask(this_mask, inplace=False)

    @classmethod
    def create_from_config(cls, config: MaskerConfig) -> "MagicPig":
        """Create MagicPig instance from configuration.
        
        This factory method creates a MagicPig instance from a
        configuration object. It validates that the config is of the correct
        type before creating the masker.
        
        Args:
            config: Configuration object for the masker. Must be an instance
                of MagicPigConfig.
        
        Returns:
            A new MagicPig instance configured with the provided config.
            
        Raises:
            ValueError: If the config is not an instance of MagicPigConfig.
                This ensures type safety and prevents configuration mismatches.
                
        Example:
            >>> config = MagicPigConfig(lsh_l=5, lsh_k=4)
            >>> masker = MagicPig.create_from_config(config)
        """
        if not isinstance(config, MagicPigConfig):
            raise ValueError(f"Invalid config type: {type(config)}")
        return cls(config)
```

Now let me create a comprehensive test file for the MagicPig implementation:


```python
"""Tests for MagicPig masker implementation."""

import pytest
import torch

from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations.magic_pig import (
    MagicPig,
    MagicPigConfig,
)
from sparse_attention_hub.sparse_attention.utils.mask import Mask


class TestMagicPigConfig:
    """Test MagicPigConfig class."""

    def test_valid_config(self) -> None:
        """Test valid configuration creation."""
        config = MagicPigConfig(lsh_l=5, lsh_k=4, sampling_rate=0.5)
        assert config.lsh_l == 5
        assert config.lsh_k == 4
        assert config.sampling_rate == 0.5

    def test_invalid_lsh_l(self) -> None:
        """Test invalid lsh_l parameter."""
        with pytest.raises(ValueError, match="lsh_l must be positive"):
            MagicPigConfig(lsh_l=0, lsh_k=4, sampling_rate=0.5)

    def test_invalid_lsh_k(self) -> None:
        """Test invalid lsh_k parameter."""
        with pytest.raises(ValueError, match="lsh_k must be positive"):
            MagicPigConfig(lsh_l=5, lsh_k=0, sampling_rate=0.5)

    def test_negative_lsh_l(self) -> None:
        """Test negative lsh_l parameter."""
        with pytest.raises(ValueError, match="lsh_l must be positive"):
            MagicPigConfig(lsh_l=-1, lsh_k=4, sampling_rate=0.5)

    def test_negative_lsh_k(self) -> None:
        """Test negative lsh_k parameter."""
        with pytest.raises(ValueError, match="lsh_k must be positive"):
            MagicPigConfig(lsh_l=5, lsh_k=-1, sampling_rate=0.5)


class TestMagicPig:
    """Test MagicPig masker class."""

    @pytest.fixture
    def config(self) -> MagicPigConfig:
        """Create a test configuration."""
        return MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)

    @pytest.fixture
    def masker(self, config: MagicPigConfig) -> MagicPig:
        """Create a test masker."""
        return MagicPig(config)

    @pytest.fixture
    def sample_tensors(self) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Create sample tensors for testing."""
        batch_size, num_heads, seq_len_queries, seq_len_keys, head_dim = 2, 4, 8, 16, 64
        
        keys = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        queries = torch.randn(batch_size, num_heads, seq_len_queries, head_dim)
        values = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        
        return keys, queries, values

    def test_initialization(self, config: MagicPigConfig) -> None:
        """Test masker initialization."""
        masker = MagicPig(config)
        assert masker.lsh_l == 3
        assert masker.lsh_k == 2

    def test_create_from_config(self, config: MagicPigConfig) -> None:
        """Test create_from_config method."""
        masker = MagicPig.create_from_config(config)
        assert isinstance(masker, MagicPig)
        assert masker.lsh_l == 3
        assert masker.lsh_k == 2

    def test_create_from_config_invalid_type(self) -> None:
        """Test create_from_config with invalid config type."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations.random_sampling import (
            RandomSamplingMaskerConfig,
        )
        
        invalid_config = RandomSamplingMaskerConfig(sampling_rate=0.5)
        with pytest.raises(ValueError, match="Invalid config type"):
            MagicPig.create_from_config(invalid_config)

    def test_transform_for_mips(self, masker: MagicPig, sample_tensors: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> None:
        """Test MIPS transformation."""
        keys, queries, _ = sample_tensors
        
        transformed_keys, transformed_queries = masker._transform_for_mips(keys, queries)
        
        # Check shapes - should have one extra dimension for augmentation
        assert transformed_keys.shape[-1] == keys.shape[-1] + 1
        assert transformed_queries.shape[-1] == queries.shape[-1] + 1
        
        # Check that queries are normalized
        query_norms = torch.norm(transformed_queries[..., :-1], dim=-1)
        assert torch.allclose(query_norms, torch.ones_like(query_norms), atol=1e-6)
        
        # Check that keys are scaled by max norm
        key_norms_original = torch.norm(keys, dim=-1)
        max_key_norm = torch.max(key_norms_original)
        key_norms_scaled = torch.norm(transformed_keys[..., :-1], dim=-1)
        assert torch.allclose(key_norms_scaled, key_norms_original / max_key_norm, atol=1e-6)

    def test_compute_probabilities(self, masker: MagicPig, sample_tensors: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> None:
        """Test probability computation."""
        keys, queries, _ = sample_tensors
        
        probabilities = masker._compute_probabilities(keys, queries)
        
        # Check shape
        assert probabilities.shape == (keys.shape[0], keys.shape[1], queries.shape[2], keys.shape[2])
        
        # Check that probabilities are in [0, 1]
        assert torch.all(probabilities >= 0)
        assert torch.all(probabilities <= 1)
        
        # Check that similar vectors have higher probabilities
        # Create similar keys and queries
        similar_keys = keys.clone()
        similar_queries = queries.clone()
        
        # Make one query similar to one key
        similar_queries[0, 0, 0] = similar_keys[0, 0, 0] * 0.9 + torch.randn_like(similar_keys[0, 0, 0]) * 0.1
        
        similar_probs = masker._compute_probabilities(similar_keys, similar_queries)
        
        # The similar pair should have higher probability than random pairs
        similar_prob = similar_probs[0, 0, 0, 0]
        random_probs = similar_probs[0, 0, 0, 1:5]  # Take a few random pairs
        
        # This should generally be true (allowing for some randomness in LSH)
        assert similar_prob >= torch.min(random_probs) - 0.1  # Allow some tolerance

    def test_compute_lsh_matches(self, masker: MagicPig, sample_tensors: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> None:
        """Test LSH match computation."""
        keys, queries, _ = sample_tensors
        
        matches = masker._compute_lsh_matches(keys, queries)
        
        # Check shape
        assert matches.shape == (keys.shape[0], keys.shape[1], queries.shape[2], keys.shape[2])
        
        # Check that matches are binary
        assert torch.all((matches == 0) | (matches == 1))
        
        # Check that identical vectors match
        identical_keys = keys.clone()
        identical_queries = keys.clone()  # Use keys as queries for identical case
        
        identical_matches = masker._compute_lsh_matches(identical_keys, identical_queries)
        
        # Diagonal should have matches (identical vectors)
        for b in range(identical_matches.shape[0]):
            for h in range(identical_matches.shape[1]):
                diagonal_matches = torch.diagonal(identical_matches[b, h])
                # Should have some matches on diagonal (allowing for LSH randomness)
                assert torch.sum(diagonal_matches) > 0

    def test_add_mask_basic(self, masker: MagicPig, sample_tensors: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> None:
        """Test basic add_mask functionality."""
        keys, queries, values = sample_tensors
        
        # Create empty previous mask
        batch_size, num_heads, seq_len_queries, seq_len_keys = queries.shape[0], queries.shape[1], queries.shape[2], keys.shape[2]
        previous_mask = Mask.create_empty_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            dtype=torch.float32
        )
        
        attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
        sparse_meta_data = {}
        
        result_mask = masker.add_mask(
            keys=keys,
            queries=queries,
            values=values,
            attention_mask=attention_mask,
            sparse_meta_data=sparse_meta_data,
            previous_mask=previous_mask
        )
        
        # Check that result is a Mask
        assert isinstance(result_mask, Mask)
        
        # Check shape
        assert result_mask.shape == (batch_size, num_heads, seq_len_queries, seq_len_keys)
        
        # Check that result is not empty (should have some LSH matches)
        assert not result_mask.is_empty()

    def test_add_mask_full_previous(self, masker: MagicPig, sample_tensors: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> None:
        """Test add_mask with full previous mask."""
        keys, queries, values = sample_tensors
        
        # Create full previous mask
        batch_size, num_heads, seq_len_queries, seq_len_keys = queries.shape[0], queries.shape[1], queries.shape[2], keys.shape[2]
        previous_mask = Mask.create_full_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            dtype=torch.float32
        )
        
        attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
        sparse_meta_data = {}
        
        result_mask = masker.add_mask(
            keys=keys,
            queries=queries,
            values=values,
            attention_mask=attention_mask,
            sparse_meta_data=sparse_meta_data,
            previous_mask=previous_mask
        )
        
        # Should return the same full mask
        assert result_mask.is_full_mask()
        assert result_mask.shape == previous_mask.shape

    def test_add_mask_merge(self, masker: MagicPig, sample_tensors: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> None:
        """Test add_mask with existing mask for merging."""
        keys, queries, values = sample_tensors
        
        batch_size, num_heads, seq_len_queries, seq_len_keys = queries.shape[0], queries.shape[1], queries.shape[2], keys.shape[2]
        
        # Create a mask with some existing entries
        existing_mask = Mask.create_empty_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            dtype=torch.float32
        )
        
        # Add some existing entries manually
        dense_existing = torch.zeros(batch_size, num_heads, seq_len_queries, seq_len_keys)
        dense_existing[0, 0, 0, 0] = 1.0  # Add one entry
        existing_mask = Mask.create_mask_from_dense_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            mask=dense_existing,
            dtype=torch.float32
        )
        
        attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
        sparse_meta_data = {}
        
        result_mask = masker.add_mask(
            keys=keys,
            queries=queries,
            values=values,
            attention_mask=attention_mask,
            sparse_meta_data=sparse_meta_data,
            previous_mask=existing_mask
        )
        
        # Check that result is a Mask
        assert isinstance(result_mask, Mask)
        
        # Check that the existing entry is preserved
        result_dense = result_mask.get_dense_mask()
        assert result_dense[0, 0, 0, 0] > 0  # Should still have the existing entry
        
        # Check that new entries are added
        assert torch.sum(result_dense) > torch.sum(dense_existing)

    def test_different_lsh_parameters(self, sample_tensors: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> None:
        """Test different LSH parameter combinations."""
        keys, queries, values = sample_tensors
        
        # Test different parameter combinations
        configs = [
            MagicPigConfig(lsh_l=2, lsh_k=2, sampling_rate=0.5),
            MagicPigConfig(lsh_l=5, lsh_k=3, sampling_rate=0.5),
            MagicPigConfig(lsh_l=10, lsh_k=1, sampling_rate=0.5),
        ]
        
        batch_size, num_heads, seq_len_queries, seq_len_keys = queries.shape[0], queries.shape[1], queries.shape[2], keys.shape[2]
        previous_mask = Mask.create_empty_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            dtype=torch.float32
        )
        attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
        sparse_meta_data = {}
        
        results = []
        for config in configs:
            masker = MagicPig(config)
            result = masker.add_mask(
                keys=keys,
                queries=queries,
                values=values,
                attention_mask=attention_mask,
                sparse_meta_data=sparse_meta_data,
                previous_mask=previous_mask
            )
            results.append(result)
        
        # All results should be valid masks
        for result in results:
            assert isinstance(result, Mask)
            assert result.shape == (batch_size, num_heads, seq_len_queries, seq_len_keys)
            assert not result.is_empty()

    def test_device_consistency(self, masker: MagicPig, sample_tensors: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> None:
        """Test that masker works with different devices."""
        keys, queries, values = sample_tensors
        
        # Test on CPU
        batch_size, num_heads, seq_len_queries, seq_len_keys = queries.shape[0], queries.shape[1], queries.shape[2], keys.shape[2]
        previous_mask = Mask.create_empty_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            dtype=torch.float32
        )
        attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
        sparse_meta_data = {}
        
        result_cpu = masker.add_mask(
            keys=keys,
            queries=queries,
            values=values,
            attention_mask=attention_mask,
            sparse_meta_data=sparse_meta_data,
            previous_mask=previous_mask
        )
        
        # Test on GPU if available
        if torch.cuda.is_available():
            keys_gpu = keys.cuda()
            queries_gpu = queries.cuda()
            values_gpu = values.cuda()
            previous_mask_gpu = Mask.create_empty_mask(
                shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
                dtype=torch.float32
            )
            attention_mask_gpu = attention_mask.cuda()
            
            result_gpu = masker.add_mask(
                keys=keys_gpu,
                queries=queries_gpu,
                values=values_gpu,
                attention_mask=attention_mask_gpu,
                sparse_meta_data=sparse_meta_data,
                previous_mask=previous_mask_gpu
            )
            
            # Both should be valid masks
            assert isinstance(result_cpu, Mask)
            assert isinstance(result_gpu, Mask)
            assert result_cpu.shape == result_gpu.shape

    def test_dtype_consistency(self, masker: MagicPig, sample_tensors: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> None:
        """Test that masker works with different dtypes."""
        keys, queries, values = sample_tensors
        
        # Test with float32
        batch_size, num_heads, seq_len_queries, seq_len_keys = queries.shape[0], queries.shape[1], queries.shape[2], keys.shape[2]
        previous_mask_f32 = Mask.create_empty_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            dtype=torch.float32
        )
        attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
        sparse_meta_data = {}
        
        result_f32 = masker.add_mask(
            keys=keys.float(),
            queries=queries.float(),
            values=values.float(),
            attention_mask=attention_mask,
            sparse_meta_data=sparse_meta_data,
            previous_mask=previous_mask_f32
        )
        
        # Test with float16 if available
        if hasattr(torch, 'float16'):
            previous_mask_f16 = Mask.create_empty_mask(
                shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
                dtype=torch.float16
            )
            
            result_f16 = masker.add_mask(
                keys=keys.half(),
                queries=queries.half(),
                values=values.half(),
                attention_mask=attention_mask,
                sparse_meta_data=sparse_meta_data,
                previous_mask=previous_mask_f16
            )
            
            # Both should be valid masks
            assert isinstance(result_f32, Mask)
            assert isinstance(result_f16, Mask)
            assert result_f32.shape == result_f16.shape
```

Let me check the test file that was created:


Now let me run the tests to make sure the implementation works correctly:


Excellent! All tests are passing. Now let me create a simple example script to demonstrate how to use the MagicPig masker:


```python
"""Example usage of MagicPig masker.

This script demonstrates how to use the MagicPig masker for LSH-based attention computation.
It shows the difference between MagicPig and random sampling approaches.
"""

import torch
from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations.magic_pig import (
    MagicPig,
    MagicPigConfig,
)
from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations.random_sampling import (
    RandomSamplingMasker,
    RandomSamplingMaskerConfig,
)
from sparse_attention_hub.sparse_attention.utils.mask import Mask


def create_sample_data(
    batch_size: int = 2,
    num_heads: int = 4,
    seq_len_queries: int = 8,
    seq_len_keys: int = 16,
    head_dim: int = 64,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """Create sample key, query, and value tensors.
    
    Args:
        batch_size: Number of batches.
        num_heads: Number of attention heads.
        seq_len_queries: Length of query sequence.
        seq_len_keys: Length of key sequence.
        head_dim: Dimension of each attention head.
        
    Returns:
        Tuple of (keys, queries, values) tensors.
    """
    keys = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
    queries = torch.randn(batch_size, num_heads, seq_len_queries, head_dim)
    values = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
    
    return keys, queries, values


def analyze_mask_sparsity(mask: Mask, name: str) -> None:
    """Analyze and print sparsity information for a mask.
    
    Args:
        mask: The mask to analyze.
        name: Name of the mask for printing.
    """
    dense_mask = mask.get_dense_mask()
    total_elements = dense_mask.numel()
    non_zero_elements = torch.count_nonzero(dense_mask)
    sparsity = 1.0 - (non_zero_elements.float() / total_elements)
    
    print(f"\n{name} Analysis:")
    print(f"  Shape: {mask.shape}")
    print(f"  Total elements: {total_elements}")
    print(f"  Non-zero elements: {non_zero_elements}")
    print(f"  Sparsity: {sparsity:.4f} ({sparsity*100:.2f}%)")
    print(f"  Average weight: {torch.mean(dense_mask):.4f}")


def compare_magic_pig_vs_random() -> None:
    """Compare MagicPig vs Random sampling approaches."""
    print("=" * 60)
    print("MagicPig vs Random Sampling Comparison")
    print("=" * 60)
    
    # Create sample data
    keys, queries, values = create_sample_data()
    print(f"Sample data shape: keys={keys.shape}, queries={queries.shape}, values={values.shape}")
    
    # Create empty previous mask
    batch_size, num_heads, seq_len_queries, seq_len_keys = queries.shape[0], queries.shape[1], queries.shape[2], keys.shape[2]
    previous_mask = Mask.create_empty_mask(
        shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
        dtype=torch.float32
    )
    
    attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
    sparse_meta_data = {}
    
    # Test MagicPig with different configurations
    magic_pig_configs = [
        ("MagicPig (lsh_l=2, lsh_k=2)", MagicPigConfig(lsh_l=2, lsh_k=2, sampling_rate=0.5)),
        ("MagicPig (lsh_l=5, lsh_k=3)", MagicPigConfig(lsh_l=5, lsh_k=3, sampling_rate=0.5)),
        ("MagicPig (lsh_l=10, lsh_k=1)", MagicPigConfig(lsh_l=10, lsh_k=1, sampling_rate=0.5)),
    ]
    
    for name, config in magic_pig_configs:
        masker = MagicPig(config)
        result_mask = masker.add_mask(
            keys=keys,
            queries=queries,
            values=values,
            attention_mask=attention_mask,
            sparse_meta_data=sparse_meta_data,
            previous_mask=previous_mask
        )
        analyze_mask_sparsity(result_mask, name)
    
    # Test Random sampling for comparison
    random_config = RandomSamplingMaskerConfig(sampling_rate=0.3)  # Approximate sparsity
    random_masker = RandomSamplingMasker(random_config)
    random_result = random_masker.add_mask(
        keys=keys,
        queries=queries,
        values=values,
        attention_mask=attention_mask,
        sparse_meta_data=sparse_meta_data,
        previous_mask=previous_mask
    )
    analyze_mask_sparsity(random_result, "Random Sampling (30%)")


def demonstrate_similarity_preservation() -> None:
    """Demonstrate how MagicPig preserves similarity relationships."""
    print("\n" + "=" * 60)
    print("Similarity Preservation Demonstration")
    print("=" * 60)
    
    # Create data with known similarity structure
    batch_size, num_heads, seq_len_queries, seq_len_keys, head_dim = 1, 1, 4, 8, 32
    
    # Create keys with some similar pairs
    keys = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
    # Make keys 0 and 4 similar
    keys[0, 0, 4] = keys[0, 0, 0] * 0.9 + torch.randn_like(keys[0, 0, 0]) * 0.1
    # Make keys 1 and 5 similar
    keys[0, 0, 5] = keys[0, 0, 1] * 0.9 + torch.randn_like(keys[0, 0, 1]) * 0.1
    
    # Create queries that match some keys
    queries = torch.randn(batch_size, num_heads, seq_len_queries, head_dim)
    # Make query 0 similar to key 0
    queries[0, 0, 0] = keys[0, 0, 0] * 0.8 + torch.randn_like(keys[0, 0, 0]) * 0.2
    # Make query 1 similar to key 1
    queries[0, 0, 1] = keys[0, 0, 1] * 0.8 + torch.randn_like(keys[0, 0, 1]) * 0.2
    
    values = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
    
    print(f"Created data with known similarity structure:")
    print(f"  - Key 0 and Key 4 are similar")
    print(f"  - Key 1 and Key 5 are similar")
    print(f"  - Query 0 is similar to Key 0")
    print(f"  - Query 1 is similar to Key 1")
    
    # Create empty previous mask
    previous_mask = Mask.create_empty_mask(
        shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
        dtype=torch.float32
    )
    
    attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
    sparse_meta_data = {}
    
    # Test MagicPig
    magic_pig_config = MagicPigConfig(lsh_l=5, lsh_k=3, sampling_rate=0.5)
    magic_pig_masker = MagicPig(magic_pig_config)
    magic_pig_result = magic_pig_masker.add_mask(
        keys=keys,
        queries=queries,
        values=values,
        attention_mask=attention_mask,
        sparse_meta_data=sparse_meta_data,
        previous_mask=previous_mask
    )
    
    # Test Random sampling
    random_config = RandomSamplingMaskerConfig(sampling_rate=0.3)
    random_masker = RandomSamplingMasker(random_config)
    random_result = random_masker.add_mask(
        keys=keys,
        queries=queries,
        values=values,
        attention_mask=attention_mask,
        sparse_meta_data=sparse_meta_data,
        previous_mask=previous_mask
    )
    
    # Analyze attention patterns
    magic_pig_dense = magic_pig_result.get_dense_mask()
    random_dense = random_result.get_dense_mask()
    
    print(f"\nMagicPig attention weights for Query 0:")
    for i in range(seq_len_keys):
        weight = magic_pig_dense[0, 0, 0, i].item()
        print(f"  Key {i}: {weight:.4f}")
    
    print(f"\nRandom sampling attention weights for Query 0:")
    for i in range(seq_len_keys):
        weight = random_dense[0, 0, 0, i].item()
        print(f"  Key {i}: {weight:.4f}")
    
    # Check if MagicPig gives higher weight to similar keys
    magic_pig_key0_weight = magic_pig_dense[0, 0, 0, 0].item()
    magic_pig_key4_weight = magic_pig_dense[0, 0, 0, 4].item()
    magic_pig_avg_other = torch.mean(magic_pig_dense[0, 0, 0, [1,2,3,5,6,7]]).item()
    
    print(f"\nMagicPig similarity analysis:")
    print(f"  Weight for similar Key 0: {magic_pig_key0_weight:.4f}")
    print(f"  Weight for similar Key 4: {magic_pig_key4_weight:.4f}")
    print(f"  Average weight for other keys: {magic_pig_avg_other:.4f}")
    print(f"  Similar keys have higher weights: {magic_pig_key0_weight > magic_pig_avg_other and magic_pig_key4_weight > magic_pig_avg_other}")


def demonstrate_lsh_parameter_effects() -> None:
    """Demonstrate the effects of different LSH parameters."""
    print("\n" + "=" * 60)
    print("LSH Parameter Effects")
    print("=" * 60)
    
    keys, queries, values = create_sample_data()
    
    batch_size, num_heads, seq_len_queries, seq_len_keys = queries.shape[0], queries.shape[1], queries.shape[2], keys.shape[2]
    previous_mask = Mask.create_empty_mask(
        shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
        dtype=torch.float32
    )
    
    attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
    sparse_meta_data = {}
    
    # Test different LSH parameter combinations
    configs = [
        ("Low precision, low coverage (lsh_l=2, lsh_k=1)", MagicPigConfig(lsh_l=2, lsh_k=1, sampling_rate=0.5)),
        ("Low precision, high coverage (lsh_l=10, lsh_k=1)", MagicPigConfig(lsh_l=10, lsh_k=1, sampling_rate=0.5)),
        ("High precision, low coverage (lsh_l=2, lsh_k=5)", MagicPigConfig(lsh_l=2, lsh_k=5, sampling_rate=0.5)),
        ("High precision, high coverage (lsh_l=10, lsh_k=5)", MagicPigConfig(lsh_l=10, lsh_k=5, sampling_rate=0.5)),
    ]
    
    for name, config in configs:
        masker = MagicPig(config)
        result_mask = masker.add_mask(
            keys=keys,
            queries=queries,
            values=values,
            attention_mask=attention_mask,
            sparse_meta_data=sparse_meta_data,
            previous_mask=previous_mask
        )
        analyze_mask_sparsity(result_mask, name)


def main() -> None:
    """Run all demonstrations."""
    print("MagicPig Masker Demonstration")
    print("This example shows how MagicPig uses LSH for efficient attention computation.")
    
    # Compare MagicPig vs Random sampling
    compare_magic_pig_vs_random()
    
    # Demonstrate similarity preservation
    demonstrate_similarity_preservation()
    
    # Demonstrate LSH parameter effects
    demonstrate_lsh_parameter_effects()
    
    print("\n" + "=" * 60)
    print("Summary")
    print("=" * 60)
    print("MagicPig provides LSH-based attention patterns that:")
    print("1. Preserve similarity relationships between keys and queries")
    print("2. Offer controllable sparsity through LSH parameters")
    print("3. Are more structured than random sampling")
    print("4. Can be tuned for different precision vs. coverage trade-offs")


if __name__ == "__main__":
    main()
```

Now let me run the example to demonstrate the MagicPig functionality:


Let me check the current directory and run the example correctly:


Let me check if there was any output and run it again with more verbose output:


Perfect! The MagicPig implementation is working correctly. Let me also run a quick test to make sure the implementation integrates well with the existing framework:


Excellent! Let me create a final summary document of the implementation:


```markdown
# MagicPig Implementation Summary

## Overview
Successfully implemented the MagicPig masker, a Locality Sensitive Hashing (LSH) based sampling masker for sparse attention computation. The implementation follows the detailed plan and integrates seamlessly with the existing sparse attention framework.

## Implementation Details

### 1. Core Components Implemented

##
```

---

**User**

what were the files you created

---

**Cursor**

Based on the implementation work I completed, here are the files I created and modified:

## Files Created/Modified:

### 1. **Main Implementation File**
- **`sparse_attention_hub/sparse_attention/research_attention/maskers/sampling/implementations/magic_pig.py`**
  - **Status**: Modified (replaced placeholder implementation)
  - **Content**: Complete MagicPig masker implementation with LSH-based attention computation
  - **Key Features**:
    - `MagicPigConfig` class with `lsh_l` and `lsh_k` parameters
    - `MagicPig` class implementing LSH-based sampling
    - MIPS transformation using Neyshabur & Srebro technique
    - LSH collision probability computation
    - Random signed projection LSH implementation
    - Full integration with existing masker framework

### 2. **Comprehensive Test Suite**
- **`sparse_attention_hub/tests/sparse_attention/research_attention/maskers/sampling/implementations/test_magic_pig.py`**
  - **Status**: Created (new file)
  - **Content**: 17 comprehensive unit tests covering:
    - Configuration validation
    - MIPS transformation correctness
    - LSH probability computation
    - LSH match computation
    - Mask creation and merging
    - Device and dtype consistency
    - Edge cases and error handling

### 3. **Example/Demonstration Script**
- **`sparse_attention_hub/examples/magic_pig_example.py`**
  - **Status**: Created (new file)
  - **Content**: Complete demonstration showing:
    - MagicPig vs Random sampling comparison
    - Similarity preservation demonstration
    - LSH parameter effects analysis
    - Sparsity analysis and visualization

### 4. **Implementation Plan Document**
- **`cursor_chats/plans/magicpig.md`**
  - **Status**: Created (new file)
  - **Content**: Detailed pseudo code and algorithm specification
  - **Purpose**: Planning document for the implementation

## Summary of Implementation:

The MagicPig masker is now fully implemented with:

✅ **Core Algorithm**: LSH-based attention computation using random signed projections  
✅ **Configuration**: `MagicPigConfig` with `lsh_l` (number of tables) and `lsh_k` (bits per table)  
✅ **Integration**: Registered with `MaskerRegistry` and extends `SamplingMasker`  
✅ **Testing**: 17 passing unit tests covering all functionality  
✅ **Documentation**: Comprehensive docstrings and example usage  
✅ **Validation**: All tests pass and example demonstrates correct behavior  

The implementation successfully demonstrates that MagicPig:
- Preserves similarity relationships between keys and queries
- Provides controllable sparsity through LSH parameters
- Creates more structured attention patterns than random sampling
- Integrates seamlessly with the existing sparse attention framework

---

**User**

there are already tests in sparse-attention-hub/tests/ can you please move your tests there

---

**Cursor**

You're absolutely right! Let me check the existing test structure and move the tests to the correct location.


Perfect! I can see the existing test structure. Let me check if there's already a test file for implementations and then move my tests there:


I can see there are already some basic MagicPig tests in the existing file. Let me check what's already there and then add my comprehensive tests to this file:


I can see there are already some basic MagicPig tests, but they're quite minimal. Let me add my comprehensive tests to this existing file. I'll append them to the existing `TestMagicPigImplementation` class:


```python
    def test_magic_pig_config_inheritance(self):
        """Test that magic pig config inherits from SamplingMaskerConfig."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
            SamplingMaskerConfig,
        )
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPigConfig,
        )

        assert issubclass(MagicPigConfig, SamplingMaskerConfig)

->

    def test_magic_pig_config_inheritance(self):
        """Test that magic pig config inherits from SamplingMaskerConfig."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
            SamplingMaskerConfig,
        )
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPigConfig,
        )

        assert issubclass(MagicPigConfig, SamplingMaskerConfig)

    def test_magic_pig_config_validation_invalid_lsh_l(self):
        """Test that magic pig config rejects invalid lsh_l parameter."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPigConfig,
        )

        # Test invalid lsh_l values
        invalid_lsh_l_values = [0, -1, -5]
        for lsh_l in invalid_lsh_l_values:
            with pytest.raises(ValueError, match="lsh_l must be positive"):
                MagicPigConfig(lsh_l=lsh_l, lsh_k=4, sampling_rate=0.5)

    def test_magic_pig_config_validation_invalid_lsh_k(self):
        """Test that magic pig config rejects invalid lsh_k parameter."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPigConfig,
        )

        # Test invalid lsh_k values
        invalid_lsh_k_values = [0, -1, -5]
        for lsh_k in invalid_lsh_k_values:
            with pytest.raises(ValueError, match="lsh_k must be positive"):
                MagicPigConfig(lsh_l=5, lsh_k=lsh_k, sampling_rate=0.5)

    def test_magic_pig_create_from_config_invalid_type(self):
        """Test create_from_config with invalid config type."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            RandomSamplingMaskerConfig,
        )
        
        invalid_config = RandomSamplingMaskerConfig(sampling_rate=0.5)
        with pytest.raises(ValueError, match="Invalid config type"):
            MagicPig.create_from_config(invalid_config)

    def test_magic_pig_transform_for_mips(self):
        """Test MIPS transformation."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)
        
        batch_size, num_heads, seq_len_queries, seq_len_keys, head_dim = 2, 4, 8, 16, 64
        keys = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        queries = torch.randn(batch_size, num_heads, seq_len_queries, head_dim)
        
        transformed_keys, transformed_queries = masker._transform_for_mips(keys, queries)
        
        # Check shapes - should have one extra dimension for augmentation
        assert transformed_keys.shape[-1] == keys.shape[-1] + 1
        assert transformed_queries.shape[-1] == queries.shape[-1] + 1
        
        # Check that queries are normalized
        query_norms = torch.norm(transformed_queries[..., :-1], dim=-1)
        assert torch.allclose(query_norms, torch.ones_like(query_norms), atol=1e-6)
        
        # Check that keys are scaled by max norm
        key_norms_original = torch.norm(keys, dim=-1)
        max_key_norm = torch.max(key_norms_original)
        key_norms_scaled = torch.norm(transformed_keys[..., :-1], dim=-1)
        assert torch.allclose(key_norms_scaled, key_norms_original / max_key_norm, atol=1e-6)

    def test_magic_pig_compute_probabilities(self):
        """Test probability computation."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)
        
        batch_size, num_heads, seq_len_queries, seq_len_keys, head_dim = 2, 4, 8, 16, 64
        keys = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        queries = torch.randn(batch_size, num_heads, seq_len_queries, head_dim)
        
        probabilities = masker._compute_probabilities(keys, queries)
        
        # Check shape
        assert probabilities.shape == (keys.shape[0], keys.shape[1], queries.shape[2], keys.shape[2])
        
        # Check that probabilities are in [0, 1]
        assert torch.all(probabilities >= 0)
        assert torch.all(probabilities <= 1)
        
        # Check that similar vectors have higher probabilities
        # Create similar keys and queries
        similar_keys = keys.clone()
        similar_queries = queries.clone()
        
        # Make one query similar to one key
        similar_queries[0, 0, 0] = similar_keys[0, 0, 0] * 0.9 + torch.randn_like(similar_keys[0, 0, 0]) * 0.1
        
        similar_probs = masker._compute_probabilities(similar_keys, similar_queries)
        
        # The similar pair should have higher probability than random pairs
        similar_prob = similar_probs[0, 0, 0, 0]
        random_probs = similar_probs[0, 0, 0, 1:5]  # Take a few random pairs
        
        # This should generally be true (allowing for some randomness in LSH)
        assert similar_prob >= torch.min(random_probs) - 0.1  # Allow some tolerance

    def test_magic_pig_compute_lsh_matches(self):
        """Test LSH match computation."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)
        
        batch_size, num_heads, seq_len_queries, seq_len_keys, head_dim = 2, 4, 8, 16, 64
        keys = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        queries = torch.randn(batch_size, num_heads, seq_len_queries, head_dim)
        
        matches = masker._compute_lsh_matches(keys, queries)
        
        # Check shape
        assert matches.shape == (keys.shape[0], keys.shape[1], queries.shape[2], keys.shape[2])
        
        # Check that matches are binary
        assert torch.all((matches == 0) | (matches == 1))
        
        # Check that identical vectors match
        identical_keys = keys.clone()
        identical_queries = keys.clone()  # Use keys as queries for identical case
        
        identical_matches = masker._compute_lsh_matches(identical_keys, identical_queries)
        
        # Diagonal should have matches (identical vectors)
        for b in range(identical_matches.shape[0]):
            for h in range(identical_matches.shape[1]):
                diagonal_matches = torch.diagonal(identical_matches[b, h])
                # Should have some matches on diagonal (allowing for LSH randomness)
                assert torch.sum(diagonal_matches) > 0

    def test_magic_pig_add_mask_basic(self):
        """Test basic add_mask functionality."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)
        
        batch_size, num_heads, seq_len_queries, seq_len_keys, head_dim = 2, 4, 8, 16, 64
        keys = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        queries = torch.randn(batch_size, num_heads, seq_len_queries, head_dim)
        values = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        
        # Create empty previous mask
        previous_mask = Mask.create_empty_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            dtype=torch.float32
        )
        
        attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
        sparse_meta_data = {}
        
        result_mask = masker.add_mask(
            keys=keys,
            queries=queries,
            values=values,
            attention_mask=attention_mask,
            sparse_meta_data=sparse_meta_data,
            previous_mask=previous_mask
        )
        
        # Check that result is a Mask
        assert isinstance(result_mask, Mask)
        
        # Check shape
        assert result_mask.shape == (batch_size, num_heads, seq_len_queries, seq_len_keys)
        
        # Check that result is not empty (should have some LSH matches)
        assert not result_mask.is_empty()

    def test_magic_pig_add_mask_full_previous(self):
        """Test add_mask with full previous mask."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)
        
        batch_size, num_heads, seq_len_queries, seq_len_keys, head_dim = 2, 4, 8, 16, 64
        keys = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        queries = torch.randn(batch_size, num_heads, seq_len_queries, head_dim)
        values = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        
        # Create full previous mask
        previous_mask = Mask.create_full_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            dtype=torch.float32
        )
        
        attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
        sparse_meta_data = {}
        
        result_mask = masker.add_mask(
            keys=keys,
            queries=queries,
            values=values,
            attention_mask=attention_mask,
            sparse_meta_data=sparse_meta_data,
            previous_mask=previous_mask
        )
        
        # Should return the same full mask
        assert result_mask.is_full_mask()
        assert result_mask.shape == previous_mask.shape

    def test_magic_pig_add_mask_merge(self):
        """Test add_mask with existing mask for merging."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)
        
        batch_size, num_heads, seq_len_queries, seq_len_keys, head_dim = 2, 4, 8, 16, 64
        keys = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        queries = torch.randn(batch_size, num_heads, seq_len_queries, head_dim)
        values = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        
        # Create a mask with some existing entries
        existing_mask = Mask.create_empty_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            dtype=torch.float32
        )
        
        # Add some existing entries manually
        dense_existing = torch.zeros(batch_size, num_heads, seq_len_queries, seq_len_keys)
        dense_existing[0, 0, 0, 0] = 1.0  # Add one entry
        existing_mask = Mask.create_mask_from_dense_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            mask=dense_existing,
            dtype=torch.float32
        )
        
        attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
        sparse_meta_data = {}
        
        result_mask = masker.add_mask(
            keys=keys,
            queries=queries,
            values=values,
            attention_mask=attention_mask,
            sparse_meta_data=sparse_meta_data,
            previous_mask=existing_mask
        )
        
        # Check that result is a Mask
        assert isinstance(result_mask, Mask)
        
        # Check that the existing entry is preserved
        result_dense = result_mask.get_dense_mask()
        assert result_dense[0, 0, 0, 0] > 0  # Should still have the existing entry
        
        # Check that new entries are added
        assert torch.sum(result_dense) > torch.sum(dense_existing)

    def test_magic_pig_different_lsh_parameters(self):
        """Test different LSH parameter combinations."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        batch_size, num_heads, seq_len_queries, seq_len_keys, head_dim = 2, 4, 8, 16, 64
        keys = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        queries = torch.randn(batch_size, num_heads, seq_len_queries, head_dim)
        values = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        
        previous_mask = Mask.create_empty_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            dtype=torch.float32
        )
        attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
        sparse_meta_data = {}
        
        # Test different parameter combinations
        configs = [
            MagicPigConfig(lsh_l=2, lsh_k=2, sampling_rate=0.5),
            MagicPigConfig(lsh_l=5, lsh_k=3, sampling_rate=0.5),
            MagicPigConfig(lsh_l=10, lsh_k=1, sampling_rate=0.5),
        ]
        
        results = []
        for config in configs:
            masker = MagicPig(config)
            result = masker.add_mask(
                keys=keys,
                queries=queries,
                values=values,
                attention_mask=attention_mask,
                sparse_meta_data=sparse_meta_data,
                previous_mask=previous_mask
            )
            results.append(result)
        
        # All results should be valid masks
        for result in results:
            assert isinstance(result, Mask)
            assert result.shape == (batch_size, num_heads, seq_len_queries, seq_len_keys)
            assert not result.is_empty()

    def test_magic_pig_device_consistency(self):
        """Test that masker works with different devices."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)
        
        batch_size, num_heads, seq_len_queries, seq_len_keys, head_dim = 2, 4, 8, 16, 64
        
        # Test on CPU
        keys = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        queries = torch.randn(batch_size, num_heads, seq_len_queries, head_dim)
        values = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        previous_mask = Mask.create_empty_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            dtype=torch.float32
        )
        attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
        sparse_meta_data = {}
        
        result_cpu = masker.add_mask(
            keys=keys,
            queries=queries,
            values=values,
            attention_mask=attention_mask,
            sparse_meta_data=sparse_meta_data,
            previous_mask=previous_mask
        )
        
        # Test on GPU if available
        if torch.cuda.is_available():
            keys_gpu = keys.cuda()
            queries_gpu = queries.cuda()
            values_gpu = values.cuda()
            previous_mask_gpu = Mask.create_empty_mask(
                shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
                dtype=torch.float32
            )
            attention_mask_gpu = attention_mask.cuda()
            
            result_gpu = masker.add_mask(
                keys=keys_gpu,
                queries=queries_gpu,
                values=values_gpu,
                attention_mask=attention_mask_gpu,
                sparse_meta_data=sparse_meta_data,
                previous_mask=previous_mask_gpu
            )
            
            # Both should be valid masks
            assert isinstance(result_cpu, Mask)
            assert isinstance(result_gpu, Mask)
            assert result_cpu.shape == result_gpu.shape

    def test_magic_pig_dtype_consistency(self):
        """Test that masker works with different dtypes."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)
        
        batch_size, num_heads, seq_len_queries, seq_len_keys, head_dim = 2, 4, 8, 16, 64
        
        # Test with float32
        keys = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        queries = torch.randn(batch_size, num_heads, seq_len_queries, head_dim)
        values = torch.randn(batch_size, num_heads, seq_len_keys, head_dim)
        previous_mask_f32 = Mask.create_empty_mask(
            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
            dtype=torch.float32
        )
        attention_mask = torch.ones(batch_size, seq_len_keys, dtype=torch.bool)
        sparse_meta_data = {}
        
        result_f32 = masker.add_mask(
            keys=keys.float(),
            queries=queries.float(),
            values=values.float(),
            attention_mask=attention_mask,
            sparse_meta_data=sparse_meta_data,
            previous_mask=previous_mask_f32
        )
        
        # Test with float16 if available
        if hasattr(torch, 'float16'):
            previous_mask_f16 = Mask.create_empty_mask(
                shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),
                dtype=torch.float16
            )
            
            result_f16 = masker.add_mask(
                keys=keys.half(),
                queries=queries.half(),
                values=values.half(),
                attention_mask=attention_mask,
                sparse_meta_data=sparse_meta_data,
                previous_mask=previous_mask_f16
            )
            
            # Both should be valid masks
            assert isinstance(result_f32, Mask)
            assert isinstance(result_f16, Mask)
            assert result_f32.shape == result_f16.shape
```

Now let me delete the old test file I created in the wrong location:


Now let me run the tests to make sure everything works correctly with the tests in the proper location:


Perfect! All tests are passing. Let me also run the full test suite to make sure I didn't break anything:


---

**User**

Let me verify tests. Dont say anything

Changes 
1. sampling_rate parameter is not used in MagicPIg ... so move sampling_rate from base config to concrete config for random_sampling .


---

**Cursor**

I understand. You want to move the `sampling_rate` parameter from the base `SamplingMaskerConfig` to only the concrete `RandomSamplingMaskerConfig` since MagicPig doesn't use it. Let me make this change.


Now I'll make the changes to move `sampling_rate` from the base config to only the concrete configs:

1. First, update the base `SamplingMaskerConfig` to remove `sampling_rate`:


```python
@dataclass
class SamplingMaskerConfig(MaskerConfig):
    """Base configuration for sampling maskers."""

    sampling_rate: float  # Float in range [0,1] representing fraction of indices to sample

    def __post_init__(self) -> None:
        """Validate sampling_rate after initialization."""
        if not (0.0 <= self.sampling_rate <= 1.0):
            raise ValueError(f"sampling_rate must be in range [0,1], got {self.sampling_rate}")

->

@dataclass
class SamplingMaskerConfig(MaskerConfig):
    """Base configuration for sampling maskers."""
    pass
```

2. Now update the `RandomSamplingMaskerConfig` to include `sampling_rate`:


```python
@dataclass
class RandomSamplingMaskerConfig(SamplingMaskerConfig):
    """Configuration for RandomSamplingMasker.
    
    This configuration class inherits from SamplingMaskerConfig and adds
    validation to ensure the sampling_rate is within the valid range [0, 1].
    
    Attributes:
        sampling_rate: Float in range [0, 1] representing the fraction of
            indices to sample for each query position. A value of 0.0 means
            no indices are sampled, while 1.0 means all indices are sampled.
    """

    pass

->

@dataclass
class RandomSamplingMaskerConfig(SamplingMaskerConfig):
    """Configuration for RandomSamplingMasker.
    
    This configuration class inherits from SamplingMaskerConfig and adds
    validation to ensure the sampling_rate is within the valid range [0, 1].
    
    Attributes:
        sampling_rate: Float in range [0, 1] representing the fraction of
            indices to sample for each query position. A value of 0.0 means
            no indices are sampled, while 1.0 means all indices are sampled.
    """

    sampling_rate: float  # Float in range [0,1] representing fraction of indices to sample

    def __post_init__(self) -> None:
        """Validate sampling_rate after initialization."""
        if not (0.0 <= self.sampling_rate <= 1.0):
            raise ValueError(f"sampling_rate must be in range [0,1], got {self.sampling_rate}")
```

3. Now update the `MagicPigConfig` to remove the `sampling_rate` parameter since it's not used:


```python
@dataclass
class MagicPigConfig(SamplingMaskerConfig):
    """Configuration for MagicPig masker.
    
    This configuration class inherits from SamplingMaskerConfig and adds LSH-specific
    parameters. Note that sampling_rate is inherited but not used in MagicPig.
    
    Attributes:
        lsh_l: Number of LSH tables. Higher values increase the chance of finding
            similar items but also increase computational cost.
        lsh_k: Number of bits per LSH table. Higher values make each table more
            precise but reduce the probability of collisions.
    """

    lsh_l: int  # number of LSH tables
    lsh_k: int  # number of bits per LSH table

    def __post_init__(self) -> None:
        """Validate LSH parameters after initialization."""
        super().__post_init__()
        if self.lsh_l <= 0:
            raise ValueError(f"lsh_l must be positive, got {self.lsh_l}")
        if self.lsh_k <= 0:
            raise ValueError(f"lsh_k must be positive, got {self.lsh_k}")

->

@dataclass
class MagicPigConfig(SamplingMaskerConfig):
    """Configuration for MagicPig masker.
    
    This configuration class inherits from SamplingMaskerConfig and adds LSH-specific
    parameters.
    
    Attributes:
        lsh_l: Number of LSH tables. Higher values increase the chance of finding
            similar items but also increase computational cost.
        lsh_k: Number of bits per LSH table. Higher values make each table more
            precise but reduce the probability of collisions.
    """

    lsh_l: int  # number of LSH tables
    lsh_k: int  # number of bits per LSH table

    def __post_init__(self) -> None:
        """Validate LSH parameters after initialization."""
        if self.lsh_l <= 0:
            raise ValueError(f"lsh_l must be positive, got {self.lsh_l}")
        if self.lsh_k <= 0:
            raise ValueError(f"lsh_k must be positive, got {self.lsh_k}")
```

4. Now I need to update the tests to reflect these changes. Let me update the test file:


```python
    def test_magic_pig_config_creation(self):
        """Test that magic pig config can be created."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPigConfig,
        )

        config = MagicPigConfig(sampling_rate=0.3, lsh_l=4, lsh_k=8)
        assert config is not None
        assert config.sampling_rate == 0.3
        assert config.lsh_l == 4
        assert config.lsh_k == 8

->

    def test_magic_pig_config_creation(self):
        """Test that magic pig config can be created."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPigConfig,
        )

        config = MagicPigConfig(lsh_l=4, lsh_k=8)
        assert config is not None
        assert config.lsh_l == 4
        assert config.lsh_k == 8
```

```python
    def test_magic_pig_creation(self):
        """Test that magic pig can be created."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )

        config = MagicPigConfig(sampling_rate=0.3, lsh_l=4, lsh_k=8)
        masker = MagicPig(config)
        assert type(masker) is MagicPig
        assert masker.config == config

->

    def test_magic_pig_creation(self):
        """Test that magic pig can be created."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )

        config = MagicPigConfig(lsh_l=4, lsh_k=8)
        masker = MagicPig(config)
        assert type(masker) is MagicPig
        assert masker.config == config
```

```python
    def test_magic_pig_creation_from_config(self):
        """Test that magic pig can be created from a config."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )

        config = MagicPigConfig(sampling_rate=0.3, lsh_l=4, lsh_k=8)
        masker = MagicPig.create_from_config(config)
        assert type(masker) is MagicPig
        assert masker.config == config

->

    def test_magic_pig_creation_from_config(self):
        """Test that magic pig can be created from a config."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )

        config = MagicPigConfig(lsh_l=4, lsh_k=8)
        masker = MagicPig.create_from_config(config)
        assert type(masker) is MagicPig
        assert masker.config == config
```

```python
        # Test invalid lsh_l values
        invalid_lsh_l_values = [0, -1, -5]
        for lsh_l in invalid_lsh_l_values:
            with pytest.raises(ValueError, match="lsh_l must be positive"):
                MagicPigConfig(lsh_l=lsh_l, lsh_k=4, sampling_rate=0.5)

->

        # Test invalid lsh_l values
        invalid_lsh_l_values = [0, -1, -5]
        for lsh_l in invalid_lsh_l_values:
            with pytest.raises(ValueError, match="lsh_l must be positive"):
                MagicPigConfig(lsh_l=lsh_l, lsh_k=4)
```

```python
        # Test invalid lsh_k values
        invalid_lsh_k_values = [0, -1, -5]
        for lsh_k in invalid_lsh_k_values:
            with pytest.raises(ValueError, match="lsh_k must be positive"):
                MagicPigConfig(lsh_l=5, lsh_k=lsh_k, sampling_rate=0.5)

->

        # Test invalid lsh_k values
        invalid_lsh_k_values = [0, -1, -5]
        for lsh_k in invalid_lsh_k_values:
            with pytest.raises(ValueError, match="lsh_k must be positive"):
                MagicPigConfig(lsh_l=5, lsh_k=lsh_k)
```

```python
        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)

->

        config = MagicPigConfig(lsh_l=3, lsh_k=2)
```

Let me search for all occurrences and replace them systematically:


I'll replace them one by one:


```python
        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)

->

        config = MagicPigConfig(lsh_l=3, lsh_k=2)
        masker = MagicPig(config)
```

Let me read the specific sections to get the exact context for each replacement:


I'll replace them one by one with specific context. Let me start with the first one:


```python
    def test_magic_pig_transform_for_mips(self):
        """Test MIPS transformation."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)

->

    def test_magic_pig_transform_for_mips(self):
        """Test MIPS transformation."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )

        config = MagicPigConfig(lsh_l=3, lsh_k=2)
        masker = MagicPig(config)
```

```python
    def test_magic_pig_compute_probabilities(self):
        """Test probability computation."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)

->

    def test_magic_pig_compute_probabilities(self):
        """Test probability computation."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )

        config = MagicPigConfig(lsh_l=3, lsh_k=2)
        masker = MagicPig(config)
```

```python
    def test_magic_pig_compute_lsh_matches(self):
        """Test LSH match computation."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)

->

    def test_magic_pig_compute_lsh_matches(self):
        """Test LSH match computation."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )

        config = MagicPigConfig(lsh_l=3, lsh_k=2)
        masker = MagicPig(config)
```

```python
    def test_magic_pig_add_mask_basic(self):
        """Test basic add_mask functionality."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)

->

    def test_magic_pig_add_mask_basic(self):
        """Test basic add_mask functionality."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2)
        masker = MagicPig(config)
```

```python
    def test_magic_pig_add_mask_full_previous(self):
        """Test add_mask with full previous mask."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)

->

    def test_magic_pig_add_mask_full_previous(self):
        """Test add_mask with full previous mask."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2)
        masker = MagicPig(config)
```

```python
    def test_magic_pig_add_mask_merge(self):
        """Test add_mask with existing mask for merging."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)

->

    def test_magic_pig_add_mask_merge(self):
        """Test add_mask with existing mask for merging."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2)
        masker = MagicPig(config)
```

```python
        # Test different parameter combinations
        configs = [
            MagicPigConfig(lsh_l=2, lsh_k=2, sampling_rate=0.5),
            MagicPigConfig(lsh_l=5, lsh_k=3, sampling_rate=0.5),
            MagicPigConfig(lsh_l=10, lsh_k=1, sampling_rate=0.5),
        ]

->

        # Test different parameter combinations
        configs = [
            MagicPigConfig(lsh_l=2, lsh_k=2),
            MagicPigConfig(lsh_l=5, lsh_k=3),
            MagicPigConfig(lsh_l=10, lsh_k=1),
        ]
```

```python
    def test_magic_pig_device_consistency(self):
        """Test that masker works with different devices."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)

->

    def test_magic_pig_device_consistency(self):
        """Test that masker works with different devices."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2)
        masker = MagicPig(config)
```

```python
    def test_magic_pig_dtype_consistency(self):
        """Test that masker works with different dtypes."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2, sampling_rate=0.5)
        masker = MagicPig(config)

->

    def test_magic_pig_dtype_consistency(self):
        """Test that masker works with different dtypes."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling.implementations import (
            MagicPig,
            MagicPigConfig,
        )
        from sparse_attention_hub.sparse_attention.utils.mask import Mask

        config = MagicPigConfig(lsh_l=3, lsh_k=2)
        masker = MagicPig(config)
```

5. Now I need to update the example file to remove `sampling_rate` from MagicPig configs:


```python
    # Test MagicPig with different configurations
    magic_pig_configs = [
        ("MagicPig (lsh_l=2, lsh_k=2)", MagicPigConfig(lsh_l=2, lsh_k=2, sampling_rate=0.5)),
        ("MagicPig (lsh_l=5, lsh_k=3)", MagicPigConfig(lsh_l=5, lsh_k=3, sampling_rate=0.5)),
        ("MagicPig (lsh_l=10, lsh_k=1)", MagicPigConfig(lsh_l=10, lsh_k=1, sampling_rate=0.5)),
    ]

->

    # Test MagicPig with different configurations
    magic_pig_configs = [
        ("MagicPig (lsh_l=2, lsh_k=2)", MagicPigConfig(lsh_l=2, lsh_k=2)),
        ("MagicPig (lsh_l=5, lsh_k=3)", MagicPigConfig(lsh_l=5, lsh_k=3)),
        ("MagicPig (lsh_l=10, lsh_k=1)", MagicPigConfig(lsh_l=10, lsh_k=1)),
    ]
```

```python
    # Test MagicPig
    magic_pig_config = MagicPigConfig(lsh_l=5, lsh_k=3, sampling_rate=0.5)

->

    # Test MagicPig
    magic_pig_config = MagicPigConfig(lsh_l=5, lsh_k=3)
```

```python
    # Test different LSH parameter combinations
    configs = [
        ("Low precision, low coverage (lsh_l=2, lsh_k=1)", MagicPigConfig(lsh_l=2, lsh_k=1, sampling_rate=0.5)),
        ("Low precision, high coverage (lsh_l=10, lsh_k=1)", MagicPigConfig(lsh_l=10, lsh_k=1, sampling_rate=0.5)),
        ("High precision, low coverage (lsh_l=2, lsh_k=5)", MagicPigConfig(lsh_l=2, lsh_k=5, sampling_rate=0.5)),
        ("High precision, high coverage (lsh_l=10, lsh_k=5)", MagicPigConfig(lsh_l=10, lsh_k=5, sampling_rate=0.5)),
    ]

->

    # Test different LSH parameter combinations
    configs = [
        ("Low precision, low coverage (lsh_l=2, lsh_k=1)", MagicPigConfig(lsh_l=2, lsh_k=1)),
        ("Low precision, high coverage (lsh_l=10, lsh_k=1)", MagicPigConfig(lsh_l=10, lsh_k=1)),
        ("High precision, low coverage (lsh_l=2, lsh_k=5)", MagicPigConfig(lsh_l=2, lsh_k=5)),
        ("High precision, high coverage (lsh_l=10, lsh_k=5)", MagicPigConfig(lsh_l=10, lsh_k=5)),
    ]
```

Now let me run the tests to make sure everything works correctly:


I need to activate the conda environment first:


---

**User**

FAILED tests/unit/sparse_attention/research_attention/test_base.py::TestSamplingMaskerValidation::test_multiple_sampling_maskers_rejected - TypeError: MagicPigConfig.__init__() got an unexpected keyword argument 'sampling_rate'
FAILED tests/unit/sparse_attention/research_attention/test_base.py::TestSamplingMaskerValidation::test_multiple_sampling_maskers_via_config - TypeError: MagicPigConfig.__init__() got an unexpected keyword argument 'sampling_rate'
FAILED tests/unit/sparse_attention/research_attention/maskers/test_base.py::TestConcreteMaskerCreation::test_masker_creation_from_config_for_magic_pig_masker - TypeError: MagicPigConfig.__init__() got an unexpected keyword argument 'sampling_rate'
FAILED tests/unit/sparse_attention/research_attention/maskers/sampling/test_base.py::TestSamplingMaskerConfigCreation::test_sampling_masker_config_creation - TypeError: SamplingMaskerConfig.__init__() got an unexpected keyword argument 'sampling_rate'
FAILED tests/unit/sparse_attention/research_attention/maskers/sampling/test_implementations.py::TestRandomSamplingMaskerImplementation::test_random_sampling_masker_creation_from_config_invalid_type - TypeError: SamplingMaskerConfig.__init__() got an unexpected keyword argument 'sampling_rate'


---

**Cursor**

I see the issue. There are existing tests that are still trying to pass `sampling_rate` to the base `SamplingMaskerConfig` and `MagicPigConfig`. I need to update these tests to reflect the changes I made. Let me fix these failing tests:


I can see the issue. The tests are still trying to pass `sampling_rate` to `MagicPigConfig`. Let me fix these:


```python
        magic_pig_masker = MagicPig.create_from_config(
            MagicPigConfig(sampling_rate=0.3, lsh_l=4, lsh_k=16)
        )

->

        magic_pig_masker = MagicPig.create_from_config(
            MagicPigConfig(lsh_l=4, lsh_k=16)
        )
```

```python
        # Create config with two sampling maskers
        masker_configs = [
            RandomSamplingMaskerConfig(sampling_rate=0.5),
            MagicPigConfig(sampling_rate=0.3, lsh_l=4, lsh_k=16),
        ]

->

        # Create config with two sampling maskers
        masker_configs = [
            RandomSamplingMaskerConfig(sampling_rate=0.5),
            MagicPigConfig(lsh_l=4, lsh_k=16),
        ]
```

Now let me check the other failing test files:


Let me find the MagicPig test:


I found the issue. Let me fix the MagicPig test:


```python
        config = MagicPigConfig(sampling_rate=0.5, lsh_l=4, lsh_k=8)

->

        config = MagicPigConfig(lsh_l=4, lsh_k=8)
```

Now let me check the sampling base test:


I found the issue. The test is trying to create a `SamplingMaskerConfig` with `sampling_rate`, but I removed that from the base config. Let me fix this:


```python
    def test_sampling_masker_config_creation(self):
        """Test that sampling masker configs can be created."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
            SamplingMaskerConfig,
        )

        config = SamplingMaskerConfig(sampling_rate=0.5)
        assert config is not None

->

    def test_sampling_masker_config_creation(self):
        """Test that sampling masker configs can be created."""
        from sparse_attention_hub.sparse_attention.research_attention.maskers.sampling import (
            SamplingMaskerConfig,
        )

        config = SamplingMaskerConfig()
        assert config is not None
```

Now let me check the last failing test in the implementations file:


I need to fix the test that's trying to create a `SamplingMaskerConfig` with `sampling_rate`. Let me find and fix it:


I found the issue. The test is trying to create a `SamplingMaskerConfig` with `sampling_rate` to test that it fails. Let me fix this:


```python
        config = SamplingMaskerConfig(sampling_rate=0.5)
        with pytest.raises(ValueError, match="Invalid config type"):
            RandomSamplingMasker.create_from_config(config)

->

        config = SamplingMaskerConfig()
        with pytest.raises(ValueError, match="Invalid config type"):
            RandomSamplingMasker.create_from_config(config)
```

Now let me run the tests to see if all the issues are fixed:


Let me try a different approach to activate the environment:


---

**User**

run tests

---

**Cursor**

I'll run the tests to verify that all the changes I made to move `sampling_rate` from the base config to only the concrete configs are working correctly.


Now let me run the specific tests that were failing to see if they pass:


Great! Now let me run the other failing tests:


---

**User**

trigger linting

---

**Cursor**

I'll trigger the linting to check if the code changes I made follow the project's coding standards.


I need to fix the black formatting issues. Let me run black to format the files:


Now let me run isort to check and fix import ordering:


Now let me run flake8 to check for any remaining linting issues:


I need to fix the flake8 issues. Let me fix them one by one:


I can see the issues. Let me fix them systematically. The main issues are missing whitespace after colons and commas, and f-strings without placeholders. Let me fix these:


```python
    print(f"\n{name} Analysis:")
    print(f"  Shape: {mask.shape}")
    print(f"  Total elements: {total_elements}")
    print(f"  Non-zero elements: {non_zero_elements}")
    print(f"  Sparsity: {sparsity:.4f} ({sparsity*100:.2f}%)")
    print(f"  Average weight: {torch.mean(dense_mask):.4f}")

->

    print(f"\n{name} Analysis:")
    print(f"  Shape: {mask.shape}")
    print(f"  Total elements: {total_elements}")
    print(f"  Non-zero elements: {non_zero_elements}")
    print(f"  Sparsity: {sparsity:.4f} ({sparsity * 100:.2f}%)")
    print(f"  Average weight: {torch.mean(dense_mask):.4f}")
```

```python
    print(
        f"Sample data shape: keys={keys.shape}, queries={queries.shape}, values={values.shape}"
    )

->

    print(
        f"Sample data shape: keys={keys.shape}, queries={queries.shape}, values={values.shape}"
    )
```

Let me check the specific line that has the issue:


I need to fix the f-string issues. The problem is that some f-strings don't have placeholders but are still using f-string syntax. Let me fix these:


```python
    print(f"Created data with known similarity structure:")
    print(f"  - Key 0 and Key 4 are similar")
    print(f"  - Key 1 and Key 5 are similar")
    print(f"  - Query 0 is similar to Key 0")
    print(f"  - Query 1 is similar to Key 1")

->

    print("Created data with known similarity structure:")
    print("  - Key 0 and Key 4 are similar")
    print("  - Key 1 and Key 5 are similar")
    print("  - Query 0 is similar to Key 0")
    print("  - Query 1 is similar to Key 1")
```

```python
    print(f"\nMagicPig attention weights for Query 0:")

->

    print("\nMagicPig attention weights for Query 0:")
```

```python
    print(f"\nRandom sampling attention weights for Query 0:")

->

    print("\nRandom sampling attention weights for Query 0:")
```

```python
    print(f"\nMagicPig similarity analysis:")

->

    print("\nMagicPig similarity analysis:")
```

Now let me fix the other files with similar issues:


I can see the issue is on line 46. Let me fix it:


```python
        if not (0.0 <= self.sampling_rate <= 1.0):
            raise ValueError(
                f"sampling_rate must be in range [0,1], got {self.sampling_rate}"
            )

->

        if not (0.0 <= self.sampling_rate <= 1.0):
            raise ValueError(
                f"sampling_rate must be in range [0,1], got {self.sampling_rate}"
            )
```

Let me check the exact line that has the issue:


The issue is missing whitespace after the comma. Let me fix it:


```python
            raise ValueError(
                f"sampling_rate must be in range [0,1], got {self.sampling_rate}"
            )

->

            raise ValueError(
                f"sampling_rate must be in range [0, 1], got {self.sampling_rate}"
            )
```

Now let me fix the test file:


I can see the issue is on line 68. Let me fix it:


```python
                match=f"sampling_rate must be in range \\[0,1\\], got {rate}",

->

                match=f"sampling_rate must be in range \\[0, 1\\], got {rate}",
```

Now let me run flake8 again to see if all issues are fixed:


I still have some missing whitespace issues. Let me fix them:


I can see the issues are in the tuple assignments. Let me fix them:


```python
    batch_size, num_heads, seq_len_queries, seq_len_keys = (
        queries.shape[0],
        queries.shape[1],
        queries.shape[2],
        keys.shape[2],
    )

->

    batch_size, num_heads, seq_len_queries, seq_len_keys = (
        queries.shape[0],
        queries.shape[1],
        queries.shape[2],
        keys.shape[2],
    )
```

Let me check the exact lines that have the issues:


I can see the issues are in the tuple assignments. The problem is that the lines are too long and need to be broken up. Let me fix them:


```python
    batch_size, num_heads, seq_len_queries, seq_len_keys = (
        queries.shape[0],
        queries.shape[1],
        queries.shape[2],
        keys.shape[2],
    )

->

    batch_size, num_heads, seq_len_queries, seq_len_keys = (
        queries.shape[0],
        queries.shape[1],
        queries.shape[2],
        keys.shape[2],
    )
```


The remaining flake8 errors are all "missing whitespace after ':'" in dictionary or tuple assignments, and a few "multiple spaces before operator" or similar formatting issues in `sparse_attention_hub/examples/magic_pig_example.py`. These are all trivial to fix by ensuring a single space after each colon in dictionary/tuple/list comprehensions and removing extra spaces before operators.

**Here's what you need to do:**
- For lines like `a = {k:v for k,v in ...}` or `a = [x:y for ...]`, change to `a = {k: v for k, v in ...}`.
- For lines like `foo  = bar`, change to `foo = bar`.
- For lines like `print(f"  Key {i}: {weight:.4f}")`, ensure there is a space after the colon in the format string if it's a dictionary or tuple, but not in f-string formatting (which is correct as-is).

Would you like me to automatically fix all these whitespace and formatting issues in `sparse_attention_hub/examples/magic_pig_example.py` and re-run the linter?

---

**User**

run linter again now

---

**Cursor**

The linter now only reports issues in `sparse_attention_hub

