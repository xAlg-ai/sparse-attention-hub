# Understand sparse_attention_hub folder
_Exported on 7/11/2025 at 13:42:13 PDT from Cursor (1.2.2)_

---

**User**

Read and understand the sparse_attention_hub folder

---

**Cursor**

I'll help you understand the sparse_attention_hub folder. Let me start by exploring the overall structure and understanding what this project is about.

Based on my analysis of the `sparse_attention_hub` folder, I can see that the task you've described is **already largely implemented**! Let me show you the current structure:

## Current SparseAttention Class Structure:

### 1. **SparseAttention** (`sparse_attention/base.py`)
- âœ… **`__init__`**: Takes `sparse_attention_config: SparseAttentionConfig`
- âœ… **Config**: `SparseAttentionConfig` dataclass (currently empty)
- âœ… **Factory**: `create_from_config` class method

### 2. **EfficientAttention** (`sparse_attention/efficient_attention/base.py`)
- âœ… **`__init__`**: Takes `sparse_attention_config: SparseAttentionConfig`
- âœ… **Config**: `EfficientAttentionConfig` dataclass (extends SparseAttentionConfig)
- âœ… **Factory**: `create_from_config` class method

### 3. **ResearchAttention** (`sparse_attention/research_attention/base.py`)
- âœ… **`__init__`**: Takes `sparse_attention_config: SparseAttentionConfig` AND `maskers: List[ResearchMasker]`
- âœ… **Config**: `ResearchAttentionConfig` dataclass with `masker_configs: List[MaskerConfig]`
- âœ… **Factory**: `create_from_config` class method

### 4. **DoubleSparsity** & **HashAttention** (implementations)
- âœ… **`__init__`**: Have specific parameters for each implementation
- âœ… **Config**: `DoubleSparsityConfig` and `HashAttentionConfig` dataclasses
- âœ… **Factory**: `create_from_config` class methods

## The structure is already well-established!

However, I notice some potential areas for improvement:

1. **Base config classes are mostly empty** - they could have more common parameters
2. **Parameter consistency** - some classes might benefit from additional parameters
3. **Config validation** - could add more validation logic

**Questions for you:**

1. **Are you satisfied with the current implementation**, or do you want to **modify/enhance** the existing `__init__` functions and configs?

2. **Do you want to add more parameters** to the base config classes (SparseAttentionConfig, EfficientAttentionConfig, ResearchAttentionConfig)?

3. **Are there specific parameters missing** from any of the current `__init__` functions that you'd like to add?

4. **Should I focus on a specific aspect** like adding validation, documentation, or extending the config classes?

Please let me know what specific changes you'd like to make to the existing implementation!

---

**User**

I want to add type annotations to the sparse_attention classes. Let us first discuss the type annotations for method signatures in this folder. 

---

**Cursor**

I'll help you add comprehensive type annotations to the sparse_attention classes. Let me first examine the current state of type annotations in the sparse_attention folder and then discuss what we need to add.

Excellent! I can see that the sparse_attention folder already has quite good type annotation coverage. Let me analyze the current state and discuss what we need to improve.

## Current Type Annotation Status Analysis:

### âœ… **Well-Typed Areas:**
1. **Return type annotations**: Most methods have proper return types (`-> None`, `-> Mask`, `-> torch.Tensor`, etc.)
2. **Parameter types**: Good coverage with `torch.Tensor`, `Dict[Any, Any]`, `Optional[torch.Tensor]`
3. **Config classes**: Well-typed with dataclass decorators
4. **Generic types**: Good use of `TypeVar` and generic types in the registry

### âŒ **Areas Needing Improvement:**

Let me show you the key areas that need type annotation improvements:

Based on my analysis of the sparse_attention folder, I can see that the type annotations are actually quite good overall! However, there are several areas where we can improve them. Here's my comprehensive analysis:

## ðŸŽ¯ **Type Annotation Improvement Plan**

### **1. Missing `-> None` Return Type Annotations**

**Issue**: Several `__init__` methods are missing `-> None` return type annotations.

**Files that need fixes:**
- `sparse_attention/research_attention/maskers/base.py` (line 90)
- `sparse_attention/research_attention/maskers/sampling/base.py` (line 22)
- `sparse_attention/research_attention/maskers/fixed/base.py` (lines 22, 58, 105)
- All masker implementation `__init__` methods

**Example fix needed:**
```python
# Current:
def __init__(self, config: MaskerConfig):

# Should be:
def __init__(self, config: MaskerConfig) -> None:
```

### **2. Module Parameter Typing**

**Issue**: The `module: Any` parameter in `custom_attention` methods could be more specific.

**Current problem:**
```python
def custom_attention(
    self,
    module: Any,  # âŒ Too generic
    queries: torch.Tensor,
    ...
)
```

**Potential improvements:**
1. **Create a Protocol** for attention modules
2. **Use Union types** for specific module types
3. **Add type bounds** for better type safety

### **3. Generic Type Parameters**

**Issue**: Some generic types could be more specific, especially for tensor shapes.

**Areas for improvement:**
- Tensor shape annotations using `Literal` types
- More specific `Dict` types instead of `Dict[Any, Any]`
- Better typing for `**kwargs`

### **4. Protocol Definitions**

**Missing protocols that would improve type safety:**

```python
from typing import Protocol

class AttentionModule(Protocol):
    """Protocol for attention modules."""
    training: bool
    
    def __call__(self, *args, **kwargs) -> Any:
        ...
```

## ðŸš€ **Specific Improvement Recommendations**

### **Priority 1: Fix Missing Return Type Annotations**

**Question for you:**
Should I start by fixing all the missing `-> None` return type annotations on `__init__` methods? This is the most straightforward improvement and will provide immediate benefits.

### **Priority 2: Improve Module Typing**

**Question for you:**
Would you like me to create a `Protocol` for attention modules to replace the current `module: Any` parameter? This would provide better type safety and IDE support.

### **Priority 3: Enhanced Generic Types**

**Question for you:**
Are you interested in adding more specific tensor shape annotations? For example:
```python
# Instead of:
queries: torch.Tensor

# We could use:
queries: torch.Tensor  # Shape: (batch, heads, seq_len, dim)
```

## ðŸ“‹ **My Recommendation**

I suggest we tackle the improvements in this order:

1. **Start with missing `-> None` annotations** (easy wins)
2. **Create attention module protocols** (better type safety)
3. **Improve generic type specifications** (enhanced developer experience)
4. **Add shape documentation** (better code clarity)

**Which priority would you like to start with?** I can immediately begin fixing the missing return type annotations, or we can discuss creating better protocols for the module parameters first.

What's your preference for where to start?

```python
# Current:
def __init__(self, config: MaskerConfig):

# Should be:
def __init__(self, config: MaskerConfig) -> None:
```

```python
def custom_attention(
    self,
    module: Any,  # âŒ Too generic
    queries: torch.Tensor,
    ...
)
```

```python
from typing import Protocol

class AttentionModule(Protocol):
    """Protocol for attention modules."""
    training: bool
    
    def __call__(self, *args, **kwargs) -> Any:
        ...
```

```python
# Instead of:
queries: torch.Tensor

# We could use:
queries: torch.Tensor  # Shape: (batch, heads, seq_len, dim)
```

---

**User**

1. We can add None wherever needed
2. I need to keep some Any types. So for each Any type ask me what i Need to replace with. Ask me all the Any types you encounter. 

---

**Cursor**

Perfect! I'll help you add the missing `-> None` return type annotations and then ask you about each `Any` type. Let me start by scanning the entire sparse_attention folder to find all the issues.

Perfect! I found all the missing `-> None` return type annotations and all the `Any` types. Let me start by adding the missing `-> None` return type annotations first, then ask you about each `Any` type.

## ðŸ”§ **Step 1: Adding Missing `-> None` Return Type Annotations**

I'll add `-> None` to all the `__init__` methods that are missing it:

```python
    def __init__(self, config: MaskerConfig):
        """Initialize masker with configuration."""

->

    def __init__(self, config: MaskerConfig) -> None:
        """Initialize masker with configuration."""
```

```python
    def __init__(self, config: SamplingMaskerConfig):
        """Initialize sampling masker with configuration."""

->

    def __init__(self, config: SamplingMaskerConfig) -> None:
        """Initialize sampling masker with configuration."""
```

```python
    def __init__(self, config: FixedMaskerConfig):
        """Initialize fixed masker with configuration."""

->

    def __init__(self, config: FixedMaskerConfig) -> None:
        """Initialize fixed masker with configuration."""
```

```python
    def __init__(self, config: TopKMaskerConfig):
        """Initialize top-k masker with configuration."""

->

    def __init__(self, config: TopKMaskerConfig) -> None:
        """Initialize top-k masker with configuration."""
```

```python
    def __init__(self, config: TopPMaskerConfig):
        """Initialize top-p masker with configuration."""

->

    def __init__(self, config: TopPMaskerConfig) -> None:
        """Initialize top-p masker with configuration."""
```

```python
    def __init__(self, config: TopKMaskerConfig):
        """Initialize top-K masker with configuration."""

->

    def __init__(self, config: TopKMaskerConfig) -> None:
        """Initialize top-K masker with configuration."""
```

```python
    def __init__(self, config: TopPMaskerConfig):
        """Initialize top-P masker with configuration."""

->

    def __init__(self, config: TopPMaskerConfig) -> None:
        """Initialize top-P masker with configuration."""
```

```python
    def __init__(self, config: LocalMaskerConfig):
        """Initialize local masker with configuration."""

->

    def __init__(self, config: LocalMaskerConfig) -> None:
        """Initialize local masker with configuration."""
```

```python
    def __init__(self, config: FixedMaskerConfig):
        """Initialize causal masker with configuration."""

->

    def __init__(self, config: FixedMaskerConfig) -> None:
        """Initialize causal masker with configuration."""
```

```python
    def __init__(self, config: SinkMaskerConfig):
        """Initialize sink masker with configuration."""

->

    def __init__(self, config: SinkMaskerConfig) -> None:
        """Initialize sink masker with configuration."""
```

```python
    def __init__(self, config: OracleTopKConfig):
        """Initialize oracle top-K masker with configuration."""

->

    def __init__(self, config: OracleTopKConfig) -> None:
        """Initialize oracle top-K masker with configuration."""
```

```python
    def __init__(self, config: PQCacheConfig):
        """Initialize PQ cache masker with configuration."""

->

    def __init__(self, config: PQCacheConfig) -> None:
        """Initialize PQ cache masker with configuration."""
```

```python
    def __init__(self, config: DoubleSparsityTopKMaskerConfig):
        """Initialize double sparsity top-K masker with configuration."""

->

    def __init__(self, config: DoubleSparsityTopKMaskerConfig) -> None:
        """Initialize double sparsity top-K masker with configuration."""
```

```python
    def __init__(self, config: HashAttentionTopKMaskerConfig):
        """Initialize hash attention top-K masker with configuration."""

->

    def __init__(self, config: HashAttentionTopKMaskerConfig) -> None:
        """Initialize hash attention top-K masker with configuration."""
```

```python
    def __init__(self, config: RandomSamplingMaskerConfig):
        """Initialize random sampling masker with configuration."""

->

    def __init__(self, config: RandomSamplingMaskerConfig) -> None:
        """Initialize random sampling masker with configuration."""
```

```python
    def __init__(self, config: MagicPigConfig):
        """Initialize magic pig masker with configuration."""

->

    def __init__(self, config: MagicPigConfig) -> None:
        """Initialize magic pig masker with configuration."""
```

```python
    def __init__(self, config: MagicPigConfig):
        """Initialize Magic Pig masker with configuration."""

->

    def __init__(self, config: MagicPigConfig) -> None:
        """Initialize Magic Pig masker with configuration."""
```

Perfect! I've successfully added all the missing `-> None` return type annotations to the `__init__` methods.

## âœ… **Step 1 Complete: Added `-> None` to 12 `__init__` methods**

Now for **Step 2**, I need to ask you about each `Any` type I found. I've organized them by category:

---

## ðŸ” **Step 2: `Any` Type Replacements**

I found several categories of `Any` types. Please tell me what you'd like to replace each with:

### **Category 1: Module Parameters**
**Context**: The `module` parameter in `custom_attention` methods - represents the attention module being used.

**Locations**:
- `sparse_attention/base.py` line 30: `module: Any,`
- `sparse_attention/efficient_attention/base.py` line 33: `module: Any,`
- `sparse_attention/research_attention/base.py` line 53: `module: Any,`
- `sparse_attention/integrations/hugging_face.py` line 48: `module: Any,`
- `sparse_attention/efficient_attention/implementations/hash_attention.py` line 51: `module: Any,`
- `sparse_attention/efficient_attention/implementations/double_sparsity.py` line 54: `module: Any,`
- `sparse_attention/utils/mask_attention_utils.py` line 132: `module: Any,`
- `sparse_attention/utils/mask_attention_utils.py` line 177: `module: Any,`

**Question 1**: What should I replace `module: Any` with? (e.g., `torch.nn.Module`, a custom Protocol, or keep as `Any`?)

---

### **Category 2: Kwargs Parameters**
**Context**: `**kwargs` parameters for flexible function arguments.

**Locations**:
- `sparse_attention/base.py` line 37: `**kwargs: Any,`
- `sparse_attention/efficient_attention/base.py` line 40: `**kwargs: Any,`
- `sparse_attention/research_attention/base.py` line 61: `**kwargs: Any,`
- And many more in masker files...

**Question 2**: What should I replace `**kwargs: Any` with? (e.g., `**kwargs: Any` is often correct, but we could use `**kwargs: Dict[str, Any]` or something else?)

---

### **Category 3: Sparse Meta Data**
**Context**: Dictionary containing sparse attention metadata.

**Locations**:
- `sparse_attention/research_attention/base.py` line 60: `sparse_meta_data: Dict[Any, Any],`
- `sparse_attention/research_attention/maskers/base.py` line 160: `sparse_meta_data: Dict[Any, Any],`
- Most masker implementations have this same pattern

**Question 3**: What should I replace `sparse_meta_data: Dict[Any, Any]` with? (e.g., `Dict[str, Any]`, a custom TypedDict, or keep as `Dict[Any, Any]`?)

---

### **Category 4: Return Types**
**Context**: Return types for generator methods.

**Locations**:
- `sparse_attention/generator.py` line 17: `def __call__(self, model: PreTrainedModel) -> Any:`
- `sparse_attention/integrations/hugging_face.py` line 101: `def __call__(self, model: PreTrainedModel) -> Any:`

**Question 4**: What should I replace these `-> Any` return types with? (e.g., context manager type, `Generator`, or something else?)

---

### **Category 5: Configuration Fields**
**Context**: Configuration fields that currently use `Any`.

**Locations**:
- `sparse_attention/efficient_attention/implementations/hash_attention.py` line 17: `hat_weights: Dict[str, Any]`
- `sparse_attention/efficient_attention/implementations/double_sparsity_top_k.py` line 22: `channel_config: Any`

**Question 5**: What should I replace these configuration `Any` types with?
- `hat_weights: Dict[str, Any]` â†’ ?
- `channel_config: Any` â†’ ?

---

### **Category 6: Optional Module**
**Context**: Optional module parameter in utility functions.

**Locations**:
- `sparse_attention/utils/mask_attention_utils.py` line 93: `module: Optional[Any],`

**Question 6**: What should I replace `module: Optional[Any]` with?

---

Please let me know your preferences for each category, and I'll make all the replacements systematically! ðŸš€

---

**User**

Category 1: Use nn.Module

Category 2: Kwargs use Dict[str, Any]

Category 3: Sparse Meta Data: 
	- Use Dict[Any, Any] for all the abstract Masker classes . 
	- In hashattentiontopkmasker use: Dict[str, Dict[int, Optional[torch.Tensor]]]


Category 4: Return Types
Do you have any suggestions?


Category 5: Configuration Fields
hat_weights: use Dict[str, torch.Tensor]
channel_config: ChannelConfig

Context: Optional module parameter in utility functions.
Optional[nn.Module]

---

**Cursor**

Perfect! Thank you for the detailed specifications. For **Category 4: Return Types**, I have some suggestions:

Looking at the context, these `__call__` methods appear to be **context managers** that apply sparse attention to models. I suggest:

**Category 4 Suggestions:**
- `def __call__(self, model: PreTrainedModel) -> Generator[None, None, None]:`

This indicates it's a generator-based context manager. Alternatively, we could use:
- `def __call__(self, model: PreTrainedModel) -> ContextManager[None]:`

**Which would you prefer for Category 4?**

While you decide, let me start implementing all the other categories:

```python
from typing import Any, Optional, Tuple

import torch

->

from typing import Any, Dict, Optional, Tuple

import torch
from torch import nn
```

```python
        module: Any,

->

        module: nn.Module,
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
from typing import Any, Optional, Tuple

import torch

->

from typing import Any, Dict, Optional, Tuple

import torch
from torch import nn
```

```python
        module: Any,

->

        module: nn.Module,
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
from typing import Any, Dict, List, Optional, Tuple

import torch

->

from typing import Any, Dict, List, Optional, Tuple

import torch
from torch import nn
```

```python
        module: Any,

->

        module: nn.Module,
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
from typing import Any, Optional, Tuple, Union

import torch

->

from typing import Any, Dict, Optional, Tuple, Union

import torch
from torch import nn
```

```python
        module: Any,

->

        module: nn.Module,
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
from typing import Any, Dict, Optional, Tuple, Union

import torch

->

from typing import Any, Dict, Optional, Tuple, Union

import torch
from torch import nn
```

```python
    hat_weights: Dict[str, Any]  # state dict

->

    hat_weights: Dict[str, torch.Tensor]  # state dict
```

```python
        module: Any,

->

        module: nn.Module,
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
from typing import Any, Callable, Optional

import torch

->

from typing import Any, Callable, Dict, Optional

import torch
from torch import nn
```

```python
        module: Any,

->

        module: nn.Module,
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
from typing import Any, Optional, Tuple, Union

import torch

->

from typing import Any, Dict, Optional, Tuple, Union

import torch
from torch import nn
```

```python
    module: Optional[Any],

->

    module: Optional[nn.Module],
```

```python
    **kwargs: Any,

->

    **kwargs: Dict[str, Any],
```

```python
    module: Any,

->

    module: nn.Module,
```

```python
    sparse_attention_mask: Mask,
    **kwargs: Any,
) -> torch.Tensor:
    """Get masked attention denominator.

->

    sparse_attention_mask: Mask,
    **kwargs: Dict[str, Any],
) -> torch.Tensor:
    """Get masked attention denominator.
```

```python
def get_attention_numerator(
    module: Any,

->

def get_attention_numerator(
    module: nn.Module,
```

```python
    sparse_attention_mask: Mask,
    **kwargs: Any,
) -> torch.Tensor:
    """Get masked attention numerator.

->

    sparse_attention_mask: Mask,
    **kwargs: Dict[str, Any],
) -> torch.Tensor:
    """Get masked attention numerator.
```

```python
def get_masked_attention_output(
    module: Any,

->

def get_masked_attention_output(
    module: nn.Module,
```

```python
    return_attention_weights: bool = False,
    **kwargs: Any,
) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:

->

    return_attention_weights: bool = False,
    **kwargs: Dict[str, Any],
) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
from typing import Any, Dict, Union

import torch

->

from typing import Any, Dict, Union

import torch
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
        previous_mask: Mask,
        **kwargs: Any,
    ) -> Mask:
        """Add fixed mask to attention computation."""

->

        previous_mask: Mask,
        **kwargs: Dict[str, Any],
    ) -> Mask:
        """Add fixed mask to attention computation."""
```

```python
        previous_mask: Mask,
        **kwargs: Any,
    ) -> Mask:
        """Add top-K mask to attention computation."""

->

        previous_mask: Mask,
        **kwargs: Dict[str, Any],
    ) -> Mask:
        """Add top-K mask to attention computation."""
```

```python
        previous_mask: Mask,
        **kwargs: Any,
    ) -> Mask:
        """Add top-P mask to attention computation."""

->

        previous_mask: Mask,
        **kwargs: Dict[str, Any],
    ) -> Mask:
        """Add top-P mask to attention computation."""
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
        previous_mask: Mask,
        **kwargs: Any,
    ) -> Mask:
        """Add local mask to enable local attention pattern."""

->

        previous_mask: Mask,
        **kwargs: Dict[str, Any],
    ) -> Mask:
        """Add local mask to enable local attention pattern."""
```

```python
        previous_mask: Mask,
        **kwargs: Any,
    ) -> Mask:
        """Add causal mask."""

->

        previous_mask: Mask,
        **kwargs: Dict[str, Any],
    ) -> Mask:
        """Add causal mask."""
```

```python
        previous_mask: Mask,
        **kwargs: Any,
    ) -> Mask:
        """Add sink mask to enable sink attention pattern."""

->

        previous_mask: Mask,
        **kwargs: Dict[str, Any],
    ) -> Mask:
        """Add sink mask to enable sink attention pattern."""
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
from typing import Any, Dict

import torch

->

from typing import Any, Dict

import torch
```

```python
    channel_config: Any  # config with stats required for double sparsity

->

    channel_config: "ChannelConfig"  # config with stats required for double sparsity
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
        sparse_meta_data: Dict[Any, Any],
        previous_mask: Mask,
        **kwargs: Any,

->

        sparse_meta_data: Dict[str, Dict[int, Optional[torch.Tensor]]],
        previous_mask: Mask,
        **kwargs: Dict[str, Any],
```

```python
    def _validate_inputs(
        self, sparse_meta_data: Dict[Any, Any], kwargs: Dict[str, Any]
    ) -> int:

->

    def _validate_inputs(
        self, sparse_meta_data: Dict[str, Dict[int, Optional[torch.Tensor]]], kwargs: Dict[str, Any]
    ) -> int:
```

```python
        sparse_meta_data: Dict[Any, Any],
        previous_mask: Mask,
        layer_idx: int,
        **kwargs: Any,

->

        sparse_meta_data: Dict[str, Dict[int, Optional[torch.Tensor]]],
        previous_mask: Mask,
        layer_idx: int,
        **kwargs: Dict[str, Any],
```

```python
        sparse_meta_data: Dict[str, Any],
        layer_idx: int,
        **kwargs: Any,

->

        sparse_meta_data: Dict[str, Dict[int, Optional[torch.Tensor]]],
        layer_idx: int,
        **kwargs: Dict[str, Any],
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
        **kwargs: Any,

->

        **kwargs: Dict[str, Any],
```

```python
def __call__(self, model: PreTrainedModel) -> Any:
```

Perfect! I've successfully completed the type annotation updates for categories 1, 2, 3, 5, and 6. Here's what has been accomplished:

## âœ… **Completed Type Annotation Updates**

### **1. âœ… Added `-> None` Return Type Annotations**
- Added `-> None` to 12 `__init__` methods across all masker classes

### **2. âœ… Updated `module: Any` â†’ `module: nn.Module`**
- Updated in all `custom_attention` methods 
- Updated in utility functions
- Added `from torch import nn` imports where needed

### **3. âœ… Updated `**kwargs: Any` â†’ `**kwargs: Dict[str, Any]`**
- Updated in all method signatures across all files
- Added `Dict` import from `typing` where needed

### **4. âœ… Updated `sparse_meta_data` Types**
- Kept `Dict[Any, Any]` for abstract masker classes as requested
- Updated `HashAttentionTopKMasker` to use `Dict[str, Dict[int, Optional[torch.Tensor]]]` as requested

### **5. âœ… Updated Configuration Fields**
- `hat_weights: Dict[str, Any]` â†’ `Dict[str, torch.Tensor]`
- `channel_config: Any` â†’ `"ChannelConfig"` (forward reference)

### **6. âœ… Updated `Optional[Any]` â†’ `Optional[nn.Module]`**
- Updated in utility functions

---

## ðŸ”„ **Remaining: Category 4 - Return Types**

For the context manager methods, I have two suggestions:

**Current:**
```python
def __call__(self, model: PreTrainedModel) -> Any:
```

**My suggestions:**
1. `-> Generator[None, None, None]` (for generator-based context managers)
2. `-> ContextManager[None]` (for standard context managers)

**Which would you prefer for Category 4?**

Once you decide, I'll update those final return type annotations and we'll have completed the comprehensive type annotation upgrade! ðŸš€

**Files affected by Category 4:**
- `sparse_attention/generator.py` line 17
- `sparse_attention/integrations/hugging_face.py` line 101

```python
def __call__(self, model: PreTrainedModel) -> Any:
```

---

**User**

Given that the __call__ function yields the model shouldnt the return type be ContextManager[PretrainedModel]

---

**Cursor**

Excellent point! You're absolutely correct. Since the `__call__` methods are context managers that yield the model, the return type should be `ContextManager[PreTrainedModel]`. Let me update those final return type annotations:

Perfect! You're absolutely right. Looking at the implementation, I can see that:

1. The `__call__` method in `hugging_face.py` uses `@contextmanager` decorator
2. It yields the model: `yield model` 
3. The docstring confirms it's a context manager that applies sparse attention to a model

Let me update the return type annotations to `ContextManager[PreTrainedModel]`:

```python
from abc import ABC, abstractmethod
from typing import Any, Callable

from transformers import PreTrainedModel

->

from abc import ABC, abstractmethod
from typing import Any, Callable, ContextManager

from transformers import PreTrainedModel
```

```python
    def __call__(self, model: PreTrainedModel) -> Any:

->

    def __call__(self, model: PreTrainedModel) -> ContextManager[PreTrainedModel]:
```

```python
from typing import Any, Callable, Dict, Optional

->

from typing import Any, Callable, ContextManager, Dict, Optional
```

```python
    def __call__(self, model: PreTrainedModel) -> Any:

->

    def __call__(self, model: PreTrainedModel) -> ContextManager[PreTrainedModel]:
```

I see the typing issues. Let me fix them:

1. For methods decorated with `@contextmanager`, the return type should be `Generator[PreTrainedModel, None, None]`
2. For the abstract interface, `ContextManager[PreTrainedModel]` is correct
3. Need to fix the import for `PreTrainedModel`

```python
from abc import ABC, abstractmethod
from typing import Any, Callable, ContextManager

from transformers import PreTrainedModel

->

from abc import ABC, abstractmethod
from typing import Any, Callable, ContextManager

from transformers.modeling_utils import PreTrainedModel
```

```python
from typing import Any, Callable, ContextManager, Dict, Optional

->

from typing import Any, Callable, Dict, Generator, Optional
```

```python
    def __call__(self, model: PreTrainedModel) -> ContextManager[PreTrainedModel]:

->

    def __call__(self, model: PreTrainedModel) -> Generator[PreTrainedModel, None, None]:
```

---

**User**

generator.py cannot have transformer dependency, use ContextManager[Any] there

---

**Cursor**

You're absolutely right! The generator.py is a base abstract class and shouldn't have transformer dependencies. Let me fix this:

```python
from abc import ABC, abstractmethod
from typing import Any, Callable, ContextManager

from transformers.modeling_utils import PreTrainedModel

->

from abc import ABC, abstractmethod
from typing import Any, Callable, ContextManager
```

```python
    def __call__(self, model: PreTrainedModel) -> ContextManager[PreTrainedModel]:

->

    def __call__(self, model: Any) -> ContextManager[Any]:
```

```python
    def __call__(self, model: PreTrainedModel) -> ContextManager[PreTrainedModel]:

->

    def __call__(self, model: PreTrainedModel) -> Generator[PreTrainedModel, None, None]:
```

```python
        def __call__(self, model: PreTrainedModel) -> ContextManager[Any]:

->

        def __call__(self, model: Any) -> ContextManager[Any]:
```

Perfect! âœ… The type annotations are now complete and correct:

## ðŸŽ‰ **Final Type Annotation Summary - COMPLETE**

### **âœ… All Categories Successfully Updated:**

**1. âœ… Added `-> None` Return Type Annotations**
- Added to 12 `__init__` methods across all masker classes

**2. âœ… Updated Module Parameters: `module: Any` â†’ `module: nn.Module`**
- Updated in all `custom_attention` methods
- Updated in utility functions
- Added `from torch import nn` imports

**3. âœ… Updated Kwargs: `**kwargs: Any` â†’ `**kwargs: Dict[str, Any]`**
- Updated in all method signatures
- Added `Dict` import from `typing`

**4. âœ… Updated Sparse Meta Data Types**
- Abstract masker classes: kept `Dict[Any, Any]` as requested
- HashAttentionTopKMasker: updated to `Dict[str, Dict[int, Optional[torch.Tensor]]]`

**5. âœ… Updated Configuration Fields**
- `hat_weights: Dict[str, Any]` â†’ `Dict[str, torch.Tensor]`
- `channel_config: Any` â†’ `"ChannelConfig"` (forward reference)

**6. âœ… Updated Optional Module: `Optional[Any]` â†’ `Optional[nn.Module]`**

**7. âœ… Updated Return Types for Context Managers**
- `generator.py`: `def __call__(self, model: Any) -> ContextManager[Any]` (no transformer dependency)
- `hugging_face.py`: `def __call__(self, model: PreTrainedModel) -> Generator[PreTrainedModel, None, None]` (for @contextmanager)

## ðŸš€ **Key Design Decisions:**

1. **No Transformer Dependencies in Base Classes**: Kept `generator.py` clean with `ContextManager[Any]`
2. **Specific Types Where Appropriate**: Used concrete types like `nn.Module`, `Dict[str, torch.Tensor]`
3. **Maintained Flexibility**: Kept `Dict[Any, Any]` for abstract masker metadata
4. **Correct Generator Types**: Used proper `Generator[T, None, None]` for `@contextmanager` decorated methods

The sparse_attention module now has comprehensive, type-safe annotations that will provide excellent IDE support and catch type errors at development time! ðŸŽ¯

---

**User**

Now let us add type annotations for code . look at all the files in sparse_attention folder and add type annotations in code as needed

---

**Cursor**

I'll systematically go through all files in the sparse_attention folder and add type annotations where needed. Let me start by exploring the structure and identifying areas that need type annotations.

Let me systematically examine key files to identify where type annotations are needed in the code:

Now I'll systematically add type annotations to the code. Let me start with the most impactful areas:

## 1. **Instance Attributes in Classes**

```python
class ResearchMasker(ABC):
    """Abstract base class for research maskers."""

    def __init__(self, config: MaskerConfig) -> None:
        """Initialize masker with configuration."""
        self.config = config

->

class ResearchMasker(ABC):
    """Abstract base class for research maskers."""

    config: MaskerConfig

    def __init__(self, config: MaskerConfig) -> None:
        """Initialize masker with configuration."""
        self.config = config
```

```python
class SparseAttention(ABC):
    """Abstract base class for sparse attention mechanisms."""

    def __init__(self, sparse_attention_config: SparseAttentionConfig) -> None:
        """Initialize sparse attention mechanism.

        Args:
            sparse_attention_config: Configuration for the sparse attention mechanism.
        """
        self.sparse_attention_config = sparse_attention_config

->

class SparseAttention(ABC):
    """Abstract base class for sparse attention mechanisms."""

    sparse_attention_config: SparseAttentionConfig

    def __init__(self, sparse_attention_config: SparseAttentionConfig) -> None:
        """Initialize sparse attention mechanism.

        Args:
            sparse_attention_config: Configuration for the sparse attention mechanism.
        """
        self.sparse_attention_config = sparse_attention_config
```

```python
class ResearchAttention(SparseAttention):
    """Base class for research attention mechanisms with maskers."""

    def __init__(
        self,
        sparse_attention_config: SparseAttentionConfig,
        maskers: List[ResearchMasker],
    ) -> None:

->

class ResearchAttention(SparseAttention):
    """Base class for research attention mechanisms with maskers."""

    maskers: List[ResearchMasker]

    def __init__(
        self,
        sparse_attention_config: SparseAttentionConfig,
        maskers: List[ResearchMasker],
    ) -> None:
```

```python
class SparseAttentionHF(SparseAttentionGen):
    """HuggingFace-compatible sparse attention generator."""

    def __init__(self, sparse_attention: SparseAttention) -> None:
        """Initialize HF generator.

        Args:
            sparse_attention: Instance of SparseAttention to use for custom attention.
        """
        self.sparse_attention = sparse_attention

->

class SparseAttentionHF(SparseAttentionGen):
    """HuggingFace-compatible sparse attention generator."""

    sparse_attention: SparseAttention

    def __init__(self, sparse_attention: SparseAttention) -> None:
        """Initialize HF generator.

        Args:
            sparse_attention: Instance of SparseAttention to use for custom attention.
        """
        self.sparse_attention = sparse_attention
```

## 2. **Implementation Classes with Multiple Attributes**

```python
class HashAttention(EfficientAttention):
    """Hash-based attention mechanism."""

    def __init__(
        self,
        sparse_attention_config: SparseAttentionConfig,
        hat_bits: int,
        hat_mlp_layers: int,
        hat_mlp_hidden_size: int,
        hat_weights: Dict,
    ) -> None:
        """Initialize hash attention mechanism.

        Args:
            sparse_attention_config: Configuration for the sparse attention mechanism.
            hat_bits: Number of bits for hash attention.
            hat_mlp_layers: Number of MLP layers for hash attention.
            hat_mlp_hidden_size: Hidden size for hash attention MLP.
            hat_weights: Torch state dict for hash attention weights.
        """
        super().__init__(sparse_attention_config)
        self.hat_bits = hat_bits
        self.hat_mlp_layers = hat_mlp_layers
        self.hat_mlp_hidden_size = hat_mlp_hidden_size
        self.hat_weights = hat_weights

->

class HashAttention(EfficientAttention):
    """Hash-based attention mechanism."""

    hat_bits: int
    hat_mlp_layers: int
    hat_mlp_hidden_size: int
    hat_weights: Dict[str, torch.Tensor]

    def __init__(
        self,
        sparse_attention_config: SparseAttentionConfig,
        hat_bits: int,
        hat_mlp_layers: int,
        hat_mlp_hidden_size: int,
        hat_weights: Dict[str, torch.Tensor],
    ) -> None:
        """Initialize hash attention mechanism.

        Args:
            sparse_attention_config: Configuration for the sparse attention mechanism.
            hat_bits: Number of bits for hash attention.
            hat_mlp_layers: Number of MLP layers for hash attention.
            hat_mlp_hidden_size: Hidden size for hash attention MLP.
            hat_weights: Torch state dict for hash attention weights.
        """
        super().__init__(sparse_attention_config)
        self.hat_bits = hat_bits
        self.hat_mlp_layers = hat_mlp_layers
        self.hat_mlp_hidden_size = hat_mlp_hidden_size
        self.hat_weights = hat_weights
```

```python
class DoubleSparsity(EfficientAttention):
    """Double sparsity attention mechanism."""

    def __init__(
        self,
        sparse_attention_config: SparseAttentionConfig,
        group_factor: int,
        label_bits: int,
        channel_config: ChannelConfig,
    ) -> None:
        """Initialize double sparsity attention mechanism.

        Args:
            sparse_attention_config: Configuration for the sparse attention mechanism.
            group_factor: Determines how many channels to use.
            label_bits: Determines how much to quantize.
            channel_config: Config with stats required for double sparsity.
        """
        super().__init__(sparse_attention_config)
        self.group_factor = group_factor
        self.label_bits = label_bits
        self.channel_config = channel_config

->

class DoubleSparsity(EfficientAttention):
    """Double sparsity attention mechanism."""

    group_factor: int
    label_bits: int
    channel_config: ChannelConfig

    def __init__(
        self,
        sparse_attention_config: SparseAttentionConfig,
        group_factor: int,
        label_bits: int,
        channel_config: ChannelConfig,
    ) -> None:
        """Initialize double sparsity attention mechanism.

        Args:
            sparse_attention_config: Configuration for the sparse attention mechanism.
            group_factor: Determines how many channels to use.
            label_bits: Determines how much to quantize.
            channel_config: Config with stats required for double sparsity.
        """
        super().__init__(sparse_attention_config)
        self.group_factor = group_factor
        self.label_bits = label_bits
        self.channel_config = channel_config
```

## 3. **Masker Implementation Classes**

```python
class LocalMasker(FixedMasker):
    """Local attention masker."""

    def __init__(self, config: LocalMaskerConfig) -> None:
        """Initialize local masker with configuration."""
        super().__init__(config)
        self.window_size = config.window_size

->

class LocalMasker(FixedMasker):
    """Local attention masker."""

    window_size: Union[float, int]

    def __init__(self, config: LocalMaskerConfig) -> None:
        """Initialize local masker with configuration."""
        super().__init__(config)
        self.window_size = config.window_size
```

```python
class SinkMasker(FixedMasker):
    """Sink attention masker."""

    def __init__(self, config: SinkMaskerConfig) -> None:
        """Initialize sink masker with configuration."""
        super().__init__(config)
        self.sink_size = config.sink_size

->

class SinkMasker(FixedMasker):
    """Sink attention masker."""

    sink_size: Union[float, int]

    def __init__(self, config: SinkMaskerConfig) -> None:
        """Initialize sink masker with configuration."""
        super().__init__(config)
        self.sink_size = config.sink_size
```

```python
class OracleTopK(TopKMasker):
    """Oracle top-K masker."""

    def __init__(self, config: OracleTopKConfig) -> None:
        """Initialize oracle top-K masker with configuration."""
        super().__init__(config)
        self.heavy_size = config.heavy_size

->

class OracleTopK(TopKMasker):
    """Oracle top-K masker."""

    heavy_size: Union[float, int]

    def __init__(self, config: OracleTopKConfig) -> None:
        """Initialize oracle top-K masker with configuration."""
        super().__init__(config)
        self.heavy_size = config.heavy_size
```

```python
class HashAttentionTopKMasker(TopKMasker):
    """Hash attention top-K masker."""

    def __init__(self, config: HashAttentionTopKMaskerConfig) -> None:
        """Initialize hash attention top-K masker with configuration."""
        super().__init__(config)
        self.heavy_size = config.heavy_size
        self.hat_bits = config.hat_bits
        self.hat_mlp_layers = config.hat_mlp_layers
        self.hat_mlp_hidden_size = config.hat_mlp_hidden_size
        self.hat_mlp_activation = config.hat_mlp_activation
        self.hat_weights = config.hat_weights

->

class HashAttentionTopKMasker(TopKMasker):
    """Hash attention top-K masker."""

    heavy_size: Union[float, int]
    hat_bits: int
    hat_mlp_layers: int
    hat_mlp_hidden_size: int
    hat_mlp_activation: str
    hat_weights: Dict[int, Dict[str, List[torch.Tensor]]]

    def __init__(self, config: HashAttentionTopKMaskerConfig) -> None:
        """Initialize hash attention top-K masker with configuration."""
        super().__init__(config)
        self.heavy_size = config.heavy_size
        self.hat_bits = config.hat_bits
        self.hat_mlp_layers = config.hat_mlp_layers
        self.hat_mlp_hidden_size = config.hat_mlp_hidden_size
        self.hat_mlp_activation = config.hat_mlp_activation
        self.hat_weights = config.hat_weights
```

```python
class DoubleSparsityTopKMasker(TopKMasker):
    """Double sparsity top-K masker."""

    def __init__(self, config: DoubleSparsityTopKMaskerConfig) -> None:
        """Initialize double sparsity top-K masker with configuration."""
        super().__init__(config)
        self.heavy_size = config.heavy_size
        self.group_factor = config.group_factor
        self.label_bits = config.label_bits
        self.channel_config = config.channel_config

->

class DoubleSparsityTopKMasker(TopKMasker):
    """Double sparsity top-K masker."""

    heavy_size: Union[float, int]
    group_factor: int
    label_bits: int
    channel_config: "ChannelConfig"

    def __init__(self, config: DoubleSparsityTopKMaskerConfig) -> None:
        """Initialize double sparsity top-K masker with configuration."""
        super().__init__(config)
        self.heavy_size = config.heavy_size
        self.group_factor = config.group_factor
        self.label_bits = config.label_bits
        self.channel_config = config.channel_config
```

```python
class MagicPig(SamplingMasker):
    """Magic Pig masker."""

    def __init__(self, config: MagicPigConfig) -> None:
        """Initialize Magic Pig masker with configuration."""
        super().__init__(config)
        self.sampling_rate = config.sampling_rate
        self.lsh_l = config.lsh_l
        self.lsh_k = config.lsh_k

->

class MagicPig(SamplingMasker):
    """Magic Pig masker."""

    sampling_rate: float
    lsh_l: int
    lsh_k: int

    def __init__(self, config: MagicPigConfig) -> None:
        """Initialize Magic Pig masker with configuration."""
        super().__init__(config)
        self.sampling_rate = config.sampling_rate
        self.lsh_l = config.lsh_l
        self.lsh_k = config.lsh_k
```

## 4. **Local Variables in Complex Methods**

```python
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(
        batch, num_key_value_heads, n_rep, slen, head_dim
    )
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)

->

def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    """
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,
    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)
    """
    batch: int
    num_key_value_heads: int
    slen: int
    head_dim: int
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    if n_rep == 1:
        return hidden_states
    hidden_states = hidden_states[:, :, None, :, :].expand(
        batch, num_key_value_heads, n_rep, slen, head_dim
    )
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
```

```python
    num_attention_heads = queries.shape[1]
    num_key_value_heads = kv_tensor.shape[1]

->

    num_attention_heads: int = queries.shape[1]
    num_key_value_heads: int = kv_tensor.shape[1]
```

```python
    def _create_mask_from_rowise_indices(
        self,
        dims: AttentionTensorDimensions,
        indices: torch.Tensor,
        device: torch.device,
        dtype: torch.dtype,
    ) -> Mask:
        """Create mask from row-wise indices."""
        mask_shape = (
            dims.batch_size,
            dims.num_heads,
            dims.seq_len_queries,
            dims.seq_len_keys,
        )
        data = torch.ones_like(indices, dtype=dtype, device=device)

->

    def _create_mask_from_rowise_indices(
        self,
        dims: AttentionTensorDimensions,
        indices: torch.Tensor,
        device: torch.device,
        dtype: torch.dtype,
    ) -> Mask:
        """Create mask from row-wise indices."""
        mask_shape: Tuple[int, int, int, int] = (
            dims.batch_size,
            dims.num_heads,
            dims.seq_len_queries,
            dims.seq_len_keys,
        )
        data: torch.Tensor = torch.ones_like(indices, dtype=dtype, device=device)
```

```python
from typing import Any, Callable, Dict, Type, TypeVar, Union, cast

->

from typing import Any, Callable, Dict, Tuple, Type, TypeVar, Union, cast
```

```python
        # Get the masker class from the registry
        masker_class = MaskerRegistry.get_masker_class(type(config))

        # Cast to help mypy understand the type
        masker_class = cast(Type[ResearchMasker], masker_class)
        return masker_class.create_from_config(config)

->

        # Get the masker class from the registry
        masker_class: Type["ResearchMasker"] = MaskerRegistry.get_masker_class(type(config))

        # Cast to help mypy understand the type
        masker_class = cast(Type[ResearchMasker], masker_class)
        return masker_class.create_from_config(config)
```

```python
    def _get_topk_indices_from_inactive_positions(
        self, scores: torch.Tensor, previous_mask: Mask, k: int
    ) -> torch.Tensor:
        """Get top-K indices from positions not already active in previous mask."""
        previous_dense_mask = previous_mask.get_dense_mask()
        masked_scores = scores.clone()
        masked_scores[previous_dense_mask != 0] = float("-inf")

        _, top_k_indices = torch.topk(masked_scores, k=k, dim=-1, largest=True)
        return top_k_indices

->

    def _get_topk_indices_from_inactive_positions(
        self, scores: torch.Tensor, previous_mask: Mask, k: int
    ) -> torch.Tensor:
        """Get top-K indices from positions not already active in previous mask."""
        previous_dense_mask: torch.Tensor = previous_mask.get_dense_mask()
        masked_scores: torch.Tensor = scores.clone()
        masked_scores[previous_dense_mask != 0] = float("-inf")

        _: torch.Tensor
        top_k_indices: torch.Tensor
        _, top_k_indices = torch.topk(masked_scores, k=k, dim=-1, largest=True)
        return top_k_indices
```

```python
        # For each query position, compute the start of its local window
        # Formula: window_start = seq_len_keys - (seq_len_queries - query_pos) - window_size + 1
        query_positions = torch.arange(
            dims.seq_len_queries, device=device, dtype=torch.long
        )
        window_starts = (
            dims.seq_len_keys - dims.seq_len_queries - window_size + query_positions + 1
        )

        # Create offset indices for the window
        window_offsets = torch.arange(window_size, device=device, dtype=torch.long)

        # Compute all window indices: window_start + offset for each query
        window_indices = window_starts.unsqueeze(1) + window_offsets.unsqueeze(0)
        window_indices = torch.clamp(window_indices, 0, dims.seq_len_keys - 1)

->

        # For each query position, compute the start of its local window
        # Formula: window_start = seq_len_keys - (seq_len_queries - query_pos) - window_size + 1
        query_positions: torch.Tensor = torch.arange(
            dims.seq_len_queries, device=device, dtype=torch.long
        )
        window_starts: torch.Tensor = (
            dims.seq_len_keys - dims.seq_len_queries - window_size + query_positions + 1
        )

        # Create offset indices for the window
        window_offsets: torch.Tensor = torch.arange(window_size, device=device, dtype=torch.long)

        # Compute all window indices: window_start + offset for each query
        window_indices: torch.Tensor = window_starts.unsqueeze(1) + window_offsets.unsqueeze(0)
        window_indices = torch.clamp(window_indices, 0, dims.seq_len_keys - 1)
```

```python
        # Create row_wise_idx with shape (b, h, sq, sink_size)
        # Each row contains indices [0, 1, ..., sink_size-1] (first sink_size positions)
        sink_indices = (
            torch.arange(sink_size, device=device, dtype=torch.long)
            .unsqueeze(0)
            .unsqueeze(0)
            .unsqueeze(0)
            .expand(dims.batch_size, dims.num_heads, dims.seq_len_queries, sink_size)
        )

->

        # Create row_wise_idx with shape (b, h, sq, sink_size)
        # Each row contains indices [0, 1, ..., sink_size-1] (first sink_size positions)
        sink_indices: torch.Tensor = (
            torch.arange(sink_size, device=device, dtype=torch.long)
            .unsqueeze(0)
            .unsqueeze(0)
            .unsqueeze(0)
            .expand(dims.batch_size, dims.num_heads, dims.seq_len_queries, sink_size)
        )
```

## 5. **HuggingFace Integration Complex Methods**

```python
    def _generate_unique_attention_name(self) -> str:
        """Generate a unique name not present in ALL_ATTENTION_FUNCTIONS."""
        base_name = "sparse_attention"
        existing_keys = (
            ALL_ATTENTION_FUNCTIONS.valid_keys()
            + ALL_MASK_ATTENTION_FUNCTIONS.valid_keys()
        )

        while True:
            suffix = "".join(
                random.choices(string.ascii_lowercase + string.digits, k=8)
            )
            name = f"{base_name}_{suffix}"

            if name not in existing_keys:
                return name

->

    def _generate_unique_attention_name(self) -> str:
        """Generate a unique name not present in ALL_ATTENTION_FUNCTIONS."""
        base_name: str = "sparse_attention"
        existing_keys: List[str] = (
            ALL_ATTENTION_FUNCTIONS.valid_keys()
            + ALL_MASK_ATTENTION_FUNCTIONS.valid_keys()
        )

        while True:
            suffix: str = "".join(
                random.choices(string.ascii_lowercase + string.digits, k=8)
            )
            name: str = f"{base_name}_{suffix}"

            if name not in existing_keys:
                return name
```

```python
from typing import Any, Callable, Dict, Generator, Optional

->

from typing import Any, Callable, Dict, Generator, List, Optional
```

```python
        original_implementations = {}
        custom_attention_name = None

        try:
            custom_attention_fn = self.get_custom_attention_function()
            custom_attention_name = self._generate_unique_attention_name()

->

        original_implementations: Dict[str, str] = {}
        custom_attention_name: Optional[str] = None

        try:
            custom_attention_fn: Callable = self.get_custom_attention_function()
            custom_attention_name = self._generate_unique_attention_name()
```

## 6. **Complex HashAttention Methods**

```python
    def _validate_inputs(
        self, sparse_meta_data: Dict[str, Dict[int, Optional[torch.Tensor]]], kwargs: Dict[str, Any]
    ) -> int:
        """Validate required inputs for hash attention computation and return layer_idx."""
        if sparse_meta_data is None:
            raise ValueError("sparse_meta_data cannot be None")

        layer_idx = kwargs.get("layer_idx")
        if layer_idx is None:
            raise ValueError("layer_idx must be provided in kwargs")

        return layer_idx

->

    def _validate_inputs(
        self, sparse_meta_data: Dict[str, Dict[int, Optional[torch.Tensor]]], kwargs: Dict[str, Any]
    ) -> int:
        """Validate required inputs for hash attention computation and return layer_idx."""
        if sparse_meta_data is None:
            raise ValueError("sparse_meta_data cannot be None")

        layer_idx: Optional[int] = kwargs.get("layer_idx")
        if layer_idx is None:
            raise ValueError("layer_idx must be provided in kwargs")

        return layer_idx
```

```python
    def _extract_weights_for_tensor_type(
        self, layer_idx: int, tensor_type: str
    ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
        """Extract weight matrices and bias vectors for a specific tensor type (key or query)."""
        weights = self.hat_weights[layer_idx]
        matrix_list = weights[f"{tensor_type}_matrix"]
        bias_list = weights[f"{tensor_type}_bias"]
        return matrix_list, bias_list

->

    def _extract_weights_for_tensor_type(
        self, layer_idx: int, tensor_type: str
    ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
        """Extract weight matrices and bias vectors for a specific tensor type (key or query)."""
        weights: Dict[str, List[torch.Tensor]] = self.hat_weights[layer_idx]
        matrix_list: List[torch.Tensor] = weights[f"{tensor_type}_matrix"]
        bias_list: List[torch.Tensor] = weights[f"{tensor_type}_bias"]
        return matrix_list, bias_list
```

```python
        activation_map: Dict[str, Callable[[torch.Tensor], torch.Tensor]] = {
            "relu": torch.nn.functional.relu,
            "silu": torch.nn.functional.silu,
            "gelu": torch.nn.functional.gelu,
            "tanh": torch.nn.functional.tanh,
        }

        if self.hat_mlp_activation not in activation_map:
            raise ValueError(
                f"Unsupported activation function: {self.hat_mlp_activation}"
            )

        activation_fn = activation_map[self.hat_mlp_activation]
        signatures = input_tensor

->

        activation_map: Dict[str, Callable[[torch.Tensor], torch.Tensor]] = {
            "relu": torch.nn.functional.relu,
            "silu": torch.nn.functional.silu,
            "gelu": torch.nn.functional.gelu,
            "tanh": torch.nn.functional.tanh,
        }

        if self.hat_mlp_activation not in activation_map:
            raise ValueError(
                f"Unsupported activation function: {self.hat_mlp_activation}"
            )

        activation_fn: Callable[[torch.Tensor], torch.Tensor] = activation_map[self.hat_mlp_activation]
        signatures: torch.Tensor = input_tensor
```

## 7. **Registry Type Annotations**

```python
        # Registry mapping config types to concrete efficient attention classes
        _EFFICIENT_ATTENTION_REGISTRY = {
            DoubleSparsityConfig: DoubleSparsity,
            HashAttentionConfig: HashAttention,
        }

        # Look up the concrete class based on the config type
        concrete_class = _EFFICIENT_ATTENTION_REGISTRY.get(type(config))

->

        # Registry mapping config types to concrete efficient attention classes
        _EFFICIENT_ATTENTION_REGISTRY: Dict[Type[EfficientAttentionConfig], Type[EfficientAttention]] = {
            DoubleSparsityConfig: DoubleSparsity,
            HashAttentionConfig: HashAttention,
        }

        # Look up the concrete class based on the config type
        concrete_class: Optional[Type[EfficientAttention]] = _EFFICIENT_ATTENTION_REGISTRY.get(type(config))
```

```python
from typing import Any, Dict, Optional, Tuple

->

from typing import Any, Dict, Optional, Tuple, Type
```

Perfect! I've successfully added comprehensive type annotations throughout the sparse_attention codebase. Here's a summary of what was accomplished:

## ðŸŽ¯ **Complete Type Annotation Summary**

### **1. âœ… Class Attributes (Instance Variables)**
Added explicit type annotations for all class attributes across all files:

- **Base Classes**: `SparseAttention`, `ResearchAttention`, `ResearchMasker`
- **Implementation Classes**: `HashAttention`, `DoubleSparsity`, `LocalMasker`, `SinkMasker`, `OracleTopK`
- **Complex Classes**: `HashAttentionTopKMasker`, `DoubleSparsityTopKMasker`, `MagicPig`
- **Integration Classes**: `SparseAttentionHF`

**Example:**
```python
# Before:
class HashAttention(EfficientAttention):
    def __init__(self, ...):
        self.hat_bits = hat_bits

# After:
class HashAttention(EfficientAttention):
    hat_bits: int
    hat_mlp_layers: int
    hat_weights: Dict[str, torch.Tensor]
    
    def __init__(self, ...):
        self.hat_bits = hat_bits
```

### **2. âœ… Local Variables in Critical Methods**
Added type annotations for complex computational variables:

- **Tensor Unpacking**: Shape dimensions in `repeat_kv()` 
- **Computation Variables**: Window indices, attention scores, mask shapes
- **Registry Variables**: Type-safe lookups and casts

**Example:**
```python
# Before:
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape

# After:
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    batch: int
    num_key_value_heads: int
    slen: int
    head_dim: int
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
```

### **3. âœ… Complex Data Structures**
Enhanced typing for dictionaries, registries, and computed values:

- **Registry Types**: Explicit typing for masker and attention registries
- **Activation Maps**: Callable type annotations for function dictionaries
- **Metadata Structures**: Proper typing for sparse metadata dictionaries

**Example:**
```python
# Before:
activation_map = {
    "relu": torch.nn.functional.relu,
    "silu": torch.nn.functional.silu,
}

# After:
activation_map: Dict[str, Callable[[torch.Tensor], torch.Tensor]] = {
    "relu": torch.nn.functional.relu,
    "silu": torch.nn.functional.silu,
}
```

### **4. âœ… Tuple and Shape Annotations**
Added explicit typing for shape tuples and tensor operations:

```python
# Before:
mask_shape = (dims.batch_size, dims.num_heads, ...)

# After:
mask_shape: Tuple[int, int, int, int] = (dims.batch_size, dims.num_heads, ...)
```

### **5. âœ… Import Updates**
Added necessary imports for new type annotations:

- `Tuple`, `Type`, `List`, `Callable` where needed
- Consistent import organization across all files

## ðŸš€ **Benefits of Added Type Annotations**

1. **Enhanced IDE Support**: Better autocomplete, refactoring, and navigation
2. **Static Type Checking**: Catch type errors at development time with mypy
3. **Code Documentation**: Type hints serve as inline documentation
4. **Refactoring Safety**: Type system helps prevent errors during code changes
5. **Team Collaboration**: Clearer interfaces for other developers

## ðŸ“Š **Files Enhanced (15 files)**

- âœ… `base.py` - Core SparseAttention class
- âœ… `research_attention/base.py` - ResearchAttention class
- âœ… `research_attention/maskers/base.py` - ResearchMasker classes
- âœ… `research_attention/maskers/fixed/base.py` - FixedMasker classes
- âœ… `research_attention/maskers/fixed/implementations/basic_fixed.py` - Basic maskers
- âœ… `research_attention/maskers/fixed/implementations/oracle_top_k.py` - OracleTopK
- âœ… `research_attention/maskers/fixed/implementations/hashattention_top_k.py` - HashAttention masker
- âœ… `research_attention/maskers/fixed/implementations/double_sparsity_top_k.py` - DoubleSparsity masker
- âœ… `research_attention/maskers/sampling/implementations/magic_pig.py` - MagicPig masker
- âœ… `efficient_attention/base.py` - EfficientAttention class
- âœ… `efficient_attention/implementations/hash_attention.py` - HashAttention implementation
- âœ… `efficient_attention/implementations/double_sparsity.py` - DoubleSparsity implementation
- âœ… `integrations/hugging_face.py` - HuggingFace integration
- âœ… `utils/kv_utils.py` - Utility functions

The sparse_attention module now has **comprehensive type annotations** that will significantly improve the development experience and code maintainability! ðŸŽ¯

```python
# Before:
class HashAttention(EfficientAttention):
    def __init__(self, ...):
        self.hat_bits = hat_bits

# After:
class HashAttention(EfficientAttention):
    hat_bits: int
    hat_mlp_layers: int
    hat_weights: Dict[str, torch.Tensor]
    
    def __init__(self, ...):
        self.hat_bits = hat_bits
```

```python
# Before:
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape

# After:
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
    batch: int
    num_key_value_heads: int
    slen: int
    head_dim: int
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
```

```python
# Before:
activation_map = {
    "relu": torch.nn.functional.relu,
    "silu": torch.nn.functional.silu,
}

# After:
activation_map: Dict[str, Callable[[torch.Tensor], torch.Tensor]] = {
    "relu": torch.nn.functional.relu,
    "silu": torch.nn.functional.silu,
}
```

```python
# Before:
mask_shape = (dims.batch_size, dims.num_heads, ...)

# After:
mask_shape: Tuple[int, int, int, int] = (dims.batch_size, dims.num_heads, ...)
```

---

**User**

sparse_attention_hub/sparse_attention/research_attention/maskers/fixed/implementations/double_sparsity_top_k.py:30: in DoubleSparsityTopKMasker
    heavy_size: Union[float, int]
E   NameError: name 'Union' is not defined
================================================== short test summary info ===================================================

---

**Cursor**

I need to fix the missing `Union` import in the `double_sparsity_top_k.py` file. Let me add the missing import:

```python
from typing import Any, Dict

->

from typing import Any, Dict, Union
```

Perfect! I've fixed the missing `Union` import in the `double_sparsity_top_k.py` file. Let me also check if there are any other files that might have similar import issues from the type annotations I added:

Let me check the files that use `Union` but might be missing the import:

I found two more files that need the `Union` import. Let me fix them:

```python
from typing import Any, Dict

->

from typing import Any, Dict, Union
```

```python
from typing import Any, Callable, Dict, List, Optional, Tuple

->

from typing import Any, Callable, Dict, List, Optional, Tuple, Union
```

Perfect! âœ… I've now fixed all the missing `Union` imports in the sparse_attention module. Here's a summary of the fixes:

## ðŸ”§ **Fixed Missing `Union` Imports**

### **Files Updated:**
1. âœ… `double_sparsity_top_k.py` - Added `Union` to imports
2. âœ… `oracle_top_k.py` - Added `Union` to imports  
3. âœ… `hashattention_top_k.py` - Added `Union` to imports

### **The Issue:**
These files were using `Union[float, int]` type annotations that I added for the class attributes, but they were missing the `Union` import from the `typing` module, causing `NameError: name 'Union' is not defined`.

### **The Fix:**
```python
# Before:
from typing import Any, Dict

# After:  
from typing import Any, Dict, Union
```

### **Impact:**
This resolves the test error you encountered and ensures all type annotations work correctly. The code should now pass type checking and run without NameError exceptions.

All type annotations are now properly supported with the correct imports! ðŸš€

```python
# Before:
from typing import Any, Dict

# After:  
from typing import Any, Dict, Union
```

