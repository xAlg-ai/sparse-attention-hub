{"id": "19bd1697-0dc8-406e-ac3e-57081832b817", "code": "\"\"\"OpenEvolve masker implementation.\n\nThis module provides the OpenEvolve masker that implements evolutionary attention patterns.\nThe current implementation is a bare metal version that returns the previous mask.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nimport torch\nfrom scipy.stats import norm\n\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers import (\n    ResearchMasker,\n)\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers.base import (\n    MaskerConfig,\n    MaskerRegistry,\n)\nfrom sparse_attention_hub.sparse_attention.utils.mask import Mask\nfrom sparse_attention_hub.sparse_attention.utils.mask_attention_utils import (\n    _get_num_key_value_groups,\n    apply_inv_mask_sum,\n    create_sampling_mask_with_per_head_budget,\n    repeat_kv,\n)\n\n\n@dataclass\nclass OpenEvolveMaskerConfig(MaskerConfig):\n    \"\"\"Configuration for OpenEvolveMasker.\n\n    This configuration class inherits from MaskerConfig and provides\n    parameters for the attention mechanism evolved out of openevolve.\n    empty placeholder\n    \"\"\"\n\n    pass\n\n\n@MaskerRegistry.register(OpenEvolveMaskerConfig)\nclass OpenEvolveMasker(ResearchMasker):\n    \"\"\"OpenEvolve masker for evolutionary attention computation.\n\n    This masker implements evolutionary attention patterns that adapt over time.\n    The current implementation is a bare metal version that returns the previous mask.\n\n    Attributes:\n        evolution_rate: The rate of evolution for attention patterns.\n            This value is set from the configuration and controls how quickly\n            the attention patterns evolve.\n\n    Important Notes:\n        - This is a bare metal implementation that simply returns the previous mask.\n        - Future implementations will include evolutionary algorithms for attention pattern optimization.\n        - The evolution_rate parameter is currently unused but will be utilized in future versions.\n\n    Example:\n        >>> config = OpenEvolveMaskerConfig(evolution_rate=1.0)\n        >>> masker = OpenEvolveMasker(config)\n        >>> # Use masker.add_mask() to apply evolutionary attention patterns\n    \"\"\"\n\n    def __init__(self, config: OpenEvolveMaskerConfig) -> None:\n        \"\"\"Initialize OpenEvolve masker with configuration.\n\n        Args:\n            config: Configuration object containing the evolution rate and other\n                parameters for the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the evolution_rate in config is negative.\n                This validation is performed in the config's __post_init__ method.\n        \"\"\"\n        self.base_rate_sampling = 0.01\n        self.epsilon = 0.3\n        self.delta = 0.3\n        self.init_offset = 0.001\n        self.local_offset = 0.001\n\n        super().__init__(config)\n\n    def _compute_exp_attention_scores(\n        self,\n        queries: torch.Tensor,\n        keys: torch.Tensor,\n        scaling: float,\n        attention_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute exponential attention scores with numerical stability.\"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        raw_scores = torch.matmul(queries, keys.transpose(-2, -1)) * scaling\n        if attention_mask is not None:\n            raw_scores = raw_scores + attention_mask[:, :, :, : keys.shape[-2]]\n        max_scores = torch.max(raw_scores, dim=-1, keepdim=True)[0]\n        return torch.exp(raw_scores - max_scores)\n\n    def _get_sampling_range(self, seq_len_keys: int) -> tuple[int, int, int]:\n        \"\"\"Get sampling range and validate it.\n\n        Args:\n            seq_len_keys: Number of keys in the sequence.\n\n        Returns:\n            Tuple of (start_idx, end_idx, sampling_range).\n\n        Raises:\n            ValueError: If the computed sampling range is invalid.\n        \"\"\"\n        # Compute start index\n        if isinstance(self.init_offset, float):\n            start_idx: int = int(self.init_offset * seq_len_keys)\n        else:\n            start_idx = self.init_offset\n\n        # Compute end index\n        if isinstance(self.local_offset, float):\n            end_idx: int = seq_len_keys - int(self.local_offset * seq_len_keys)\n        else:\n            end_idx = seq_len_keys - self.local_offset\n\n        sampling_range = end_idx - start_idx\n\n        if sampling_range <= 0:\n            raise ValueError(f\"Invalid sampling range: {sampling_range}\")\n\n        return start_idx, end_idx, sampling_range\n\n    def _get_base_sample_count(self, sampling_range: int) -> int:\n        \"\"\"Get number of base samples based on configuration.\"\"\"\n        # Ensure at least 2 samples since it is used for std estimation\n        if isinstance(self.base_rate_sampling, int):\n            return max(2, self.base_rate_sampling)\n        return max(2, int(self.base_rate_sampling * sampling_range))\n\n    def _get_std_estimate_using_base_sample(\n        self,\n        expwts: torch.Tensor,\n        batch_size: int,\n        num_heads: int,\n        seq_len_queries: int,\n        seq_len_keys: int,\n        start_idx: int,\n        end_idx: int,\n        num_base_samples: int,\n        dtype: torch.dtype,\n    ) -> tuple[Mask, torch.Tensor]:\n        \"\"\"Get standard deviation estimate using base sampling and create base mask.\"\"\"\n        # Create base sampling indices\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n\n        # Extract values and compute std\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        total_rows = batch_size * num_heads * seq_len_queries\n        row_sampled_values = sampled_values.view(total_rows, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n\n        # Create base sampling mask\n        sampling_range = end_idx - start_idx\n        base_data = torch.full_like(\n            base_row_wise_idx, num_base_samples / sampling_range, dtype=expwts.dtype\n        )\n\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=dtype,\n        )\n\n        return base_mask, std_estimate\n\n    def _compute_adaptive_budget(\n        self,\n        std_estimate: torch.Tensor,\n        estimated_denominator: torch.Tensor,\n        sampling_range: int,\n    ) -> torch.Tensor:\n        \"\"\"Compute adaptive budget based on statistical bounds.\"\"\"\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        # Ensure budget is positive and within bounds\n        budget = torch.clamp(\n            budget_squared,\n            min=1.0,  # Minimum 1 sample\n            max=float(sampling_range),  # Maximum sampling_range samples\n        ).long()\n\n        return budget\n\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"\n        Improved ultra-sparse: Dynamically adapt threshold and shrink tail sampling even more aggressively using statistical coverage.\n        Yields lower density while maintaining the high accuracy of full attention approximation.\n\n        - (A) For each (b,h,q), deterministically select the minimum top-k tokens by exp(attn mass) to cover at least 97% (rather than 98%) of the total row mass. This shrinks k for many rows while keeping the main mass.\n        - (B) For the tail, shrink the statistical adaptive sample budget further using a lower scaling, and skip sampling on rows that are nearly uniform, as measured by the relative std. This maximizes sparsity without significant error.\n        - (C) The mask is the union of (A), (B), and previous mask.\n        \"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        seq_len_keys = keys.shape[-2]\n        # Slightly more aggressive/optimized static for lowest density\n        self.base_rate_sampling = 0.0065\n        self.epsilon = 0.13\n        self.delta = 0.11\n        self.init_offset = 0.0\n        self.local_offset = 0.0\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size, dims.num_heads, dims.seq_len_queries, dims.seq_len_keys\n        )\n\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # -- (A) Deterministic top-k per row for 97% exp(attn) mass, with tighter k cap\n        sorted_expwts, sorted_idx = torch.sort(expwts, dim=-1, descending=True)\n        cumsum = torch.cumsum(sorted_expwts, dim=-1)\n        total = cumsum[..., -1:]\n        frac = cumsum / (total + 1e-8)\n        k_mass = (frac < 0.97).sum(dim=-1, keepdim=True) + 1\n        # Cap k much lower for maximal sparsity, but at least 1 for each row\n        k_mass = torch.clamp(k_mass, min=1, max=min(10, seq_len_keys // 12 + 1))\n        max_k = int(k_mass.max())\n        arange_k = torch.arange(max_k, device=expwts.device)[None, None, None, :]\n        active_idx_mask = (arange_k < k_mass)\n        idx_selected = torch.where(\n            active_idx_mask,\n            sorted_idx[..., :max_k],\n            torch.zeros_like(sorted_idx[..., :max_k])\n        )\n        data_selected = torch.where(\n            active_idx_mask,\n            torch.ones_like(sorted_idx[..., :max_k], dtype=previous_mask.dtype),\n            torch.zeros_like(sorted_idx[..., :max_k], dtype=previous_mask.dtype)\n        )\n        topk_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=idx_selected,\n            data=data_selected,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        # -- (B) Statistical tail: aggressively shrink and suppress as much as safely possible\n        start_idx, end_idx, sampling_range = 0, seq_len_keys, seq_len_keys\n        num_base_samples = max(2, int(self.base_rate_sampling * sampling_range))\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        row_sampled_values = sampled_values.view(batch_size * num_heads * seq_len_queries, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n        base_data = torch.full_like(base_row_wise_idx, num_base_samples / sampling_range, dtype=previous_mask.dtype)\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n        sampled_denominator = apply_inv_mask_sum(expwts, base_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        min_budget = num_base_samples\n        max_budget = max(min_budget, int(0.03 * sampling_range))\n        rel_std = std_estimate / (estimated_denominator / sampling_range + 1e-8)\n        # Even more aggressive: lower the shrink (so tail is only enabled for highly non-uniform rows)\n        shrink = torch.where(rel_std < 0.06, 0.11, 0.32)\n        budget = torch.clamp(torch.ceil(budget_squared * shrink).long(), min=min_budget, max=max_budget)\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n        # For ultra-flat rows (i.e., almost uniform), set tail budget to zero\u2014skip tail entirely\n        flat_row = (rel_std < 0.025).squeeze(-1)\n        if torch.any(flat_row):\n            budget = torch.where(flat_row, torch.full_like(budget, 0), budget)\n            sampling_probabilities = torch.where(\n                flat_row, torch.zeros_like(sampling_probabilities), sampling_probabilities\n            )\n\n        tail_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n\n        # -- (C) Merge: previous \u222a topk \u222a tail\n        merged = previous_mask.merge_mask(topk_mask, inplace=False).merge_mask(tail_mask, inplace=False)\n        return merged\n\n    @classmethod\n    def create_from_config(cls, config: MaskerConfig) -> \"OpenEvolveMasker\":\n        \"\"\"Create OpenEvolve masker instance from configuration.\n\n        Args:\n            config: Configuration for the OpenEvolve masker.\n\n        Returns:\n            Instance of the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the config type is invalid.\n        \"\"\"\n        # not checking for config type here since we will be replacing this masker class\n        # with the new masker class in the evaluator.py file\n        return cls(config)\n", "language": "python", "parent_id": "71e3f0c5-2e1a-4702-8752-e0af6897643d", "generation": 5, "timestamp": 1757494375.0324214, "iteration_found": 82, "metrics": {"density": 100, "error": 100, "combined_score": -100}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 134 lines with 130 lines", "parent_metrics": {"density": 100, "error": 100, "combined_score": -100}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"diff_user": {"system": "You are an expert statistician and python / PyTorch programmer specializing statistical analysis \nof approximations. Focus on improving specified algorihm's ability to select the least amount of tokens (entries in the Mask which are non-zero) while maintaining the approximation quality. \nYou have to write the add_mask function using native pytorch operations -- mask created in this function should be as sparse as possible while preserving the ouptut of full attention as much as possible.\n\nThis file is part of a project which is a comprehensive framework designed for implementing and experimenting with various sparse attention mechanisms in deep learning models.  You will be improving on one of the Maskers. \nOverview of Maskers:\nMaskers are the core components that define how attention patterns are computed in the sparse attention framework. They implement different strategies for determining which key-query pairs should be attended to. A sparse attention in this framework is a series of Masker. Each Masker adds tokens that need to be activated in sparse attention. The data structure to store these active tokens is a Mask class. Details of the Mask class are as follows\n### **Mask Class Structure and Functions**\nThe `Mask` class is a sophisticated data structure that represents attention patterns in multiple formats for optimal performance and memory usage. It is a sparse matrix of size `(batch_size, heads, q_len, k_len)` where if an element is 0, that token is inactive for the particular head and query. If it is non-zero, then it is active and the value stores the probability with which the token is sampled. \n#### **Core Attributes:**\n```python\nclass Mask:\n    def __init__(self, shape, dtype, mask=None, indices=None, ptr=None, \n                data=None, from_dense_mask=False, from_index=False, is_full=False):\n        self.shape = shape                    # Shape of the mask (*, n)\n        self.dtype = dtype                    # Data type for mask values\n        self.from_dense_mask = from_dense_mask  # Whether created from dense tensor\n        self.from_index = from_index          # Whether created from sparse indices\n        self.is_full = is_full                # Whether this is a full mask (all 1.0)\n        \n        # Storage attributes (only one is active at a time)\n        self.mask = None      # Dense tensor representation\n        self.indices = None   # Sparse indices\n        self.ptr = None       # Sparse pointer array\n        self.data = None      # Sparse data values\n```\n#### **Mask Class Functions and Signatures:**\n**Creation Methods:**\n```python\n@classmethod\ndef create_full_mask(shape: Tuple[int, ...], dtype: torch.dtype = torch.float32) -> \"Mask\"\n# Creates a full mask (all elements are 1.0) with optimized representation\n@classmethod\ndef create_mask_from_dense_mask(shape: Tuple[int, ...], mask: torch.Tensor, dtype: torch.dtype = torch.float32) -> \"Mask\"\n# Creates a Mask from dense mask tensor\n@classmethod\ndef create_mask_from_indices(shape: Tuple[int, ...], indices: torch.Tensor, ptr: torch.Tensor, data: Optional[torch.Tensor] = None, dtype: torch.dtype = torch.float32) -> \"Mask\"\n# Creates a Mask from sparse indices and pointer representation\n@classmethod\ndef create_from_row_wise_idx(shape: Tuple[int, ...], row_wise_idx: torch.Tensor, data: torch.Tensor, type: str = \"index\", dtype: torch.dtype = torch.float32) -> \"Mask\"\n# Creates a Mask from row-wise indices\n@classmethod\ndef create_empty_mask(shape: Tuple[int, ...], dtype: torch.dtype = torch.float32, mask_type: str = \"dense\") -> \"Mask\"\n# Creates a mask object with all values set to zero\n```\n**Access Methods:**\n```python\ndef get_dense_mask(self) -> torch.Tensor\n# Returns the dense mask representation\ndef get_index_mask(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n# Returns the sparse index representation (indices, ptr, data)\ndef apply_mask(self, input_tensor: torch.Tensor) -> torch.Tensor\n# Applies the mask to an input tensor (0 => inactive, >0 => active with weight)\ndef apply_inv_mask(self, input_tensor: torch.Tensor) -> torch.Tensor\n# Applies the inverse mask to an input tensor (output[IDX] = input[IDX] * 1.0 / mask[IDX])\n```\n**Utility Methods:**\n```python\ndef is_full_mask(self) -> bool\n# Checks if this is a full mask (all elements are 1.0)\ndef is_empty(self) -> bool\n# Checks if the mask is empty (all elements are 0.0)\ndef get_density(self) -> float\n# Returns the density/sparsity of the mask (ratio of non-zero elements)\ndef merge_mask(self, other_mask: \"Mask\", inplace: bool = False) -> \"Mask\"\n# Merges this mask with another mask in sparse format\n```\n**Three Representation Formats:**\n1. **Dense Representation** (`from_dense_mask=True`): Full tensor storage for compatibility\n2. **Sparse Representation** (`from_index=True`): CSR-like format for memory efficiency  \n3. **Full Mask Optimization** (`is_full=True`): Zero storage for all-ones masks\nMask is a sparse matrix of size batch_size, heads, q_len, k_len . where if an element is 0, that token is inactive for the particular head and query. if it is non-zero then it is active. The final attention using mask is computed as follows,\n### **`get_masked_attention_output` Function:**\n**Function Signature:**\n```python\ndef get_masked_attention_output(\n    module: nn.Module,\n    queries: torch.Tensor,\n    keys: torch.Tensor,\n    values: torch.Tensor,\n    attention_mask: Optional[torch.Tensor],\n    scaling: float,\n    dropout: float,\n    sparse_attention_mask: Mask,\n    return_attention_weights: bool = False,\n    **kwargs: Dict[str, Any],\n) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n```\n**English Description:**\nThis function computes masked attention output by dividing numerator by denominator. It takes the attention module, query/key/value tensors, optional attention mask, scaling factor, dropout probability, and the sparse attention mask object. It optionally returns attention weights along with the output. The function efficiently computes exponential attention weights once and reuses them for both numerator and denominator calculations to avoid redundant computation. **Important**: The function uses `apply_inv_mask` (inverse mask application) during the computation process, which applies the inverse of the mask weights to the attention scores.\n**Key Steps:**\n1. **Compute exponential attention weights** using `_compute_masked_exp_attention_weights`\n2. **Apply inverse mask** using `sparse_attention_mask.apply_inv_mask()` to the exponential attention weights\n3. **Calculate numerator** by multiplying exponential weights with values\n4. **Calculate denominator** by summing exponential weights along the last dimension\n5. **Compute final output** by dividing numerator by denominator\n6. **Optionally return attention weights** by normalizing exponential weights with denominator\n**Return Values:**\n- If `return_attention_weights=False`: Returns attention output tensor of shape `(b, h, sq, d)`\n- If `return_attention_weights=True`: Returns tuple of `(attention_output, attention_weights)` where attention_weights has shape `(b, h, sq, sk)`\nYou are supposed to write add_mask function for this class which decides what tokens are active and what weights are associated with them . The `add_mask` function is the core method in every masker that defines how attention patterns are computed and applied. It serves as the primary interface between the attention mechanism and the masking logic.\n### **Function Signature:**\n```python\ndef add_mask(\n    self,\n    keys: torch.Tensor,\n    queries: torch.Tensor,\n    values: torch.Tensor,\n    attention_mask: torch.Tensor,\n    scaling: float,\n    dropout: float,\n    sparse_meta_data: Dict[Any, Any],\n    previous_mask: Mask,\n    **kwargs: Dict[str, Any],\n) -> Mask:\n```\n### **Key Responsibilities:**\n1. **Pattern Computation**: Determines which key-query pairs should be attended to based on the masker's strategy\n2. **Mask Generation**: Creates a `Mask` object representing the computed attention pattern\n3. **Mask Merging**: Combines the new mask with any previous masks to build cumulative attention patterns\nImportant things to remember: Use query and key head computation to compute num_groups and use that to repeat keys before you use them. \n\nFocus on evolving the add_mask function to produce the best makser possible that is as sparse as possible while preserving the ouptut of full attention as much as possible.\n\nExplain step-by-step the reasoning process for your solution and how this will lead to a better masker.\n", "user": "# Current Program Information\n- Current performance metrics: - density: 100.0000\n- error: 100.0000\n- combined_score: -100.0000\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: density, error. Consider continuing with similar changes.\n- Metrics showing regression: combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 3\n- Changes: Unknown changes\n- Performance: density: -0.0702, error: -0.0364, combined_score: -0.0533\n- Outcome: Regression in all metrics\n\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: density: -0.0715, error: -0.0351, combined_score: -0.0533\n- Outcome: Regression in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: density: -0.0712, error: -0.0353, combined_score: -0.0532\n- Outcome: Regression in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: -0.0532)\n```python\n\"\"\"OpenEvolve masker implementation.\n\nThis module provides the OpenEvolve masker that implements evolutionary attention patterns.\nThe current implementation is a bare metal version that returns the previous mask.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nimport torch\nfrom scipy.stats import norm\n\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers import (\n    ResearchMasker,\n)\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers.base import (\n    MaskerConfig,\n    MaskerRegistry,\n)\nfrom sparse_attention_hub.sparse_attention.utils.mask import Mask\nfrom sparse_attention_hub.sparse_attention.utils.mask_attention_utils import (\n    _get_num_key_value_groups,\n    apply_inv_mask_sum,\n    create_sampling_mask_with_per_head_budget,\n    repeat_kv,\n)\n\n\n@dataclass\nclass OpenEvolveMaskerConfig(MaskerConfig):\n    \"\"\"Configuration for OpenEvolveMasker.\n\n    This configuration class inherits from MaskerConfig and provides\n    parameters for the attention mechanism evolved out of openevolve.\n    empty placeholder\n    \"\"\"\n\n    pass\n\n\n@MaskerRegistry.register(OpenEvolveMaskerConfig)\nclass OpenEvolveMasker(ResearchMasker):\n    \"\"\"OpenEvolve masker for evolutionary attention computation.\n\n    This masker implements evolutionary attention patterns that adapt over time.\n    The current implementation is a bare metal version that returns the previous mask.\n\n    Attributes:\n        evolution_rate: The rate of evolution for attention patterns.\n            This value is set from the configuration and controls how quickly\n            the attention patterns evolve.\n\n    Important Notes:\n        - This is a bare metal implementation that simply returns the previous mask.\n        - Future implementations will include evolutionary algorithms for attention pattern optimization.\n        - The evolution_rate parameter is currently unused but will be utilized in future versions.\n\n    Example:\n        >>> config = OpenEvolveMaskerConfig(evolution_rate=1.0)\n        >>> masker = OpenEvolveMasker(config)\n        >>> # Use masker.add_mask() to apply evolutionary attention patterns\n    \"\"\"\n\n    def __init__(self, config: OpenEvolveMaskerConfig) -> None:\n        \"\"\"Initialize OpenEvolve masker with configuration.\n\n        Args:\n            config: Configuration object containing the evolution rate and other\n                parameters for the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the evolution_rate in config is negative.\n                This validation is performed in the config's __post_init__ method.\n        \"\"\"\n        self.base_rate_sampling = 0.01\n        self.epsilon = 0.3\n        self.delta = 0.3\n        self.init_offset = 0.001\n        self.local_offset = 0.001\n\n        super().__init__(config)\n\n    def _compute_exp_attention_scores(\n        self,\n        queries: torch.Tensor,\n        keys: torch.Tensor,\n        scaling: float,\n        attention_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute exponential attention scores with numerical stability.\"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        raw_scores = torch.matmul(queries, keys.transpose(-2, -1)) * scaling\n        if attention_mask is not None:\n            raw_scores = raw_scores + attention_mask[:, :, :, : keys.shape[-2]]\n        max_scores = torch.max(raw_scores, dim=-1, keepdim=True)[0]\n        return torch.exp(raw_scores - max_scores)\n\n    def _get_sampling_range(self, seq_len_keys: int) -> tuple[int, int, int]:\n        \"\"\"Get sampling range and validate it.\n\n        Args:\n            seq_len_keys: Number of keys in the sequence.\n\n        Returns:\n            Tuple of (start_idx, end_idx, sampling_range).\n\n        Raises:\n            ValueError: If the computed sampling range is invalid.\n        \"\"\"\n        # Compute start index\n        if isinstance(self.init_offset, float):\n            start_idx: int = int(self.init_offset * seq_len_keys)\n        else:\n            start_idx = self.init_offset\n\n        # Compute end index\n        if isinstance(self.local_offset, float):\n            end_idx: int = seq_len_keys - int(self.local_offset * seq_len_keys)\n        else:\n            end_idx = seq_len_keys - self.local_offset\n\n        sampling_range = end_idx - start_idx\n\n        if sampling_range <= 0:\n            raise ValueError(f\"Invalid sampling range: {sampling_range}\")\n\n        return start_idx, end_idx, sampling_range\n\n    def _get_base_sample_count(self, sampling_range: int) -> int:\n        \"\"\"Get number of base samples based on configuration.\"\"\"\n        # Ensure at least 2 samples since it is used for std estimation\n        if isinstance(self.base_rate_sampling, int):\n            return max(2, self.base_rate_sampling)\n        return max(2, int(self.base_rate_sampling * sampling_range))\n\n    def _get_std_estimate_using_base_sample(\n        self,\n        expwts: torch.Tensor,\n        batch_size: int,\n        num_heads: int,\n        seq_len_queries: int,\n        seq_len_keys: int,\n        start_idx: int,\n        end_idx: int,\n        num_base_samples: int,\n        dtype: torch.dtype,\n    ) -> tuple[Mask, torch.Tensor]:\n        \"\"\"Get standard deviation estimate using base sampling and create base mask.\"\"\"\n        # Create base sampling indices\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n\n        # Extract values and compute std\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        total_rows = batch_size * num_heads * seq_len_queries\n        row_sampled_values = sampled_values.view(total_rows, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n\n        # Create base sampling mask\n        sampling_range = end_idx - start_idx\n        base_data = torch.full_like(\n            base_row_wise_idx, num_base_samples / sampling_range, dtype=expwts.dtype\n        )\n\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=dtype,\n        )\n\n        return base_mask, std_estimate\n\n    def _compute_adaptive_budget(\n        self,\n        std_estimate: torch.Tensor,\n        estimated_denominator: torch.Tensor,\n        sampling_range: int,\n    ) -> torch.Tensor:\n        \"\"\"Compute adaptive budget based on statistical bounds.\"\"\"\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        # Ensure budget is positive and within bounds\n        budget = torch.clamp(\n            budget_squared,\n            min=1.0,  # Minimum 1 sample\n            max=float(sampling_range),  # Maximum sampling_range samples\n        ).long()\n\n        return budget\n\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"\n        Improved hybrid: deterministically select minimal set of top keys per query to cover ~97% exp(attn) mass,\n        and sample adaptively and aggressively only the remaining low-mass tail. This produces much sparser masks while preserving approximation.\n        \"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n\n        seq_len_keys = keys.shape[-2]\n        # Dynamic parameter adaptation (as in best versions)\n        if seq_len_keys > 10000:\n            self.base_rate_sampling = 0.02\n            self.epsilon = 0.2\n            self.delta = 0.2\n        elif seq_len_keys > 5000:\n            self.base_rate_sampling = 0.018\n            self.epsilon = 0.22\n            self.delta = 0.22\n        else:\n            self.base_rate_sampling = 0.015\n            self.epsilon = 0.25\n            self.delta = 0.25\n\n        self.init_offset = 0.002\n        self.local_offset = 0.002\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size, dims.num_heads, dims.seq_len_queries, dims.seq_len_keys\n        )\n\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # Get main sampling parameters\n        start_idx, end_idx, sampling_range = self._get_sampling_range(seq_len_keys)\n        num_base_samples = self._get_base_sample_count(sampling_range)\n        # Base sampling + std estimation (used for adaptive tail only)\n        base_sampling_mask, std_estimate = self._get_std_estimate_using_base_sample(\n            expwts,\n            batch_size,\n            num_heads,\n            seq_len_queries,\n            seq_len_keys,\n            start_idx,\n            end_idx,\n            num_base_samples,\n            previous_mask.dtype,\n        )\n        sampled_denominator = apply_inv_mask_sum(expwts, base_sampling_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n\n        # Compute error-controlled budget\n        budget = self._compute_adaptive_budget(\n            std_estimate, estimated_denominator, sampling_range\n        )\n\n        # -- Hybrid: select minimal number of keys covering \u226597% mass per row, rest gets aggressive tail sampling --\n        sorted_expwts, sorted_idx = torch.sort(expwts, dim=-1, descending=True)\n        cumsum = torch.cumsum(sorted_expwts, dim=-1)\n        total = cumsum[..., -1:]\n        frac = cumsum / (total + 1e-8)\n        # How many keys needed to reach \u226597% mass\n        k_97 = (frac < 0.97).sum(dim=-1, keepdim=True) + 1\n        k_97 = torch.clamp(k_97, min=1, max=min(10, seq_len_keys // 10 + 1))\n        max_topk = int(k_97.max())\n        arange_topk = torch.arange(max_topk, device=expwts.device)[None, None, None, :]\n        mask_top = arange_topk < k_97\n        top_idx = sorted_idx[..., :max_topk]\n        top_data = torch.where(\n            mask_top, torch.ones_like(top_idx, dtype=previous_mask.dtype), torch.zeros_like(top_idx, dtype=previous_mask.dtype)\n        )\n        topmass_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=top_idx,\n            data=top_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        # Aggressively shrink tail budget where possible (if most mass is covered)\n        rel_mass_covered = cumsum[..., -1:] / (static_denominator + 1e-8)\n        tail_scale = torch.where(rel_mass_covered > 0.97, 0.19, 0.38)\n        min_base = max(num_base_samples, 2)\n        budget = torch.ceil(budget.float() * tail_scale).long()\n        budget = torch.clamp(budget, min=min_base)\n        max_budget = max(min_base, int(0.05 * sampling_range))\n        budget = torch.clamp(budget, max=max_budget)\n        # Remove tail for ultra-short cases where top mass covers almost all tokens\n        budget = torch.where(k_97 >= seq_len_keys - 2, torch.full_like(budget, 0), budget)\n\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n        adaptive_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n\n        # Merge: previous \u222a topmass \u222a adaptive tail\n        merged = previous_mask.merge_mask(topmass_mask, inplace=False).merge_mask(adaptive_mask, inplace=False)\n        return merged\n\n    @classmethod\n    def create_from_config(cls, config: MaskerConfig) -> \"OpenEvolveMasker\":\n        \"\"\"Create OpenEvolve masker instance from configuration.\n\n        Args:\n            config: Configuration for the OpenEvolve masker.\n\n        Returns:\n            Instance of the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the config type is invalid.\n        \"\"\"\n        # not checking for config type here since we will be replacing this masker class\n        # with the new masker class in the evaluator.py file\n        return cls(config)\n\n```\nKey features: Performs well on density (-0.0712), Performs well on error (-0.0353), Performs well on combined_score (-0.0532)\n\n\n### Program 2 (Score: -0.0533)\n```python\n\"\"\"OpenEvolve masker implementation.\n\nThis module provides the OpenEvolve masker that implements evolutionary attention patterns.\nThe current implementation is a bare metal version that returns the previous mask.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nimport torch\nfrom scipy.stats import norm\n\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers import (\n    ResearchMasker,\n)\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers.base import (\n    MaskerConfig,\n    MaskerRegistry,\n)\nfrom sparse_attention_hub.sparse_attention.utils.mask import Mask\nfrom sparse_attention_hub.sparse_attention.utils.mask_attention_utils import (\n    _get_num_key_value_groups,\n    apply_inv_mask_sum,\n    create_sampling_mask_with_per_head_budget,\n    repeat_kv,\n)\n\n\n@dataclass\nclass OpenEvolveMaskerConfig(MaskerConfig):\n    \"\"\"Configuration for OpenEvolveMasker.\n\n    This configuration class inherits from MaskerConfig and provides\n    parameters for the attention mechanism evolved out of openevolve.\n    empty placeholder\n    \"\"\"\n\n    pass\n\n\n@MaskerRegistry.register(OpenEvolveMaskerConfig)\nclass OpenEvolveMasker(ResearchMasker):\n    \"\"\"OpenEvolve masker for evolutionary attention computation.\n\n    This masker implements evolutionary attention patterns that adapt over time.\n    The current implementation is a bare metal version that returns the previous mask.\n\n    Attributes:\n        evolution_rate: The rate of evolution for attention patterns.\n            This value is set from the configuration and controls how quickly\n            the attention patterns evolve.\n\n    Important Notes:\n        - This is a bare metal implementation that simply returns the previous mask.\n        - Future implementations will include evolutionary algorithms for attention pattern optimization.\n        - The evolution_rate parameter is currently unused but will be utilized in future versions.\n\n    Example:\n        >>> config = OpenEvolveMaskerConfig(evolution_rate=1.0)\n        >>> masker = OpenEvolveMasker(config)\n        >>> # Use masker.add_mask() to apply evolutionary attention patterns\n    \"\"\"\n\n    def __init__(self, config: OpenEvolveMaskerConfig) -> None:\n        \"\"\"Initialize OpenEvolve masker with configuration.\n\n        Args:\n            config: Configuration object containing the evolution rate and other\n                parameters for the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the evolution_rate in config is negative.\n                This validation is performed in the config's __post_init__ method.\n        \"\"\"\n        self.base_rate_sampling = 0.01\n        self.epsilon = 0.3\n        self.delta = 0.3\n        self.init_offset = 0.001\n        self.local_offset = 0.001\n\n        super().__init__(config)\n\n    def _compute_exp_attention_scores(\n        self,\n        queries: torch.Tensor,\n        keys: torch.Tensor,\n        scaling: float,\n        attention_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute exponential attention scores with numerical stability.\"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        raw_scores = torch.matmul(queries, keys.transpose(-2, -1)) * scaling\n        if attention_mask is not None:\n            raw_scores = raw_scores + attention_mask[:, :, :, : keys.shape[-2]]\n        max_scores = torch.max(raw_scores, dim=-1, keepdim=True)[0]\n        return torch.exp(raw_scores - max_scores)\n\n    def _get_sampling_range(self, seq_len_keys: int) -> tuple[int, int, int]:\n        \"\"\"Get sampling range and validate it.\n\n        Args:\n            seq_len_keys: Number of keys in the sequence.\n\n        Returns:\n            Tuple of (start_idx, end_idx, sampling_range).\n\n        Raises:\n            ValueError: If the computed sampling range is invalid.\n        \"\"\"\n        # Compute start index\n        if isinstance(self.init_offset, float):\n            start_idx: int = int(self.init_offset * seq_len_keys)\n        else:\n            start_idx = self.init_offset\n\n        # Compute end index\n        if isinstance(self.local_offset, float):\n            end_idx: int = seq_len_keys - int(self.local_offset * seq_len_keys)\n        else:\n            end_idx = seq_len_keys - self.local_offset\n\n        sampling_range = end_idx - start_idx\n\n        if sampling_range <= 0:\n            raise ValueError(f\"Invalid sampling range: {sampling_range}\")\n\n        return start_idx, end_idx, sampling_range\n\n    def _get_base_sample_count(self, sampling_range: int) -> int:\n        \"\"\"Get number of base samples based on configuration.\"\"\"\n        # Ensure at least 2 samples since it is used for std estimation\n        if isinstance(self.base_rate_sampling, int):\n            return max(2, self.base_rate_sampling)\n        return max(2, int(self.base_rate_sampling * sampling_range))\n\n    def _get_std_estimate_using_base_sample(\n        self,\n        expwts: torch.Tensor,\n        batch_size: int,\n        num_heads: int,\n        seq_len_queries: int,\n        seq_len_keys: int,\n        start_idx: int,\n        end_idx: int,\n        num_base_samples: int,\n        dtype: torch.dtype,\n    ) -> tuple[Mask, torch.Tensor]:\n        \"\"\"Get standard deviation estimate using base sampling and create base mask.\"\"\"\n        # Create base sampling indices\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n\n        # Extract values and compute std\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        total_rows = batch_size * num_heads * seq_len_queries\n        row_sampled_values = sampled_values.view(total_rows, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n\n        # Create base sampling mask\n        sampling_range = end_idx - start_idx\n        base_data = torch.full_like(\n            base_row_wise_idx, num_base_samples / sampling_range, dtype=expwts.dtype\n        )\n\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=dtype,\n        )\n\n        return base_mask, std_estimate\n\n    def _compute_adaptive_budget(\n        self,\n        std_estimate: torch.Tensor,\n        estimated_denominator: torch.Tensor,\n        sampling_range: int,\n    ) -> torch.Tensor:\n        \"\"\"Compute adaptive budget based on statistical bounds.\"\"\"\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        # Ensure budget is positive and within bounds\n        budget = torch.clamp(\n            budget_squared,\n            min=1.0,  # Minimum 1 sample\n            max=float(sampling_range),  # Maximum sampling_range samples\n        ).long()\n\n        return budget\n\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"\n        Hybrid ultra-sparse: Combine high-mass deterministic selection with a minimal adaptive tail sampling for better density/quality tradeoff.\n\n        - (A) For each (b,h,q), deterministically select the top-k tokens by exp(attn mass) to cover ~97% of the total attention score mass.\n        - (B) For the remaining \"tail\", use aggressive adaptive random sampling as before, but with a smaller cap and fewer samples.\n        - (C) The new mask is the union of the high-mass top tokens, the adaptive tail mask, and the previous mask.\n        \"\"\"\n\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n\n        seq_len_keys = keys.shape[-2]\n        # Dynamic parameter adaptation (as in best versions)\n        if seq_len_keys > 10000:\n            self.base_rate_sampling = 0.02\n            self.epsilon = 0.2\n            self.delta = 0.2\n        elif seq_len_keys > 5000:\n            self.base_rate_sampling = 0.018\n            self.epsilon = 0.22\n            self.delta = 0.22\n        else:\n            self.base_rate_sampling = 0.015\n            self.epsilon = 0.25\n            self.delta = 0.25\n\n        self.init_offset = 0.002\n        self.local_offset = 0.002\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        # Get tensor dimensions\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size, dims.num_heads, dims.seq_len_queries, dims.seq_len_keys\n        )\n\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # ---- (A) Deterministic high-mass top-k tokens per row ----\n        # Cover 97% attention mass, min 1, max 12 for density/quality balance\n        sorted_expwts, sorted_idx = torch.sort(expwts, dim=-1, descending=True)\n        mass_cumsum = torch.cumsum(sorted_expwts, dim=-1)\n        total_mass = mass_cumsum[..., -1:]\n        frac = mass_cumsum / (total_mass + 1e-8)\n        k_mass = (frac < 0.97).sum(dim=-1, keepdim=True) + 1\n        k_mass = torch.clamp(k_mass, min=1, max=min(12, seq_len_keys // 8 + 1))\n        max_k = int(k_mass.max())\n        # Gather indices for all queries, mask off excess for batch/q shorter than max_k\n        arange_k = torch.arange(max_k, device=expwts.device)[None, None, None, :]\n        active_idx_mask = (arange_k < k_mass)\n        idx_selected = torch.where(\n            active_idx_mask,\n            sorted_idx[..., :max_k],\n            torch.zeros_like(sorted_idx[..., :max_k])\n        )\n        data_selected = torch.where(\n            active_idx_mask,\n            torch.ones_like(sorted_idx[..., :max_k], dtype=previous_mask.dtype),\n            torch.zeros_like(sorted_idx[..., :max_k], dtype=previous_mask.dtype)\n        )\n        topk_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=idx_selected,\n            data=data_selected,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        # ---- (B) Aggressive tail-sampling for the rest of the keys ----\n        start_idx, end_idx, sampling_range = self._get_sampling_range(seq_len_keys)\n        num_base_samples = self._get_base_sample_count(sampling_range)\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        row_sampled_values = sampled_values.view(batch_size * num_heads * seq_len_queries, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n        base_data = torch.full_like(base_row_wise_idx, num_base_samples / sampling_range, dtype=previous_mask.dtype)\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n        sampled_denominator = apply_inv_mask_sum(expwts, base_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        min_budget = num_base_samples\n        # scale budget aggressively but with a minimum (empirical for density/error tradeoff)\n        scale = torch.where((mass_cumsum[..., -1:] / (static_denominator + 1e-8)) > 0.97, 0.22, 0.45)\n        budget = torch.clamp(torch.ceil(budget_squared * scale).long(), min=min_budget)\n        max_budget = max(min_budget, int(0.06 * sampling_range))\n        budget = torch.clamp(budget, max=max_budget)\n        # For almost all-mass-in-topk, set tail budget to zero\n        budget = torch.where(k_mass >= seq_len_keys - 2, torch.full_like(budget, 0), budget)\n\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n        tail_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n\n        # ---- (C) Merge: previous OR topk OR tail ----\n        merged = previous_mask.merge_mask(topk_mask, inplace=False).merge_mask(tail_mask, inplace=False)\n        return merged\n\n    @classmethod\n    def create_from_config(cls, config: MaskerConfig) -> \"OpenEvolveMasker\":\n        \"\"\"Create OpenEvolve masker instance from configuration.\n\n        Args:\n            config: Configuration for the OpenEvolve masker.\n\n        Returns:\n            Instance of the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the config type is invalid.\n        \"\"\"\n        # not checking for config type here since we will be replacing this masker class\n        # with the new masker class in the evaluator.py file\n        return cls(config)\n\n```\nKey features: Performs well on density (-0.0715), Performs well on error (-0.0351), Performs well on combined_score (-0.0533)\n\n\n### Program 3 (Score: -0.0533)\n```python\n\"\"\"OpenEvolve masker implementation.\n\nThis module provides the OpenEvolve masker that implements evolutionary attention patterns.\nThe current implementation is a bare metal version that returns the previous mask.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nimport torch\nfrom scipy.stats import norm\n\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers import (\n    ResearchMasker,\n)\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers.base import (\n    MaskerConfig,\n    MaskerRegistry,\n)\nfrom sparse_attention_hub.sparse_attention.utils.mask import Mask\nfrom sparse_attention_hub.sparse_attention.utils.mask_attention_utils import (\n    _get_num_key_value_groups,\n    apply_inv_mask_sum,\n    create_sampling_mask_with_per_head_budget,\n    repeat_kv,\n)\n\n\n@dataclass\nclass OpenEvolveMaskerConfig(MaskerConfig):\n    \"\"\"Configuration for OpenEvolveMasker.\n\n    This configuration class inherits from MaskerConfig and provides\n    parameters for the attention mechanism evolved out of openevolve.\n    empty placeholder\n    \"\"\"\n\n    pass\n\n\n@MaskerRegistry.register(OpenEvolveMaskerConfig)\nclass OpenEvolveMasker(ResearchMasker):\n    \"\"\"OpenEvolve masker for evolutionary attention computation.\n\n    This masker implements evolutionary attention patterns that adapt over time.\n    The current implementation is a bare metal version that returns the previous mask.\n\n    Attributes:\n        evolution_rate: The rate of evolution for attention patterns.\n            This value is set from the configuration and controls how quickly\n            the attention patterns evolve.\n\n    Important Notes:\n        - This is a bare metal implementation that simply returns the previous mask.\n        - Future implementations will include evolutionary algorithms for attention pattern optimization.\n        - The evolution_rate parameter is currently unused but will be utilized in future versions.\n\n    Example:\n        >>> config = OpenEvolveMaskerConfig(evolution_rate=1.0)\n        >>> masker = OpenEvolveMasker(config)\n        >>> # Use masker.add_mask() to apply evolutionary attention patterns\n    \"\"\"\n\n    def __init__(self, config: OpenEvolveMaskerConfig) -> None:\n        \"\"\"Initialize OpenEvolve masker with configuration.\n\n        Args:\n            config: Configuration object containing the evolution rate and other\n                parameters for the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the evolution_rate in config is negative.\n                This validation is performed in the config's __post_init__ method.\n        \"\"\"\n        self.base_rate_sampling = 0.01\n        self.epsilon = 0.3\n        self.delta = 0.3\n        self.init_offset = 0.001\n        self.local_offset = 0.001\n\n        super().__init__(config)\n\n    def _compute_exp_attention_scores(\n        self,\n        queries: torch.Tensor,\n        keys: torch.Tensor,\n        scaling: float,\n        attention_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute exponential attention scores with numerical stability.\"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        raw_scores = torch.matmul(queries, keys.transpose(-2, -1)) * scaling\n        if attention_mask is not None:\n            raw_scores = raw_scores + attention_mask[:, :, :, : keys.shape[-2]]\n        max_scores = torch.max(raw_scores, dim=-1, keepdim=True)[0]\n        return torch.exp(raw_scores - max_scores)\n\n    def _get_sampling_range(self, seq_len_keys: int) -> tuple[int, int, int]:\n        \"\"\"Get sampling range and validate it.\n\n        Args:\n            seq_len_keys: Number of keys in the sequence.\n\n        Returns:\n            Tuple of (start_idx, end_idx, sampling_range).\n\n        Raises:\n            ValueError: If the computed sampling range is invalid.\n        \"\"\"\n        # Compute start index\n        if isinstance(self.init_offset, float):\n            start_idx: int = int(self.init_offset * seq_len_keys)\n        else:\n            start_idx = self.init_offset\n\n        # Compute end index\n        if isinstance(self.local_offset, float):\n            end_idx: int = seq_len_keys - int(self.local_offset * seq_len_keys)\n        else:\n            end_idx = seq_len_keys - self.local_offset\n\n        sampling_range = end_idx - start_idx\n\n        if sampling_range <= 0:\n            raise ValueError(f\"Invalid sampling range: {sampling_range}\")\n\n        return start_idx, end_idx, sampling_range\n\n    def _get_base_sample_count(self, sampling_range: int) -> int:\n        \"\"\"Get number of base samples based on configuration.\"\"\"\n        # Ensure at least 2 samples since it is used for std estimation\n        if isinstance(self.base_rate_sampling, int):\n            return max(2, self.base_rate_sampling)\n        return max(2, int(self.base_rate_sampling * sampling_range))\n\n    def _get_std_estimate_using_base_sample(\n        self,\n        expwts: torch.Tensor,\n        batch_size: int,\n        num_heads: int,\n        seq_len_queries: int,\n        seq_len_keys: int,\n        start_idx: int,\n        end_idx: int,\n        num_base_samples: int,\n        dtype: torch.dtype,\n    ) -> tuple[Mask, torch.Tensor]:\n        \"\"\"Get standard deviation estimate using base sampling and create base mask.\"\"\"\n        # Create base sampling indices\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n\n        # Extract values and compute std\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        total_rows = batch_size * num_heads * seq_len_queries\n        row_sampled_values = sampled_values.view(total_rows, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n\n        # Create base sampling mask\n        sampling_range = end_idx - start_idx\n        base_data = torch.full_like(\n            base_row_wise_idx, num_base_samples / sampling_range, dtype=expwts.dtype\n        )\n\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=dtype,\n        )\n\n        return base_mask, std_estimate\n\n    def _compute_adaptive_budget(\n        self,\n        std_estimate: torch.Tensor,\n        estimated_denominator: torch.Tensor,\n        sampling_range: int,\n    ) -> torch.Tensor:\n        \"\"\"Compute adaptive budget based on statistical bounds.\"\"\"\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        # Ensure budget is positive and within bounds\n        budget = torch.clamp(\n            budget_squared,\n            min=1.0,  # Minimum 1 sample\n            max=float(sampling_range),  # Maximum sampling_range samples\n        ).long()\n\n        return budget\n\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"Add adaptive sampling mask to attention computation.\n\n        This method implements the core adaptive sampling logic. It combines base\n        sampling with adaptive budget allocation based on statistical error bounds.\n\n        Args:\n            keys: Key tensor with shape (batch_size, num_heads, seq_len_keys, head_dim).\n            queries: Query tensor with shape (batch_size, num_heads, seq_len_queries, head_dim).\n            values: Value tensor with shape (batch_size, num_heads, seq_len_keys, head_dim).\n            attention_mask: Attention mask tensor indicating which positions are valid.\n            sparse_meta_data: Dictionary containing sparse attention metadata.\n            previous_mask: Previous attention mask to merge with the new adaptive sampling mask.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A new Mask object representing the attention pattern after applying\n            adaptive sampling.\n\n        Raises:\n            ValueError: If the sampling range is invalid.\n        \"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n\n        # EVOLVE-BLOCK-START\n\n        # PARAMETERS to be set - ITERATION 3: Dynamic parameter adaptation (BEST PERFORMANCE)\n        # Adaptive parameters based on sequence length\n        seq_len_keys = keys.shape[-2]\n        if seq_len_keys > 10000:\n            self.base_rate_sampling = 0.02  # Higher sampling for very long sequences\n            self.epsilon = 0.2  # Tighter bounds for long sequences\n            self.delta = 0.2\n        elif seq_len_keys > 5000:\n            self.base_rate_sampling = 0.018  # Medium sampling for long sequences\n            self.epsilon = 0.22  # Medium bounds\n            self.delta = 0.22\n        else:\n            self.base_rate_sampling = 0.015  # Default for shorter sequences\n            self.epsilon = 0.25\n            self.delta = 0.25\n\n        self.init_offset = 0.002\n        self.local_offset = 0.002\n\n        # Pre-compute delta_ppf for efficiency\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        # Extract dimensions and compute attention scores\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size,\n            dims.num_heads,\n            dims.seq_len_queries,\n            dims.seq_len_keys,\n        )\n        # Compute attention scores after removing attention_mask\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # Get sampling parameters\n        start_idx, end_idx, sampling_range = self._get_sampling_range(seq_len_keys)\n        num_base_samples = self._get_base_sample_count(sampling_range)\n\n        # Create base sampling mask and estimate std\n        base_sampling_mask, std_estimate = self._get_std_estimate_using_base_sample(\n            expwts,\n            batch_size,\n            num_heads,\n            seq_len_queries,\n            seq_len_keys,\n            start_idx,\n            end_idx,\n            num_base_samples,\n            previous_mask.dtype,\n        )\n        # Compute denominators and budget\n        sampled_denominator = apply_inv_mask_sum(expwts, base_sampling_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n        budget = self._compute_adaptive_budget(\n            std_estimate, estimated_denominator, sampling_range\n        )\n        # ------------------------------------------------------------------ #\n        # NEW \u2013 extra sparsification while keeping statistical guarantees\n        # ------------------------------------------------------------------ #\n\n        # --- DENSITY-BOOSTING STRATEGY ---\n        # Make budget scaling adaptive based on error feedback if available, or use more aggressive static scaling.\n        # Aim: Reduce density without hurting error.\n\n        # --- Hybrid: Deterministic Top-mass + Statistical Tail Sampling ---\n        # 1. Select minimal number of keys covering ~97% exp(attn) mass per row\n        # 2. Aggressively sample the remaining tail using adaptive budget\n\n        # (1) Deterministic top-mass selection\n        sorted_expwts, sorted_idx = torch.sort(expwts, dim=-1, descending=True)\n        cumsum = torch.cumsum(sorted_expwts, dim=-1)\n        total = cumsum[..., -1:]\n        frac = cumsum / (total + 1e-8)\n        # Number of keys to cover \u226597% mass\n        k_97 = (frac < 0.97).sum(dim=-1, keepdim=True) + 1\n        k_97 = torch.clamp(k_97, min=1, max=min(10, seq_len_keys // 10 + 1))\n        max_topk = int(k_97.max())\n        arange_topk = torch.arange(max_topk, device=expwts.device)[None, None, None, :]\n        mask_top = arange_topk < k_97\n        top_idx = sorted_idx[..., :max_topk]\n        top_data = torch.where(\n            mask_top, torch.ones_like(top_idx, dtype=previous_mask.dtype), torch.zeros_like(top_idx, dtype=previous_mask.dtype)\n        )\n        topmass_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=top_idx,\n            data=top_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        # (2) Aggressive statistical tail (adaptively shrunk)\n        rel_mass_covered = cumsum[..., -1:] / (static_denominator + 1e-8)\n        tail_scale = torch.where(rel_mass_covered > 0.97, 0.18, 0.35)\n        min_base = max(num_base_samples, 3)\n        budget = torch.ceil(budget.float() * tail_scale).long()\n        budget = torch.clamp(budget, min=min_base)\n        max_budget = max(min_base, int(0.045 * sampling_range))\n        budget = torch.clamp(budget, max=max_budget)\n        # Remove tail for ultra-short cases where top mass covers almost all tokens\n        budget = torch.where(k_97 >= seq_len_keys - 2, torch.full_like(budget, 0), budget)\n\n        # Further prune for extremely low denominator regions (ultra-sparse/irrelevant queries)\n        if estimated_denominator is not None:\n            min_mask_threshold = 0.012 * static_denominator.max()\n            mask_sparse_rows = estimated_denominator < min_mask_threshold\n            if torch.any(mask_sparse_rows):\n                budget = budget.masked_fill(mask_sparse_rows, 0)\n        # For both static and estimated denominator low, prune further\n        static_low_threshold = 0.018 * static_denominator.max()\n        both_sparse = (estimated_denominator < min_mask_threshold) & (static_denominator < static_low_threshold)\n        if torch.any(both_sparse):\n            budget = budget.masked_fill(both_sparse, 0)\n\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n        adaptive_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n\n        # Merge deterministic top-mass, aggressive tail, and previous mask\n        merged = previous_mask.merge_mask(topmass_mask, inplace=False).merge_mask(adaptive_mask, inplace=False)\n        # EVOLVE-BLOCK-END\n        return merged\n\n    @classmethod\n    def create_from_config(cls, config: MaskerConfig) -> \"OpenEvolveMasker\":\n        \"\"\"Create OpenEvolve masker instance from configuration.\n\n        Args:\n            config: Configuration for the OpenEvolve masker.\n\n        Returns:\n            Instance of the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the config type is invalid.\n        \"\"\"\n        # not checking for config type here since we will be replacing this masker class\n        # with the new masker class in the evaluator.py file\n        return cls(config)\n\n```\nKey features: Performs well on density (-0.0702), Performs well on error (-0.0364), Performs well on combined_score (-0.0533)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: -0.0534)\n```python\n\"\"\"OpenEvolve masker implementation.\n\nThis module provides the OpenEvolve masker that implements evolutionary attention patterns.\nThe current implementation is a bare metal version that returns the previous mask.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nimport torch\nfrom scipy.stats import norm\n\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers import (\n    ResearchMasker,\n)\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers.base import (\n    MaskerConfig,\n    MaskerRegistry,\n)\nfrom sparse_attention_hub.sparse_attention.utils.mask import Mask\nfrom sparse_attention_hub.sparse_attention.utils.mask_attention_utils import (\n    _get_num_key_value_groups,\n    apply_inv_mask_sum,\n    create_sampling_mask_with_per_head_budget,\n    repeat_kv,\n)\n\n\n@dataclass\nclass OpenEvolveMaskerConfig(MaskerConfig):\n    \"\"\"Configuration for OpenEvolveMasker.\n\n    This configuration class inherits from MaskerConfig and provides\n    parameters for the attention mechanism evolved out of openevolve.\n    empty placeholder\n    \"\"\"\n\n    pass\n\n\n@MaskerRegistry.register(OpenEvolveMaskerConfig)\nclass OpenEvolveMasker(ResearchMasker):\n    \"\"\"OpenEvolve masker for evolutionary attention computation.\n\n    This masker implements evolutionary attention patterns that adapt over time.\n    The current implementation is a bare metal version that returns the previous mask.\n\n    Attributes:\n        evolution_rate: The rate of evolution for attention patterns.\n            This value is set from the configuration and controls how quickly\n            the attention patterns evolve.\n\n    Important Notes:\n        - This is a bare metal implementation that simply returns the previous mask.\n        - Future implementations will include evolutionary algorithms for attention pattern optimization.\n        - The evolution_rate parameter is currently unused but will be utilized in future versions.\n\n    Example:\n        >>> config = OpenEvolveMaskerConfig(evolution_rate=1.0)\n        >>> masker = OpenEvolveMasker(config)\n        >>> # Use masker.add_mask() to apply evolutionary attention patterns\n    \"\"\"\n\n    def __init__(self, config: OpenEvolveMaskerConfig) -> None:\n        \"\"\"Initialize OpenEvolve masker with configuration.\n\n        Args:\n            config: Configuration object containing the evolution rate and other\n                parameters for the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the evolution_rate in config is negative.\n                This validation is performed in the config's __post_init__ method.\n        \"\"\"\n        self.base_rate_sampling = 0.01\n        self.epsilon = 0.3\n        self.delta = 0.3\n        self.init_offset = 0.001\n        self.local_offset = 0.001\n        # Shrink factor that trades accuracy for sparsity (0.7-0.8 is typical)\n        self.budget_scale = 0.75\n\n        super().__init__(config)\n\n    def _compute_exp_attention_scores(\n        self,\n        queries: torch.Tensor,\n        keys: torch.Tensor,\n        scaling: float,\n        attention_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute exponential attention scores with numerical stability.\"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        raw_scores = torch.matmul(queries, keys.transpose(-2, -1)) * scaling\n        if attention_mask is not None:\n            raw_scores = raw_scores + attention_mask[:, :, :, : keys.shape[-2]]\n        max_scores = torch.max(raw_scores, dim=-1, keepdim=True)[0]\n        return torch.exp(raw_scores - max_scores)\n\n    def _get_sampling_range(self, seq_len_keys: int) -> tuple[int, int, int]:\n        \"\"\"Get sampling range and validate it.\n\n        Args:\n            seq_len_keys: Number of keys in the sequence.\n\n        Returns:\n            Tuple of (start_idx, end_idx, sampling_range).\n\n        Raises:\n            ValueError: If the computed sampling range is invalid.\n        \"\"\"\n        # Compute start index\n        if isinstance(self.init_offset, float):\n            start_idx: int = int(self.init_offset * seq_len_keys)\n        else:\n            start_idx = self.init_offset\n\n        # Compute end index\n        if isinstance(self.local_offset, float):\n            end_idx: int = seq_len_keys - int(self.local_offset * seq_len_keys)\n        else:\n            end_idx = seq_len_keys - self.local_offset\n\n        sampling_range = end_idx - start_idx\n\n        if sampling_range <= 0:\n            raise ValueError(f\"Invalid sampling range: {sampling_range}\")\n\n        return start_idx, end_idx, sampling_range\n\n    def _get_base_sample_count(self, sampling_range: int) -> int:\n        \"\"\"Get number of base samples based on configuration.\"\"\"\n        # Ensure at least 3 samples since it is used for std estimation\n        # (two samples often yield zero variance and unstable budgets)\n        if isinstance(self.base_rate_sampling, int):\n            return max(3, self.base_rate_sampling)\n        return max(3, int(self.base_rate_sampling * sampling_range))\n\n    def _get_std_estimate_using_base_sample(\n        self,\n        expwts: torch.Tensor,\n        batch_size: int,\n        num_heads: int,\n        seq_len_queries: int,\n        seq_len_keys: int,\n        start_idx: int,\n        end_idx: int,\n        num_base_samples: int,\n        dtype: torch.dtype,\n    ) -> tuple[Mask, torch.Tensor]:\n        \"\"\"Get standard deviation estimate using base sampling and create base mask.\"\"\"\n        # Create base sampling indices\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n\n        # Extract values and compute std\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        total_rows = batch_size * num_heads * seq_len_queries\n        row_sampled_values = sampled_values.view(total_rows, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n\n        # Create base sampling mask\n        sampling_range = end_idx - start_idx\n        base_data = torch.full_like(\n            base_row_wise_idx, num_base_samples / sampling_range, dtype=expwts.dtype\n        )\n\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=dtype,\n        )\n\n        return base_mask, std_estimate\n\n    def _compute_adaptive_budget(\n        self,\n        std_estimate: torch.Tensor,\n        estimated_denominator: torch.Tensor,\n        sampling_range: int,\n    ) -> torch.Tensor:\n        \"\"\"Compute adaptive budget based on statistical bounds.\"\"\"\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        # Ensure budget is positive and within bounds\n        budget = torch.clamp(\n            budget_squared,\n            min=1.0,  # Minimum 1 sample\n            max=float(sampling_range),  # Maximum sampling_range samples\n        ).long()\n\n        return budget\n\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"Add adaptive sampling mask to attention computation.\n\n        This method implements the core adaptive sampling logic. It combines base\n        sampling with adaptive budget allocation based on statistical error bounds.\n\n        Args:\n            keys: Key tensor with shape (batch_size, num_heads, seq_len_keys, head_dim).\n            queries: Query tensor with shape (batch_size, num_heads, seq_len_queries, head_dim).\n            values: Value tensor with shape (batch_size, num_heads, seq_len_keys, head_dim).\n            attention_mask: Attention mask tensor indicating which positions are valid.\n            sparse_meta_data: Dictionary containing sparse attention metadata.\n            previous_mask: Previous attention mask to merge with the new adaptive sampling mask.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A new Mask object representing the attention pattern after applying\n            adaptive sampling.\n\n        Raises:\n            ValueError: If the sampling range is invalid.\n        \"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n\n        # EVOLVE-BLOCK-START\n\n        # PARAMETERS to be set - ITERATION 3: Dynamic parameter adaptation (BEST PERFORMANCE)\n        # Adaptive parameters based on sequence length\n        seq_len_keys = keys.shape[-2]\n        if seq_len_keys > 10000:\n            self.base_rate_sampling = 0.02  # Higher sampling for very long sequences\n            self.epsilon = 0.2  # Tighter bounds for long sequences\n            self.delta = 0.2\n        elif seq_len_keys > 5000:\n            self.base_rate_sampling = 0.018  # Medium sampling for long sequences\n            self.epsilon = 0.22  # Medium bounds\n            self.delta = 0.22\n        else:\n            self.base_rate_sampling = 0.015  # Default for shorter sequences\n            self.epsilon = 0.25\n            self.delta = 0.25\n\n        self.init_offset = 0.002\n        self.local_offset = 0.002\n\n        # Pre-compute delta_ppf for efficiency\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        # Extract dimensions and compute attention scores\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size,\n            dims.num_heads,\n            dims.seq_len_queries,\n            dims.seq_len_keys,\n        )\n        # Compute attention scores after removing attention_mask\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # Get sampling parameters\n        start_idx, end_idx, sampling_range = self._get_sampling_range(seq_len_keys)\n        num_base_samples = self._get_base_sample_count(sampling_range)\n\n        # Create base sampling mask and estimate std\n        base_sampling_mask, std_estimate = self._get_std_estimate_using_base_sample(\n            expwts,\n            batch_size,\n            num_heads,\n            seq_len_queries,\n            seq_len_keys,\n            start_idx,\n            end_idx,\n            num_base_samples,\n            previous_mask.dtype,\n        )\n        # Compute denominators and budget\n        sampled_denominator = apply_inv_mask_sum(expwts, base_sampling_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n        budget = self._compute_adaptive_budget(\n            std_estimate, estimated_denominator, sampling_range\n        )\n\n        # ----------------------------------------------------------- #\n        # Hybrid Top-mass + Adaptive Aggressive Tail\n        #   (1) Deterministically select minimal top-k to cover 98% exp(attn) mass.\n        #   (2) Aggressive statistical sampling for remaining tail, shrinking budget further where possible.\n        # ----------------------------------------------------------- #\n        # (1) Top-mass deterministic: minimal top-k for \u226598% of mass per row\n        # IMPROVEMENT: Switch to 97% threshold and cap max_topk more aggressively for lower density, but keep k at least 1 for coverage\n        sorted_expwts, sorted_idx = torch.sort(expwts, dim=-1, descending=True)\n        cumsum = torch.cumsum(sorted_expwts, dim=-1)\n        total = cumsum[..., -1:]\n        frac = cumsum / (total + 1e-8)\n        k_97 = (frac < 0.97).sum(dim=-1, keepdim=True) + 1\n        # More aggressively clamp: at least 1, at most 10 or seq_len_keys // 10 + 1 (for greater sparsity)\n        k_97 = torch.clamp(k_97, min=1, max=min(10, seq_len_keys // 10 + 1))\n        max_topk = int(k_97.max())\n        rows = torch.arange(max_topk, device=expwts.device)[None, None, None, :]\n        mask_top = rows < k_97\n        top_idx = sorted_idx[..., :max_topk]\n        top_data = torch.where(\n            mask_top, torch.ones_like(top_idx, dtype=previous_mask.dtype), torch.zeros_like(top_idx, dtype=previous_mask.dtype)\n        )\n        topmass_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=top_idx,\n            data=top_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        # (2) Aggressive statistical tail\n        min_base = max(num_base_samples, 2)\n        # Use a tighter scale (lower density) for tail, with more aggressive shrink for high-mass rows\n        rel_mass_covered = cumsum[..., -1:] / (static_denominator + 1e-8)\n        scale = torch.where(rel_mass_covered > 0.97, 0.19, 0.38)\n        budget = torch.ceil(budget.float() * scale).long()\n        budget = torch.clamp(budget, min=min_base)\n        max_budget = max(min_base, int(0.05 * sampling_range))\n        budget = torch.clamp(budget, max=max_budget)\n        # For short rows (almost all mass is top-mass), zero tail budget\n        budget = torch.where(k_97 >= seq_len_keys - 2, torch.full_like(budget, 0), budget)\n\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n        adaptive_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n        # Return mask: previous \u222a top-mass \u222a adaptive-tail\n        merged = previous_mask.merge_mask(topmass_mask, inplace=False).merge_mask(adaptive_mask, inplace=False)\n        return merged\n        # EVOLVE-BLOCK-END\n\n    @classmethod\n    def create_from_config(cls, config: MaskerConfig) -> \"OpenEvolveMasker\":\n        \"\"\"Create OpenEvolve masker instance from configuration.\n\n        Args:\n            config: Configuration for the OpenEvolve masker.\n\n        Returns:\n            Instance of the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the config type is invalid.\n        \"\"\"\n        # not checking for config type here since we will be replacing this masker class\n        # with the new masker class in the evaluator.py file\n        return cls(config)\n\n```\nKey features: Alternative approach to density, Alternative approach to error\n\n\n### Program D2 (Score: -0.0534)\n```python\n\"\"\"OpenEvolve masker implementation.\n\nThis module provides the OpenEvolve masker that implements evolutionary attention patterns.\nThe current implementation is a bare metal version that returns the previous mask.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nimport torch\nfrom scipy.stats import norm\n\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers import (\n    ResearchMasker,\n)\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers.base import (\n    MaskerConfig,\n    MaskerRegistry,\n)\nfrom sparse_attention_hub.sparse_attention.utils.mask import Mask\nfrom sparse_attention_hub.sparse_attention.utils.mask_attention_utils import (\n    _get_num_key_value_groups,\n    apply_inv_mask_sum,\n    create_sampling_mask_with_per_head_budget,\n    repeat_kv,\n)\n\n\n@dataclass\nclass OpenEvolveMaskerConfig(MaskerConfig):\n    \"\"\"Configuration for OpenEvolveMasker.\n\n    This configuration class inherits from MaskerConfig and provides\n    parameters for the attention mechanism evolved out of openevolve.\n    empty placeholder\n    \"\"\"\n\n    pass\n\n\n@MaskerRegistry.register(OpenEvolveMaskerConfig)\nclass OpenEvolveMasker(ResearchMasker):\n    \"\"\"OpenEvolve masker for evolutionary attention computation.\n\n    This masker implements evolutionary attention patterns that adapt over time.\n    The current implementation is a bare metal version that returns the previous mask.\n\n    Attributes:\n        evolution_rate: The rate of evolution for attention patterns.\n            This value is set from the configuration and controls how quickly\n            the attention patterns evolve.\n\n    Important Notes:\n        - This is a bare metal implementation that simply returns the previous mask.\n        - Future implementations will include evolutionary algorithms for attention pattern optimization.\n        - The evolution_rate parameter is currently unused but will be utilized in future versions.\n\n    Example:\n        >>> config = OpenEvolveMaskerConfig(evolution_rate=1.0)\n        >>> masker = OpenEvolveMasker(config)\n        >>> # Use masker.add_mask() to apply evolutionary attention patterns\n    \"\"\"\n\n    def __init__(self, config: OpenEvolveMaskerConfig) -> None:\n        \"\"\"Initialize OpenEvolve masker with configuration.\n\n        Args:\n            config: Configuration object containing the evolution rate and other\n                parameters for the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the evolution_rate in config is negative.\n                This validation is performed in the config's __post_init__ method.\n        \"\"\"\n        self.base_rate_sampling = 0.01\n        self.epsilon = 0.3\n        self.delta = 0.3\n        self.init_offset = 0.001\n        self.local_offset = 0.001\n\n        super().__init__(config)\n\n    def _compute_exp_attention_scores(\n        self,\n        queries: torch.Tensor,\n        keys: torch.Tensor,\n        scaling: float,\n        attention_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute exponential attention scores with numerical stability.\"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        raw_scores = torch.matmul(queries, keys.transpose(-2, -1)) * scaling\n        if attention_mask is not None:\n            raw_scores = raw_scores + attention_mask[:, :, :, : keys.shape[-2]]\n        max_scores = torch.max(raw_scores, dim=-1, keepdim=True)[0]\n        return torch.exp(raw_scores - max_scores)\n\n    def _get_sampling_range(self, seq_len_keys: int) -> tuple[int, int, int]:\n        \"\"\"Get sampling range and validate it.\n\n        Args:\n            seq_len_keys: Number of keys in the sequence.\n\n        Returns:\n            Tuple of (start_idx, end_idx, sampling_range).\n\n        Raises:\n            ValueError: If the computed sampling range is invalid.\n        \"\"\"\n        # Compute start index\n        if isinstance(self.init_offset, float):\n            start_idx: int = int(self.init_offset * seq_len_keys)\n        else:\n            start_idx = self.init_offset\n\n        # Compute end index\n        if isinstance(self.local_offset, float):\n            end_idx: int = seq_len_keys - int(self.local_offset * seq_len_keys)\n        else:\n            end_idx = seq_len_keys - self.local_offset\n\n        sampling_range = end_idx - start_idx\n\n        if sampling_range <= 0:\n            raise ValueError(f\"Invalid sampling range: {sampling_range}\")\n\n        return start_idx, end_idx, sampling_range\n\n    def _get_base_sample_count(self, sampling_range: int) -> int:\n        \"\"\"Get number of base samples based on configuration.\"\"\"\n        # Ensure at least 2 samples since it is used for std estimation\n        if isinstance(self.base_rate_sampling, int):\n            return max(2, self.base_rate_sampling)\n        return max(2, int(self.base_rate_sampling * sampling_range))\n\n    def _get_std_estimate_using_base_sample(\n        self,\n        expwts: torch.Tensor,\n        batch_size: int,\n        num_heads: int,\n        seq_len_queries: int,\n        seq_len_keys: int,\n        start_idx: int,\n        end_idx: int,\n        num_base_samples: int,\n        dtype: torch.dtype,\n    ) -> tuple[Mask, torch.Tensor]:\n        \"\"\"Get standard deviation estimate using base sampling and create base mask.\"\"\"\n        # Create base sampling indices\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n\n        # Extract values and compute std\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        total_rows = batch_size * num_heads * seq_len_queries\n        row_sampled_values = sampled_values.view(total_rows, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n\n        # Create base sampling mask\n        sampling_range = end_idx - start_idx\n        base_data = torch.full_like(\n            base_row_wise_idx, num_base_samples / sampling_range, dtype=expwts.dtype\n        )\n\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=dtype,\n        )\n\n        return base_mask, std_estimate\n\n    def _compute_adaptive_budget(\n        self,\n        std_estimate: torch.Tensor,\n        estimated_denominator: torch.Tensor,\n        sampling_range: int,\n    ) -> torch.Tensor:\n        \"\"\"Compute adaptive budget based on statistical bounds.\"\"\"\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        # Ensure budget is positive and within bounds\n        budget = torch.clamp(\n            budget_squared,\n            min=1.0,  # Minimum 1 sample\n            max=float(sampling_range),  # Maximum sampling_range samples\n        ).long()\n\n        return budget\n\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"Add adaptive sampling mask to attention computation.\n\n        This method implements the core adaptive sampling logic. It combines base\n        sampling with adaptive budget allocation based on statistical error bounds.\n\n        Args:\n            keys: Key tensor with shape (batch_size, num_heads, seq_len_keys, head_dim).\n            queries: Query tensor with shape (batch_size, num_heads, seq_len_queries, head_dim).\n            values: Value tensor with shape (batch_size, num_heads, seq_len_keys, head_dim).\n            attention_mask: Attention mask tensor indicating which positions are valid.\n            sparse_meta_data: Dictionary containing sparse attention metadata.\n            previous_mask: Previous attention mask to merge with the new adaptive sampling mask.\n            **kwargs: Additional keyword arguments.\n\n        Returns:\n            A new Mask object representing the attention pattern after applying\n            adaptive sampling.\n\n        Raises:\n            ValueError: If the sampling range is invalid.\n        \"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n\n        # EVOLVE-BLOCK-START\n\n        # PARAMETERS to be set - ITERATION 3: Dynamic parameter adaptation (BEST PERFORMANCE)\n        # Adaptive parameters based on sequence length\n        seq_len_keys = keys.shape[-2]\n        if seq_len_keys > 10000:\n            self.base_rate_sampling = 0.02  # Higher sampling for very long sequences\n            self.epsilon = 0.2  # Tighter bounds for long sequences\n            self.delta = 0.2\n        elif seq_len_keys > 5000:\n            self.base_rate_sampling = 0.018  # Medium sampling for long sequences\n            self.epsilon = 0.22  # Medium bounds\n            self.delta = 0.22\n        else:\n            self.base_rate_sampling = 0.015  # Default for shorter sequences\n            self.epsilon = 0.25\n            self.delta = 0.25\n\n        self.init_offset = 0.002\n        self.local_offset = 0.002\n\n        # Pre-compute delta_ppf for efficiency\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        # Extract dimensions and compute attention scores\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size,\n            dims.num_heads,\n            dims.seq_len_queries,\n            dims.seq_len_keys,\n        )\n        # Compute attention scores after removing attention_mask\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # Get sampling parameters\n        start_idx, end_idx, sampling_range = self._get_sampling_range(seq_len_keys)\n        num_base_samples = self._get_base_sample_count(sampling_range)\n\n        # Create base sampling mask and estimate std\n        base_sampling_mask, std_estimate = self._get_std_estimate_using_base_sample(\n            expwts,\n            batch_size,\n            num_heads,\n            seq_len_queries,\n            seq_len_keys,\n            start_idx,\n            end_idx,\n            num_base_samples,\n            previous_mask.dtype,\n        )\n        # Compute denominators and budget\n        sampled_denominator = apply_inv_mask_sum(expwts, base_sampling_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n        budget = self._compute_adaptive_budget(\n            std_estimate, estimated_denominator, sampling_range\n        )\n\n        # ------------------------------------------------------------------ #\n        # Improved: Hybrid deterministic top-mass + aggressive adaptive tail\n        # ------------------------------------------------------------------ #\n\n        # (1) Deterministic selection: minimal top-k to cover 97% exp(attn) mass (less conservative for more sparsity)\n        sorted_expwts, sorted_idx = torch.sort(expwts, dim=-1, descending=True)\n        cumsum = torch.cumsum(sorted_expwts, dim=-1)\n        total = cumsum[..., -1:]\n        k_97 = (cumsum / (total + 1e-8) < 0.97).sum(dim=-1, keepdim=True) + 1\n        # Clamp: at least 1, at most 10 or (len//10 + 1) for efficiency and sparsity\n        k_97 = torch.clamp(k_97, min=1, max=min(10, seq_len_keys // 10 + 1))\n        max_topk = int(k_97.max())\n        arange_topk = torch.arange(max_topk, device=expwts.device)[None, None, None, :]\n        mask_top = arange_topk < k_97\n        top_idx = sorted_idx[..., :max_topk]\n        top_data = torch.where(\n            mask_top, torch.ones_like(top_idx, dtype=previous_mask.dtype), torch.zeros_like(top_idx, dtype=previous_mask.dtype)\n        )\n        topmass_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=top_idx,\n            data=top_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        # (2) Aggressive statistical tail: shrink budget and cap further for sparsity\n        rel_mass_covered = cumsum[..., -1:] / (static_denominator + 1e-8)\n        tail_scale = torch.where(rel_mass_covered > 0.97, 0.19, 0.38)\n        min_base = max(num_base_samples, 2)\n        budget = torch.ceil(budget.float() * tail_scale).long()\n        budget = torch.clamp(budget, min=min_base)\n        max_budget = max(min_base, int(0.05 * sampling_range))\n        budget = torch.clamp(budget, max=max_budget)\n        # Ignore tail for ultra-short rows (where 97% mass covers almost all tokens)\n        budget = torch.where(k_97 >= seq_len_keys - 2, torch.full_like(budget, 0), budget)\n\n        # Create adaptive sampling mask\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n        adaptive_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n\n        # Final: mask is (prev \u222a top-mass \u222a tail)\n        merged = previous_mask.merge_mask(topmass_mask, inplace=False).merge_mask(adaptive_mask, inplace=False)\n        return merged\n        # EVOLVE-BLOCK-END\n\n    @classmethod\n    def create_from_config(cls, config: MaskerConfig) -> \"OpenEvolveMasker\":\n        \"\"\"Create OpenEvolve masker instance from configuration.\n\n        Args:\n            config: Configuration for the OpenEvolve masker.\n\n        Returns:\n            Instance of the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the config type is invalid.\n        \"\"\"\n        # not checking for config type here since we will be replacing this masker class\n        # with the new masker class in the evaluator.py file\n        return cls(config)\n\n```\nKey features: Alternative approach to density, Alternative approach to error\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: -0.0532, Type: Exploratory)\n```python\n\"\"\"OpenEvolve masker implementation.\n\nThis module provides the OpenEvolve masker that implements evolutionary attention patterns.\nThe current implementation is a bare metal version that returns the previous mask.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nimport torch\nfrom scipy.stats import norm\n\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers import (\n    ResearchMasker,\n)\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers.base import (\n    MaskerConfig,\n    MaskerRegistry,\n)\nfrom sparse_attention_hub.sparse_attention.utils.mask import Mask\nfrom sparse_attention_hub.sparse_attention.utils.mask_attention_utils import (\n    _get_num_key_value_groups,\n    apply_inv_mask_sum,\n    create_sampling_mask_with_per_head_budget,\n    repeat_kv,\n)\n\n\n@dataclass\nclass OpenEvolveMaskerConfig(MaskerConfig):\n    \"\"\"Configuration for OpenEvolveMasker.\n\n    This configuration class inherits from MaskerConfig and provides\n    parameters for the attention mechanism evolved out of openevolve.\n    empty placeholder\n    \"\"\"\n\n    pass\n\n\n@MaskerRegistry.register(OpenEvolveMaskerConfig)\nclass OpenEvolveMasker(ResearchMasker):\n    \"\"\"OpenEvolve masker for evolutionary attention computation.\n\n    This masker implements evolutionary attention patterns that adapt over time.\n    The current implementation is a bare metal version that returns the previous mask.\n\n    Attributes:\n        evolution_rate: The rate of evolution for attention patterns.\n            This value is set from the configuration and controls how quickly\n            the attention patterns evolve.\n\n    Important Notes:\n        - This is a bare metal implementation that simply returns the previous mask.\n        - Future implementations will include evolutionary algorithms for attention pattern optimization.\n        - The evolution_rate parameter is currently unused but will be utilized in future versions.\n\n    Example:\n        >>> config = OpenEvolveMaskerConfig(evolution_rate=1.0)\n        >>> masker = OpenEvolveMasker(config)\n        >>> # Use masker.add_mask() to apply evolutionary attention patterns\n    \"\"\"\n\n    def __init__(self, config: OpenEvolveMaskerConfig) -> None:\n        \"\"\"Initialize OpenEvolve masker with configuration.\n\n        Args:\n            config: Configuration object containing the evolution rate and other\n                parameters for the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the evolution_rate in config is negative.\n                This validation is performed in the config's __post_init__ method.\n        \"\"\"\n        self.base_rate_sampling = 0.01\n        self.epsilon = 0.3\n        self.delta = 0.3\n        self.init_offset = 0.001\n        self.local_offset = 0.001\n\n        super().__init__(config)\n\n    def _compute_exp_attention_scores(\n        self,\n        queries: torch.Tensor,\n        keys: torch.Tensor,\n        scaling: float,\n        attention_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute exponential attention scores with numerical stability.\"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        raw_scores = torch.matmul(queries, keys.transpose(-2, -1)) * scaling\n        if attention_mask is not None:\n            raw_scores = raw_scores + attention_mask[:, :, :, : keys.shape[-2]]\n        max_scores = torch.max(raw_scores, dim=-1, keepdim=True)[0]\n        return torch.exp(raw_scores - max_scores)\n\n    def _get_sampling_range(self, seq_len_keys: int) -> tuple[int, int, int]:\n        \"\"\"Get sampling range and validate it.\n\n        Args:\n            seq_len_keys: Number of keys in the sequence.\n\n        Returns:\n            Tuple of (start_idx, end_idx, sampling_range).\n\n        Raises:\n            ValueError: If the computed sampling range is invalid.\n        \"\"\"\n        # Compute start index\n        if isinstance(self.init_offset, float):\n            start_idx: int = int(self.init_offset * seq_len_keys)\n        else:\n            start_idx = self.init_offset\n\n        # Compute end index\n        if isinstance(self.local_offset, float):\n            end_idx: int = seq_len_keys - int(self.local_offset * seq_len_keys)\n        else:\n            end_idx = seq_len_keys - self.local_offset\n\n        sampling_range = end_idx - start_idx\n\n        if sampling_range <= 0:\n            raise ValueError(f\"Invalid sampling range: {sampling_range}\")\n\n        return start_idx, end_idx, sampling_range\n\n    def _get_base_sample_count(self, sampling_range: int) -> int:\n        \"\"\"Get number of base samples based on configuration.\"\"\"\n        # Ensure at least 2 samples since it is used for std estimation\n        if isinstance(self.base_rate_sampling, int):\n            return max(2, self.base_rate_sampling)\n        return max(2, int(self.base_rate_sampling * sampling_range))\n\n    def _get_std_estimate_using_base_sample(\n        self,\n        expwts: torch.Tensor,\n        batch_size: int,\n        num_heads: int,\n        seq_len_queries: int,\n        seq_len_keys: int,\n        start_idx: int,\n        end_idx: int,\n        num_base_samples: int,\n        dtype: torch.dtype,\n    ) -> tuple[Mask, torch.Tensor]:\n        \"\"\"Get standard deviation estimate using base sampling and create base mask.\"\"\"\n        # Create base sampling indices\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n\n        # Extract values and compute std\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        total_rows = batch_size * num_heads * seq_len_queries\n        row_sampled_values = sampled_values.view(total_rows, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n\n        # Create base sampling mask\n        sampling_range = end_idx - start_idx\n        base_data = torch.full_like(\n            base_row_wise_idx, num_base_samples / sampling_range, dtype=expwts.dtype\n        )\n\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=dtype,\n        )\n\n        return base_mask, std_estimate\n\n    def _compute_adaptive_budget(\n        self,\n        std_estimate: torch.Tensor,\n        estimated_denominator: torch.Tensor,\n        sampling_range: int,\n    ) -> torch.Tensor:\n        \"\"\"Compute adaptive budget based on statistical bounds.\"\"\"\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        # Ensure budget is positive and within bounds\n        budget = torch.clamp(\n            budget_squared,\n            min=1.0,  # Minimum 1 sample\n            max=float(sampling_range),  # Maximum sampling_range samples\n        ).long()\n\n        return budget\n\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"\n        Improved hybrid: deterministically select minimal set of top keys per query to cover ~97% exp(attn) mass,\n        and sample adaptively and aggressively only the remaining low-mass tail. This produces much sparser masks while preserving approximation.\n        \"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n\n        seq_len_keys = keys.shape[-2]\n        # Dynamic parameter adaptation (as in best versions)\n        if seq_len_keys > 10000:\n            self.base_rate_sampling = 0.02\n            self.epsilon = 0.2\n            self.delta = 0.2\n        elif seq_len_keys > 5000:\n            self.base_rate_sampling = 0.018\n            self.epsilon = 0.22\n            self.delta = 0.22\n        else:\n            self.base_rate_sampling = 0.015\n            self.epsilon = 0.25\n            self.delta = 0.25\n\n        self.init_offset = 0.002\n        self.local_offset = 0.002\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size, dims.num_heads, dims.seq_len_queries, dims.seq_len_keys\n        )\n\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # Get main sampling parameters\n        start_idx, end_idx, sampling_range = self._get_sampling_range(seq_len_keys)\n        num_base_samples = self._get_base_sample_count(sampling_range)\n        # Base sampling + std estimation (used for adaptive tail only)\n        base_sampling_mask, std_estimate = self._get_std_estimate_using_base_sample(\n            expwts,\n            batch_size,\n            num_heads,\n            seq_len_queries,\n            seq_len_keys,\n            start_idx,\n            end_idx,\n            num_base_samples,\n            previous_mask.dtype,\n        )\n        sampled_denominator = apply_inv_mask_sum(expwts, base_sampling_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n\n        # Compute error-controlled budget\n        budget = self._compute_adaptive_budget(\n            std_estimate, estimated_denominator, sampling_range\n        )\n\n        # -- Hybrid: select minimal number of keys covering \u226597% mass per row, rest gets aggressive tail sampling --\n        sorted_expwts, sorted_idx = torch.sort(expwts, dim=-1, descending=True)\n        cumsum = torch.cumsum(sorted_expwts, dim=-1)\n        total = cumsum[..., -1:]\n        frac = cumsum / (total + 1e-8)\n        # How many keys needed to reach \u226597% mass\n        k_97 = (frac < 0.97).sum(dim=-1, keepdim=True) + 1\n        k_97 = torch.clamp(k_97, min=1, max=min(10, seq_len_keys // 10 + 1))\n        max_topk = int(k_97.max())\n        arange_topk = torch.arange(max_topk, device=expwts.device)[None, None, None, :]\n        mask_top = arange_topk < k_97\n        top_idx = sorted_idx[..., :max_topk]\n        top_data = torch.where(\n            mask_top, torch.ones_like(top_idx, dtype=previous_mask.dtype), torch.zeros_like(top_idx, dtype=previous_mask.dtype)\n        )\n        topmass_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=top_idx,\n            data=top_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        # Aggressively shrink tail budget where possible (if most mass is covered)\n        rel_mass_covered = cumsum[..., -1:] / (static_denominator + 1e-8)\n        tail_scale = torch.where(rel_mass_covered > 0.97, 0.19, 0.38)\n        min_base = max(num_base_samples, 2)\n        budget = torch.ceil(budget.float() * tail_scale).long()\n        budget = torch.clamp(budget, min=min_base)\n        max_budget = max(min_base, int(0.05 * sampling_range))\n        budget = torch.clamp(budget, max=max_budget)\n        # Remove tail for ultra-short cases where top mass covers almost all tokens\n        budget = torch.where(k_97 >= seq_len_keys - 2, torch.full_like(budget, 0), budget)\n\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n        adaptive_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n\n        # Merge: previous \u222a topmass \u222a adaptive tail\n        merged = previous_mask.merge_mask(topmass_mask, inplace=False).merge_mask(adaptive_mask, inplace=False)\n        return merged\n\n    @classmethod\n    def create_from_config(cls, config: MaskerConfig) -> \"OpenEvolveMasker\":\n        \"\"\"Create OpenEvolve masker instance from configuration.\n\n        Args:\n            config: Configuration for the OpenEvolve masker.\n\n        Returns:\n            Instance of the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the config type is invalid.\n        \"\"\"\n        # not checking for config type here since we will be replacing this masker class\n        # with the new masker class in the evaluator.py file\n        return cls(config)\n\n```\nUnique approach: Modification: Change 1: Replace 115 lines with 121 lines, Alternative density approach, Alternative error approach\n\n\n### Inspiration 2 (Score: 33.3333, Type: High-Performer)\n```python\n\"\"\"OpenEvolve masker implementation.\n\nThis module provides the OpenEvolve masker that implements evolutionary attention patterns.\nThe current implementation is a bare metal version that returns the previous mask.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nimport torch\nfrom scipy.stats import norm\n\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers import (\n    ResearchMasker,\n)\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers.base import (\n    MaskerConfig,\n    MaskerRegistry,\n)\nfrom sparse_attention_hub.sparse_attention.utils.mask import Mask\nfrom sparse_attention_hub.sparse_attention.utils.mask_attention_utils import (\n    _get_num_key_value_groups,\n    apply_inv_mask_sum,\n    create_sampling_mask_with_per_head_budget,\n    repeat_kv,\n)\n\n\n@dataclass\nclass OpenEvolveMaskerConfig(MaskerConfig):\n    \"\"\"Configuration for OpenEvolveMasker.\n\n    This configuration class inherits from MaskerConfig and provides\n    parameters for the attention mechanism evolved out of openevolve.\n    empty placeholder\n    \"\"\"\n\n    pass\n\n\n@MaskerRegistry.register(OpenEvolveMaskerConfig)\nclass OpenEvolveMasker(ResearchMasker):\n    \"\"\"OpenEvolve masker for evolutionary attention computation.\n\n    This masker implements evolutionary attention patterns that adapt over time.\n    The current implementation is a bare metal version that returns the previous mask.\n\n    Attributes:\n        evolution_rate: The rate of evolution for attention patterns.\n            This value is set from the configuration and controls how quickly\n            the attention patterns evolve.\n\n    Important Notes:\n        - This is a bare metal implementation that simply returns the previous mask.\n        - Future implementations will include evolutionary algorithms for attention pattern optimization.\n        - The evolution_rate parameter is currently unused but will be utilized in future versions.\n\n    Example:\n        >>> config = OpenEvolveMaskerConfig(evolution_rate=1.0)\n        >>> masker = OpenEvolveMasker(config)\n        >>> # Use masker.add_mask() to apply evolutionary attention patterns\n    \"\"\"\n\n    def __init__(self, config: OpenEvolveMaskerConfig) -> None:\n        \"\"\"Initialize OpenEvolve masker with configuration.\n\n        Args:\n            config: Configuration object containing the evolution rate and other\n                parameters for the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the evolution_rate in config is negative.\n                This validation is performed in the config's __post_init__ method.\n        \"\"\"\n        self.base_rate_sampling = 0.01\n        self.epsilon = 0.3\n        self.delta = 0.3\n        self.init_offset = 0.001\n        self.local_offset = 0.001\n        # Shrink factor that trades accuracy for sparsity (0.7-0.8 is typical)\n        self.budget_scale = 0.75\n\n        super().__init__(config)\n\n    def _compute_exp_attention_scores(\n        self,\n        queries: torch.Tensor,\n        keys: torch.Tensor,\n        scaling: float,\n        attention_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute exponential attention scores with numerical stability.\"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        raw_scores = torch.matmul(queries, keys.transpose(-2, -1)) * scaling\n        if attention_mask is not None:\n            raw_scores = raw_scores + attention_mask[:, :, :, : keys.shape[-2]]\n        max_scores = torch.max(raw_scores, dim=-1, keepdim=True)[0]\n        return torch.exp(raw_scores - max_scores)\n\n    def _get_sampling_range(self, seq_len_keys: int) -> tuple[int, int, int]:\n        \"\"\"Get sampling range and validate it.\n\n        Args:\n            seq_len_keys: Number of keys in the sequence.\n\n        Returns:\n            Tuple of (start_idx, end_idx, sampling_range).\n\n        Raises:\n            ValueError: If the computed sampling range is invalid.\n        \"\"\"\n        # Compute start index\n        if isinstance(self.init_offset, float):\n            start_idx: int = int(self.init_offset * seq_len_keys)\n        else:\n            start_idx = self.init_offset\n\n        # Compute end index\n        if isinstance(self.local_offset, float):\n            end_idx: int = seq_len_keys - int(self.local_offset * seq_len_keys)\n        else:\n            end_idx = seq_len_keys - self.local_offset\n\n        sampling_range = end_idx - start_idx\n\n        if sampling_range <= 0:\n            raise ValueError(f\"Invalid sampling range: {sampling_range}\")\n\n        return start_idx, end_idx, sampling_range\n\n    def _get_base_sample_count(self, sampling_range: int) -> int:\n        \"\"\"Get number of base samples based on configuration.\"\"\"\n        # Ensure at least 3 samples since it is used for std estimation\n        # (two samples often yield zero variance and unstable budgets)\n        if isinstance(self.base_rate_sampling, int):\n            return max(3, self.base_rate_sampling)\n        return max(3, int(self.base_rate_sampling * sampling_range))\n\n    def _get_std_estimate_using_base_sample(\n        self,\n        expwts: torch.Tensor,\n        batch_size: int,\n        num_heads: int,\n        seq_len_queries: int,\n        seq_len_keys: int,\n        start_idx: int,\n        end_idx: int,\n        num_base_samples: int,\n        dtype: torch.dtype,\n    ) -> tuple[Mask, torch.Tensor]:\n        \"\"\"Get standard deviation estimate using base sampling and create base mask.\"\"\"\n        # Create base sampling indices\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n\n        # Extract values and compute std\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        total_rows = batch_size * num_heads * seq_len_queries\n        row_sampled_values = sampled_values.view(total_rows, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n\n        # Create base sampling mask\n        sampling_range = end_idx - start_idx\n        base_data = torch.full_like(\n            base_row_wise_idx, num_base_samples / sampling_range, dtype=expwts.dtype\n        )\n\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=dtype,\n        )\n\n        return base_mask, std_estimate\n\n    def _compute_adaptive_budget(\n        self,\n        std_estimate: torch.Tensor,\n        estimated_denominator: torch.Tensor,\n        sampling_range: int,\n    ) -> torch.Tensor:\n        \"\"\"Compute adaptive budget based on statistical bounds.\"\"\"\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        # Ensure budget is positive and within bounds\n        budget = torch.clamp(\n            budget_squared,\n            min=1.0,  # Minimum 1 sample\n            max=float(sampling_range),  # Maximum sampling_range samples\n        ).long()\n\n        return budget\n\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"\n        Hybrid: top-k high-mass coverage plus aggressive adaptive budget for optimal sparsity and quality.\n\n        - First, select the minimum number of tokens per row (query) that account for at least 98% of exp(atten) mass.\n        - Then, for any long-tail, sample adaptively with an aggressive statistical error budget.\n        - This minimizes density while preserving near-full attention approximation.\n\n        The mask is the union of: (a) high-mass top-k mask, (b) adaptive random mask, (c) any previous mask.\n        \"\"\"\n\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n\n        seq_len_keys = keys.shape[-2]\n        # More aggressive static for all seq lens, keeping parameterization stable\n        self.base_rate_sampling = 0.007\n        self.epsilon = 0.18\n        self.delta = 0.14\n        self.init_offset = 0.0\n        self.local_offset = 0.0\n\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        # Get tensor dimensions\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size, dims.num_heads, dims.seq_len_queries, dims.seq_len_keys\n        )\n\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # --------------- (1) Top-mass coverage ---------------\n        # For each (b, h, q), find minimum k s.t. cumsum(expwts) covers \u226598% of row mass.\n        sorted_expwts, sorted_idx = torch.sort(expwts, dim=-1, descending=True)\n        mass_cumsum = torch.cumsum(sorted_expwts, dim=-1)\n        total_mass = mass_cumsum[..., -1:]\n        mass_frac = mass_cumsum / (total_mass + 1e-8)\n        k1 = (mass_frac < 0.98).sum(dim=-1, keepdim=True) + 1  # at least 1\n        # Cap k1 for extreme cases for sparsity\n        max_topk = min(16, seq_len_keys // 8)\n        k1 = torch.clamp(k1, min=2, max=max_topk)\n\n        max_k1 = int(k1.max())\n        row_indices = torch.arange(max_k1, device=expwts.device)[None, None, None, :]\n        k1_mask = (row_indices < k1)\n        expanded_sorted_idx = sorted_idx[..., :max_k1]\n        k1_selected_idx = torch.where(\n            k1_mask,\n            expanded_sorted_idx,\n            torch.zeros_like(expanded_sorted_idx),\n        )\n        k1_data = torch.where(\n            k1_mask,\n            torch.ones_like(expanded_sorted_idx, dtype=previous_mask.dtype),\n            torch.zeros_like(expanded_sorted_idx, dtype=previous_mask.dtype),\n        )\n        topmass_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=k1_selected_idx,\n            data=k1_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        # --------------- (2) Aggressive adaptive tail mask ---------------\n        # Statistical tail sampling as before, but with more sparsity\n        start_idx, end_idx, sampling_range = 0, seq_len_keys, seq_len_keys\n        num_base_samples = self._get_base_sample_count(sampling_range)\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        row_sampled_values = sampled_values.view(batch_size * num_heads * seq_len_queries, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n        base_data = torch.full_like(\n            base_row_wise_idx, num_base_samples / sampling_range, dtype=expwts.dtype\n        )\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        sampled_denominator = apply_inv_mask_sum(expwts, base_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        # Even more aggressively capped for long tails\n        min_budget = num_base_samples\n        max_budget = max(min_budget, int(0.05 * sampling_range))\n        rel_std = std_estimate / (estimated_denominator / sampling_range + 1e-8)\n        # Ultra sparse for low-variance rows\n        shrink = torch.where(rel_std < 0.13, 0.27, 0.53)\n        budget = torch.clamp(torch.ceil(budget_squared * shrink).long(), min=min_budget, max=max_budget)\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n        # For rows with nearly flat dist, skip tail sampling (use only top-mass, keep at least min_budget)\n        flat_row = (rel_std < 0.05).squeeze(-1)\n        if torch.any(flat_row):\n            budget = torch.where(flat_row, torch.full_like(budget, min_budget), budget)\n            sampling_probabilities = torch.where(\n                flat_row, torch.full_like(sampling_probabilities, min_budget/sampling_range), sampling_probabilities\n            )\n\n        tail_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n\n        # Merge everything: previous mask OR topmass_mask OR tail_mask\n        merged_mask = previous_mask.merge_mask(topmass_mask, inplace=False).merge_mask(tail_mask, inplace=False)\n        return merged_mask\n\n    @classmethod\n    def create_from_config(cls, config: MaskerConfig) -> \"OpenEvolveMasker\":\n        \"\"\"Create OpenEvolve masker instance from configuration.\n\n        Args:\n            config: Configuration for the OpenEvolve masker.\n\n        Returns:\n            Instance of the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the config type is invalid.\n        \"\"\"\n        # not checking for config type here since we will be replacing this masker class\n        # with the new masker class in the evaluator.py file\n        return cls(config)\n\n```\nUnique approach: Modification: Change 1: Replace 128 lines with 144 lines, Excellent density (100.000), Excellent error (100.000)\n\n\n### Inspiration 3 (Score: 33.3333, Type: High-Performer)\n```python\n\"\"\"OpenEvolve masker implementation.\n\nThis module provides the OpenEvolve masker that implements evolutionary attention patterns.\nThe current implementation is a bare metal version that returns the previous mask.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nimport torch\nfrom scipy.stats import norm\n\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers import (\n    ResearchMasker,\n)\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers.base import (\n    MaskerConfig,\n    MaskerRegistry,\n)\nfrom sparse_attention_hub.sparse_attention.utils.mask import Mask\nfrom sparse_attention_hub.sparse_attention.utils.mask_attention_utils import (\n    _get_num_key_value_groups,\n    apply_inv_mask_sum,\n    create_sampling_mask_with_per_head_budget,\n    repeat_kv,\n)\n\n\n@dataclass\nclass OpenEvolveMaskerConfig(MaskerConfig):\n    \"\"\"Configuration for OpenEvolveMasker.\n\n    This configuration class inherits from MaskerConfig and provides\n    parameters for the attention mechanism evolved out of openevolve.\n    empty placeholder\n    \"\"\"\n\n    pass\n\n\n@MaskerRegistry.register(OpenEvolveMaskerConfig)\nclass OpenEvolveMasker(ResearchMasker):\n    \"\"\"OpenEvolve masker for evolutionary attention computation.\n\n    This masker implements evolutionary attention patterns that adapt over time.\n    The current implementation is a bare metal version that returns the previous mask.\n\n    Attributes:\n        evolution_rate: The rate of evolution for attention patterns.\n            This value is set from the configuration and controls how quickly\n            the attention patterns evolve.\n\n    Important Notes:\n        - This is a bare metal implementation that simply returns the previous mask.\n        - Future implementations will include evolutionary algorithms for attention pattern optimization.\n        - The evolution_rate parameter is currently unused but will be utilized in future versions.\n\n    Example:\n        >>> config = OpenEvolveMaskerConfig(evolution_rate=1.0)\n        >>> masker = OpenEvolveMasker(config)\n        >>> # Use masker.add_mask() to apply evolutionary attention patterns\n    \"\"\"\n\n    def __init__(self, config: OpenEvolveMaskerConfig) -> None:\n        \"\"\"Initialize OpenEvolve masker with configuration.\n\n        Args:\n            config: Configuration object containing the evolution rate and other\n                parameters for the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the evolution_rate in config is negative.\n                This validation is performed in the config's __post_init__ method.\n        \"\"\"\n        self.base_rate_sampling = 0.01\n        self.epsilon = 0.3\n        self.delta = 0.3\n        self.init_offset = 0.001\n        self.local_offset = 0.001\n        # New hyper-parameter: shrink the theoretical sampling\n        # budget by a constant factor to lower density while\n        # keeping the approximation quality virtually intact.\n        self.budget_scale = 0.75\n\n        super().__init__(config)\n\n    def _compute_exp_attention_scores(\n        self,\n        queries: torch.Tensor,\n        keys: torch.Tensor,\n        scaling: float,\n        attention_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute exponential attention scores with numerical stability.\"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        raw_scores = torch.matmul(queries, keys.transpose(-2, -1)) * scaling\n        if attention_mask is not None:\n            raw_scores = raw_scores + attention_mask[:, :, :, : keys.shape[-2]]\n        max_scores = torch.max(raw_scores, dim=-1, keepdim=True)[0]\n        return torch.exp(raw_scores - max_scores)\n\n    def _get_sampling_range(self, seq_len_keys: int) -> tuple[int, int, int]:\n        \"\"\"Get sampling range and validate it.\n\n        Args:\n            seq_len_keys: Number of keys in the sequence.\n\n        Returns:\n            Tuple of (start_idx, end_idx, sampling_range).\n\n        Raises:\n            ValueError: If the computed sampling range is invalid.\n        \"\"\"\n        # Compute start index\n        if isinstance(self.init_offset, float):\n            start_idx: int = int(self.init_offset * seq_len_keys)\n        else:\n            start_idx = self.init_offset\n\n        # Compute end index\n        if isinstance(self.local_offset, float):\n            end_idx: int = seq_len_keys - int(self.local_offset * seq_len_keys)\n        else:\n            end_idx = seq_len_keys - self.local_offset\n\n        sampling_range = end_idx - start_idx\n\n        if sampling_range <= 0:\n            raise ValueError(f\"Invalid sampling range: {sampling_range}\")\n\n        return start_idx, end_idx, sampling_range\n\n    def _get_base_sample_count(self, sampling_range: int) -> int:\n        \"\"\"Get number of base samples based on configuration.\"\"\"\n        # Increase minimum to 3 for better std stability, ceil for more robust estimation\n        if isinstance(self.base_rate_sampling, int):\n            return max(3, self.base_rate_sampling)\n        return max(3, int(torch.ceil(torch.tensor(self.base_rate_sampling * sampling_range)).item()))\n\n    def _get_std_estimate_using_base_sample(\n        self,\n        expwts: torch.Tensor,\n        batch_size: int,\n        num_heads: int,\n        seq_len_queries: int,\n        seq_len_keys: int,\n        start_idx: int,\n        end_idx: int,\n        num_base_samples: int,\n        dtype: torch.dtype,\n    ) -> tuple[Mask, torch.Tensor]:\n        \"\"\"Get standard deviation estimate using base sampling and create base mask.\"\"\"\n        # Create base sampling indices\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n\n        # Extract values and compute std\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        total_rows = batch_size * num_heads * seq_len_queries\n        row_sampled_values = sampled_values.view(total_rows, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n\n        # Create base sampling mask\n        sampling_range = end_idx - start_idx\n        base_data = torch.full_like(\n            base_row_wise_idx, num_base_samples / sampling_range, dtype=expwts.dtype\n        )\n\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=dtype,\n        )\n\n        return base_mask, std_estimate\n\n    def _compute_adaptive_budget(\n        self,\n        std_estimate: torch.Tensor,\n        estimated_denominator: torch.Tensor,\n        sampling_range: int,\n    ) -> torch.Tensor:\n        \"\"\"Compute adaptive budget based on statistical bounds.\"\"\"\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        # Ensure budget is positive and within bounds\n        budget = torch.clamp(\n            budget_squared,\n            min=1.0,  # Minimum 1 sample\n            max=float(sampling_range),  # Maximum sampling_range samples\n        ).long()\n\n        return budget\n\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"\n        Hybrid: deterministically select the minimal top-k (per query) to cover \u226598% of exp(attn) mass,\n        and sample adaptively (statistically) in the tail, giving extremely low density while preserving approximation.\n        \"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n\n        seq_len_keys = keys.shape[-2]\n        # Static ultra-sparse parameters for optimal density/quality\n        self.base_rate_sampling = 0.007\n        self.epsilon = 0.18\n        self.delta = 0.14\n        self.init_offset = 0.0\n        self.local_offset = 0.0\n\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size, dims.num_heads, dims.seq_len_queries, dims.seq_len_keys\n        )\n\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # 1. Top-mass deterministic: minimal top-k for \u226598% of exp(attn) mass per query row\n        sorted_expwts, sorted_idx = torch.sort(expwts, dim=-1, descending=True)\n        mass_cumsum = torch.cumsum(sorted_expwts, dim=-1)\n        total_mass = mass_cumsum[..., -1:]\n        frac = mass_cumsum / (total_mass + 1e-8)\n        k_mass = (frac < 0.98).sum(dim=-1, keepdim=True) + 1\n        # Cap k for sparsity/quality tradeoff\n        k_mass = torch.clamp(k_mass, min=2, max=min(16, seq_len_keys // 8))\n        max_k = int(k_mass.max())\n        arange_k = torch.arange(max_k, device=expwts.device)[None, None, None, :]\n        active_idx_mask = (arange_k < k_mass)\n        topk_idx = sorted_idx[..., :max_k]\n        topk_data = torch.where(\n            active_idx_mask,\n            torch.ones_like(topk_idx, dtype=previous_mask.dtype),\n            torch.zeros_like(topk_idx, dtype=previous_mask.dtype)\n        )\n        topk_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=topk_idx,\n            data=topk_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        # 2. Aggressive statistical tail, only for the remaining region (all keys)\n        start_idx, end_idx, sampling_range = 0, seq_len_keys, seq_len_keys\n        num_base_samples = max(2, int(self.base_rate_sampling * sampling_range))\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        row_sampled_values = sampled_values.view(batch_size * num_heads * seq_len_queries, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n        base_data = torch.full_like(base_row_wise_idx, num_base_samples / sampling_range, dtype=previous_mask.dtype)\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n        sampled_denominator = apply_inv_mask_sum(expwts, base_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        min_budget = num_base_samples\n        max_budget = max(min_budget, int(0.05 * sampling_range))\n        rel_std = std_estimate / (estimated_denominator / sampling_range + 1e-8)\n        # Even more aggressive shrink for almost uniform rows\n        shrink = torch.where(rel_std < 0.13, 0.27, 0.53)\n        budget = torch.clamp(torch.ceil(budget_squared * shrink).long(), min=min_budget, max=max_budget)\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n        # For rows with nearly flat dist, skip tail sampling (use only top-mass, keep at least min_budget)\n        flat_row = (rel_std < 0.05).squeeze(-1)\n        if torch.any(flat_row):\n            budget = torch.where(flat_row, torch.full_like(budget, min_budget), budget)\n            sampling_probabilities = torch.where(\n                flat_row, torch.full_like(sampling_probabilities, min_budget/sampling_range), sampling_probabilities\n            )\n\n        tail_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n\n        # Merge: previous OR topk OR tail\n        merged_mask = previous_mask.merge_mask(topk_mask, inplace=False).merge_mask(tail_mask, inplace=False)\n        return merged_mask\n\n    @classmethod\n    def create_from_config(cls, config: MaskerConfig) -> \"OpenEvolveMasker\":\n        \"\"\"Create OpenEvolve masker instance from configuration.\n\n        Args:\n            config: Configuration for the OpenEvolve masker.\n\n        Returns:\n            Instance of the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the config type is invalid.\n        \"\"\"\n        # not checking for config type here since we will be replacing this masker class\n        # with the new masker class in the evaluator.py file\n        return cls(config)\n\n```\nUnique approach: Modification: Change 1: Replace 134 lines with 124 lines, Excellent density (100.000), Excellent error (100.000)\n\n\n### Inspiration 4 (Score: -0.0562, Type: Exploratory)\n```python\n\"\"\"OpenEvolve masker implementation.\n\nThis module provides the OpenEvolve masker that implements evolutionary attention patterns.\nThe current implementation is a bare metal version that returns the previous mask.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nimport torch\nfrom scipy.stats import norm\n\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers import (\n    ResearchMasker,\n)\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers.base import (\n    MaskerConfig,\n    MaskerRegistry,\n)\nfrom sparse_attention_hub.sparse_attention.utils.mask import Mask\nfrom sparse_attention_hub.sparse_attention.utils.mask_attention_utils import (\n    _get_num_key_value_groups,\n    apply_inv_mask_sum,\n    create_sampling_mask_with_per_head_budget,\n    repeat_kv,\n)\n\n\n@dataclass\nclass OpenEvolveMaskerConfig(MaskerConfig):\n    \"\"\"Configuration for OpenEvolveMasker.\n\n    This configuration class inherits from MaskerConfig and provides\n    parameters for the attention mechanism evolved out of openevolve.\n    empty placeholder\n    \"\"\"\n\n    pass\n\n\n@MaskerRegistry.register(OpenEvolveMaskerConfig)\nclass OpenEvolveMasker(ResearchMasker):\n    \"\"\"OpenEvolve masker for evolutionary attention computation.\n\n    This masker implements evolutionary attention patterns that adapt over time.\n    The current implementation is a bare metal version that returns the previous mask.\n\n    Attributes:\n        evolution_rate: The rate of evolution for attention patterns.\n            This value is set from the configuration and controls how quickly\n            the attention patterns evolve.\n\n    Important Notes:\n        - This is a bare metal implementation that simply returns the previous mask.\n        - Future implementations will include evolutionary algorithms for attention pattern optimization.\n        - The evolution_rate parameter is currently unused but will be utilized in future versions.\n\n    Example:\n        >>> config = OpenEvolveMaskerConfig(evolution_rate=1.0)\n        >>> masker = OpenEvolveMasker(config)\n        >>> # Use masker.add_mask() to apply evolutionary attention patterns\n    \"\"\"\n\n    def __init__(self, config: OpenEvolveMaskerConfig) -> None:\n        \"\"\"Initialize OpenEvolve masker with configuration.\n\n        Args:\n            config: Configuration object containing the evolution rate and other\n                parameters for the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the evolution_rate in config is negative.\n                This validation is performed in the config's __post_init__ method.\n        \"\"\"\n        self.base_rate_sampling = 0.01\n        self.epsilon = 0.3\n        self.delta = 0.3\n        self.init_offset = 0.001\n        self.local_offset = 0.001\n\n        super().__init__(config)\n\n    def _compute_exp_attention_scores(\n        self,\n        queries: torch.Tensor,\n        keys: torch.Tensor,\n        scaling: float,\n        attention_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute exponential attention scores with numerical stability.\"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        raw_scores = torch.matmul(queries, keys.transpose(-2, -1)) * scaling\n        if attention_mask is not None:\n            raw_scores = raw_scores + attention_mask[:, :, :, : keys.shape[-2]]\n        max_scores = torch.max(raw_scores, dim=-1, keepdim=True)[0]\n        return torch.exp(raw_scores - max_scores)\n\n    def _get_sampling_range(self, seq_len_keys: int) -> tuple[int, int, int]:\n        \"\"\"Get sampling range and validate it.\n\n        Args:\n            seq_len_keys: Number of keys in the sequence.\n\n        Returns:\n            Tuple of (start_idx, end_idx, sampling_range).\n\n        Raises:\n            ValueError: If the computed sampling range is invalid.\n        \"\"\"\n        # Compute start index\n        if isinstance(self.init_offset, float):\n            start_idx: int = int(self.init_offset * seq_len_keys)\n        else:\n            start_idx = self.init_offset\n\n        # Compute end index\n        if isinstance(self.local_offset, float):\n            end_idx: int = seq_len_keys - int(self.local_offset * seq_len_keys)\n        else:\n            end_idx = seq_len_keys - self.local_offset\n\n        sampling_range = end_idx - start_idx\n\n        if sampling_range <= 0:\n            raise ValueError(f\"Invalid sampling range: {sampling_range}\")\n\n        return start_idx, end_idx, sampling_range\n\n    def _get_base_sample_count(self, sampling_range: int) -> int:\n        \"\"\"Get number of base samples based on configuration.\"\"\"\n        # Ensure at least 2 samples since it is used for std estimation\n        if isinstance(self.base_rate_sampling, int):\n            return max(2, self.base_rate_sampling)\n        return max(2, int(self.base_rate_sampling * sampling_range))\n\n    def _get_std_estimate_using_base_sample(\n        self,\n        expwts: torch.Tensor,\n        batch_size: int,\n        num_heads: int,\n        seq_len_queries: int,\n        seq_len_keys: int,\n        start_idx: int,\n        end_idx: int,\n        num_base_samples: int,\n        dtype: torch.dtype,\n    ) -> tuple[Mask, torch.Tensor]:\n        \"\"\"Get standard deviation estimate using base sampling and create base mask.\"\"\"\n        # Create base sampling indices\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n\n        # Extract values and compute std\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        total_rows = batch_size * num_heads * seq_len_queries\n        row_sampled_values = sampled_values.view(total_rows, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n\n        # Create base sampling mask\n        sampling_range = end_idx - start_idx\n        base_data = torch.full_like(\n            base_row_wise_idx, num_base_samples / sampling_range, dtype=expwts.dtype\n        )\n\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=dtype,\n        )\n\n        return base_mask, std_estimate\n\n    def _compute_adaptive_budget(\n        self,\n        std_estimate: torch.Tensor,\n        estimated_denominator: torch.Tensor,\n        sampling_range: int,\n    ) -> torch.Tensor:\n        \"\"\"Compute adaptive budget based on statistical bounds.\"\"\"\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        # Ensure budget is positive and within bounds\n        budget = torch.clamp(\n            budget_squared,\n            min=1.0,  # Minimum 1 sample\n            max=float(sampling_range),  # Maximum sampling_range samples\n        ).long()\n\n        return budget\n\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"\n        Add an adaptive sparse attention mask by aggressively shrinking budget while \n        retaining statistical guarantees, simplifying the code for maintainability.\n        \"\"\"\n\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n\n        seq_len_keys = keys.shape[-2]\n        # Dynamically adapt parameters for sequence length (tuned for higher sparsity)\n        if seq_len_keys > 10000:\n            self.base_rate_sampling = 0.015\n            self.epsilon = 0.16\n            self.delta = 0.14\n        elif seq_len_keys > 5000:\n            self.base_rate_sampling = 0.012\n            self.epsilon = 0.18\n            self.delta = 0.17\n        else:\n            self.base_rate_sampling = 0.01\n            self.epsilon = 0.2\n            self.delta = 0.2\n\n        self.init_offset = 0.002\n        self.local_offset = 0.002\n\n        # Precompute delta PPF once\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        # If fully dense mask already, no further sparsification is required\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        # Get tensor dimensions\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size, dims.num_heads, dims.seq_len_queries, dims.seq_len_keys\n        )\n\n        # Compute stable exp attention weights (after masking)\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # --- Sampling budget setup ---\n        start_idx, end_idx, sampling_range = self._get_sampling_range(seq_len_keys)\n        num_base_samples = self._get_base_sample_count(sampling_range)\n        # Base sampling + std estimation\n        base_sampling_mask, std_estimate = self._get_std_estimate_using_base_sample(\n            expwts,\n            batch_size,\n            num_heads,\n            seq_len_queries,\n            seq_len_keys,\n            start_idx,\n            end_idx,\n            num_base_samples,\n            previous_mask.dtype,\n        )\n        sampled_denominator = apply_inv_mask_sum(expwts, base_sampling_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n\n        # Statistical error-controlled budget (pre-sparsification)\n        budget = self._compute_adaptive_budget(\n            std_estimate, estimated_denominator, sampling_range\n        )\n        # Enhanced sparsification steps:\n        # (1) Softly shrink budget for higher sparsity\n        budget = torch.ceil(budget.float() * 0.70).long()\n        # (2) Enforce minimum and adaptive capped maximum budget per row\n        min_budget = num_base_samples\n        max_budget = max(num_base_samples, int(0.12 * sampling_range))\n        budget = torch.clamp(budget, min=min_budget, max=max_budget)\n\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n\n        # Fast mask creation: disables sampling for nearly-zero budget rows (bonus sparsity)\n        zero_budget = (budget == 0)\n        if torch.any(zero_budget):\n            budget = budget.masked_fill(zero_budget, 0)\n            sampling_probabilities = sampling_probabilities.masked_fill(zero_budget, 0)\n\n        # Compose the new adaptive mask\n        adaptive_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n        # Merge with any previous mask\n        return previous_mask.merge_mask(adaptive_mask, inplace=False)\n\n    @classmethod\n    def create_from_config(cls, config: MaskerConfig) -> \"OpenEvolveMasker\":\n        \"\"\"Create OpenEvolve masker instance from configuration.\n\n        Args:\n            config: Configuration for the OpenEvolve masker.\n\n        Returns:\n            Instance of the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the config type is invalid.\n        \"\"\"\n        # not checking for config type here since we will be replacing this masker class\n        # with the new masker class in the evaluator.py file\n        return cls(config)\n\n```\nUnique approach: Modification: Change 1: Replace 125 lines with 106 lines, Alternative density approach, Alternative error approach\n\n\n### Inspiration 5 (Score: 33.3333, Type: High-Performer)\n```python\n\"\"\"OpenEvolve masker implementation.\n\nThis module provides the OpenEvolve masker that implements evolutionary attention patterns.\nThe current implementation is a bare metal version that returns the previous mask.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nimport torch\nfrom scipy.stats import norm\n\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers import (\n    ResearchMasker,\n)\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers.base import (\n    MaskerConfig,\n    MaskerRegistry,\n)\nfrom sparse_attention_hub.sparse_attention.utils.mask import Mask\nfrom sparse_attention_hub.sparse_attention.utils.mask_attention_utils import (\n    _get_num_key_value_groups,\n    apply_inv_mask_sum,\n    create_sampling_mask_with_per_head_budget,\n    repeat_kv,\n)\n\n\n@dataclass\nclass OpenEvolveMaskerConfig(MaskerConfig):\n    \"\"\"Configuration for OpenEvolveMasker.\n\n    This configuration class inherits from MaskerConfig and provides\n    parameters for the attention mechanism evolved out of openevolve.\n    empty placeholder\n    \"\"\"\n\n    pass\n\n\n@MaskerRegistry.register(OpenEvolveMaskerConfig)\nclass OpenEvolveMasker(ResearchMasker):\n    \"\"\"OpenEvolve masker for evolutionary attention computation.\n\n    This masker implements evolutionary attention patterns that adapt over time.\n    The current implementation is a bare metal version that returns the previous mask.\n\n    Attributes:\n        evolution_rate: The rate of evolution for attention patterns.\n            This value is set from the configuration and controls how quickly\n            the attention patterns evolve.\n\n    Important Notes:\n        - This is a bare metal implementation that simply returns the previous mask.\n        - Future implementations will include evolutionary algorithms for attention pattern optimization.\n        - The evolution_rate parameter is currently unused but will be utilized in future versions.\n\n    Example:\n        >>> config = OpenEvolveMaskerConfig(evolution_rate=1.0)\n        >>> masker = OpenEvolveMasker(config)\n        >>> # Use masker.add_mask() to apply evolutionary attention patterns\n    \"\"\"\n\n    def __init__(self, config: OpenEvolveMaskerConfig) -> None:\n        \"\"\"Initialize OpenEvolve masker with configuration.\n\n        Args:\n            config: Configuration object containing the evolution rate and other\n                parameters for the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the evolution_rate in config is negative.\n                This validation is performed in the config's __post_init__ method.\n        \"\"\"\n        self.base_rate_sampling = 0.01\n        self.epsilon = 0.3\n        self.delta = 0.3\n        self.init_offset = 0.001\n        self.local_offset = 0.001\n\n        super().__init__(config)\n\n    def _compute_exp_attention_scores(\n        self,\n        queries: torch.Tensor,\n        keys: torch.Tensor,\n        scaling: float,\n        attention_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute exponential attention scores with numerical stability.\"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        raw_scores = torch.matmul(queries, keys.transpose(-2, -1)) * scaling\n        if attention_mask is not None:\n            raw_scores = raw_scores + attention_mask[:, :, :, : keys.shape[-2]]\n        max_scores = torch.max(raw_scores, dim=-1, keepdim=True)[0]\n        return torch.exp(raw_scores - max_scores)\n\n    def _get_sampling_range(self, seq_len_keys: int) -> tuple[int, int, int]:\n        \"\"\"Get sampling range and validate it.\n\n        Args:\n            seq_len_keys: Number of keys in the sequence.\n\n        Returns:\n            Tuple of (start_idx, end_idx, sampling_range).\n\n        Raises:\n            ValueError: If the computed sampling range is invalid.\n        \"\"\"\n        # Compute start index\n        if isinstance(self.init_offset, float):\n            start_idx: int = int(self.init_offset * seq_len_keys)\n        else:\n            start_idx = self.init_offset\n\n        # Compute end index\n        if isinstance(self.local_offset, float):\n            end_idx: int = seq_len_keys - int(self.local_offset * seq_len_keys)\n        else:\n            end_idx = seq_len_keys - self.local_offset\n\n        sampling_range = end_idx - start_idx\n\n        if sampling_range <= 0:\n            raise ValueError(f\"Invalid sampling range: {sampling_range}\")\n\n        return start_idx, end_idx, sampling_range\n\n    def _get_base_sample_count(self, sampling_range: int) -> int:\n        \"\"\"Get number of base samples based on configuration.\"\"\"\n        # Ensure at least 2 samples since it is used for std estimation\n        if isinstance(self.base_rate_sampling, int):\n            return max(2, self.base_rate_sampling)\n        return max(2, int(self.base_rate_sampling * sampling_range))\n\n    def _get_std_estimate_using_base_sample(\n        self,\n        expwts: torch.Tensor,\n        batch_size: int,\n        num_heads: int,\n        seq_len_queries: int,\n        seq_len_keys: int,\n        start_idx: int,\n        end_idx: int,\n        num_base_samples: int,\n        dtype: torch.dtype,\n    ) -> tuple[Mask, torch.Tensor]:\n        \"\"\"Get standard deviation estimate using base sampling and create base mask.\"\"\"\n        # Create base sampling indices\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n\n        # Extract values and compute std\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        total_rows = batch_size * num_heads * seq_len_queries\n        row_sampled_values = sampled_values.view(total_rows, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n\n        # Create base sampling mask\n        sampling_range = end_idx - start_idx\n        base_data = torch.full_like(\n            base_row_wise_idx, num_base_samples / sampling_range, dtype=expwts.dtype\n        )\n\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=dtype,\n        )\n\n        return base_mask, std_estimate\n\n    def _compute_adaptive_budget(\n        self,\n        std_estimate: torch.Tensor,\n        estimated_denominator: torch.Tensor,\n        sampling_range: int,\n    ) -> torch.Tensor:\n        \"\"\"Compute adaptive budget based on statistical bounds.\"\"\"\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        # Ensure budget is positive and within bounds\n        budget = torch.clamp(\n            budget_squared,\n            min=1.0,  # Minimum 1 sample\n            max=float(sampling_range),  # Maximum sampling_range samples\n        ).long()\n\n        return budget\n\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"\n        Hybrid ultra-sparse: combine deterministic high-mass coverage (top-k per row) with aggressive tail sampling.\n        This typically yields the best density/quality tradeoff. \n        \"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n\n        seq_len_keys = keys.shape[-2]\n        # Use static ultra-sparse parameters for highest density gains\n        self.base_rate_sampling = 0.008\n        self.epsilon = 0.14\n        self.delta = 0.12\n        self.init_offset = 0.0\n        self.local_offset = 0.0\n\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size, dims.num_heads, dims.seq_len_queries, dims.seq_len_keys\n        )\n\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # ---- (A) Deterministic high-mass top-k tokens per row ----\n        # Cover 98% attention mass, min 1, max 14 for density/quality balance\n        sorted_expwts, sorted_idx = torch.sort(expwts, dim=-1, descending=True)\n        mass_cumsum = torch.cumsum(sorted_expwts, dim=-1)\n        total_mass = mass_cumsum[..., -1:]\n        frac = mass_cumsum / (total_mass + 1e-8)\n        k_mass = (frac < 0.98).sum(dim=-1, keepdim=True) + 1\n        k_mass = torch.clamp(k_mass, min=2, max=min(14, seq_len_keys // 7))\n        max_k = int(k_mass.max())\n        arange_k = torch.arange(max_k, device=expwts.device)[None, None, None, :]\n        active_idx_mask = (arange_k < k_mass)\n        idx_selected = torch.where(\n            active_idx_mask,\n            sorted_idx[..., :max_k],\n            torch.zeros_like(sorted_idx[..., :max_k])\n        )\n        data_selected = torch.where(\n            active_idx_mask,\n            torch.ones_like(sorted_idx[..., :max_k], dtype=previous_mask.dtype),\n            torch.zeros_like(sorted_idx[..., :max_k], dtype=previous_mask.dtype)\n        )\n        topk_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=idx_selected,\n            data=data_selected,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        # ---- (B) Aggressive tail-sampling for the rest of the keys ----\n        # Only sample (ultra sparse) on tail\n        start_idx, end_idx, sampling_range = 0, seq_len_keys, seq_len_keys\n        num_base_samples = max(2, int(self.base_rate_sampling * sampling_range))\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        row_sampled_values = sampled_values.view(batch_size * num_heads * seq_len_queries, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n        base_data = torch.full_like(base_row_wise_idx, num_base_samples / sampling_range, dtype=previous_mask.dtype)\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n        sampled_denominator = apply_inv_mask_sum(expwts, base_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        min_budget = num_base_samples\n        max_budget = max(min_budget, int(0.04 * sampling_range))\n        rel_std = std_estimate / (estimated_denominator / sampling_range + 1e-8)\n        # Even more aggressive shrink for almost uniform rows\n        shrink = torch.where(rel_std < 0.10, 0.20, 0.45)\n        budget = torch.clamp(torch.ceil(budget_squared * shrink).long(), min=min_budget, max=max_budget)\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n        # For low-variance rows, skip tail\n        flat_row = (rel_std < 0.035).squeeze(-1)\n        if torch.any(flat_row):\n            budget = torch.where(flat_row, torch.full_like(budget, min_budget), budget)\n            sampling_probabilities = torch.where(\n                flat_row, torch.full_like(sampling_probabilities, min_budget/sampling_range), sampling_probabilities\n            )\n\n        tail_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n\n        # ---- (C) Merge: previous OR topk OR tail ----\n        merged = previous_mask.merge_mask(topk_mask, inplace=False).merge_mask(tail_mask, inplace=False)\n        return merged\n\n    @classmethod\n    def create_from_config(cls, config: MaskerConfig) -> \"OpenEvolveMasker\":\n        \"\"\"Create OpenEvolve masker instance from configuration.\n\n        Args:\n            config: Configuration for the OpenEvolve masker.\n\n        Returns:\n            Instance of the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the config type is invalid.\n        \"\"\"\n        # not checking for config type here since we will be replacing this masker class\n        # with the new masker class in the evaluator.py file\n        return cls(config)\n\n```\nUnique approach: Modification: Change 1: Replace 115 lines with 129 lines, Excellent density (100.000), Excellent error (100.000)\n\n\n\n# Current Program\n```python\n\"\"\"OpenEvolve masker implementation.\n\nThis module provides the OpenEvolve masker that implements evolutionary attention patterns.\nThe current implementation is a bare metal version that returns the previous mask.\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nimport torch\nfrom scipy.stats import norm\n\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers import (\n    ResearchMasker,\n)\nfrom sparse_attention_hub.sparse_attention.research_attention.maskers.base import (\n    MaskerConfig,\n    MaskerRegistry,\n)\nfrom sparse_attention_hub.sparse_attention.utils.mask import Mask\nfrom sparse_attention_hub.sparse_attention.utils.mask_attention_utils import (\n    _get_num_key_value_groups,\n    apply_inv_mask_sum,\n    create_sampling_mask_with_per_head_budget,\n    repeat_kv,\n)\n\n\n@dataclass\nclass OpenEvolveMaskerConfig(MaskerConfig):\n    \"\"\"Configuration for OpenEvolveMasker.\n\n    This configuration class inherits from MaskerConfig and provides\n    parameters for the attention mechanism evolved out of openevolve.\n    empty placeholder\n    \"\"\"\n\n    pass\n\n\n@MaskerRegistry.register(OpenEvolveMaskerConfig)\nclass OpenEvolveMasker(ResearchMasker):\n    \"\"\"OpenEvolve masker for evolutionary attention computation.\n\n    This masker implements evolutionary attention patterns that adapt over time.\n    The current implementation is a bare metal version that returns the previous mask.\n\n    Attributes:\n        evolution_rate: The rate of evolution for attention patterns.\n            This value is set from the configuration and controls how quickly\n            the attention patterns evolve.\n\n    Important Notes:\n        - This is a bare metal implementation that simply returns the previous mask.\n        - Future implementations will include evolutionary algorithms for attention pattern optimization.\n        - The evolution_rate parameter is currently unused but will be utilized in future versions.\n\n    Example:\n        >>> config = OpenEvolveMaskerConfig(evolution_rate=1.0)\n        >>> masker = OpenEvolveMasker(config)\n        >>> # Use masker.add_mask() to apply evolutionary attention patterns\n    \"\"\"\n\n    def __init__(self, config: OpenEvolveMaskerConfig) -> None:\n        \"\"\"Initialize OpenEvolve masker with configuration.\n\n        Args:\n            config: Configuration object containing the evolution rate and other\n                parameters for the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the evolution_rate in config is negative.\n                This validation is performed in the config's __post_init__ method.\n        \"\"\"\n        self.base_rate_sampling = 0.01\n        self.epsilon = 0.3\n        self.delta = 0.3\n        self.init_offset = 0.001\n        self.local_offset = 0.001\n\n        super().__init__(config)\n\n    def _compute_exp_attention_scores(\n        self,\n        queries: torch.Tensor,\n        keys: torch.Tensor,\n        scaling: float,\n        attention_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        \"\"\"Compute exponential attention scores with numerical stability.\"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        raw_scores = torch.matmul(queries, keys.transpose(-2, -1)) * scaling\n        if attention_mask is not None:\n            raw_scores = raw_scores + attention_mask[:, :, :, : keys.shape[-2]]\n        max_scores = torch.max(raw_scores, dim=-1, keepdim=True)[0]\n        return torch.exp(raw_scores - max_scores)\n\n    def _get_sampling_range(self, seq_len_keys: int) -> tuple[int, int, int]:\n        \"\"\"Get sampling range and validate it.\n\n        Args:\n            seq_len_keys: Number of keys in the sequence.\n\n        Returns:\n            Tuple of (start_idx, end_idx, sampling_range).\n\n        Raises:\n            ValueError: If the computed sampling range is invalid.\n        \"\"\"\n        # Compute start index\n        if isinstance(self.init_offset, float):\n            start_idx: int = int(self.init_offset * seq_len_keys)\n        else:\n            start_idx = self.init_offset\n\n        # Compute end index\n        if isinstance(self.local_offset, float):\n            end_idx: int = seq_len_keys - int(self.local_offset * seq_len_keys)\n        else:\n            end_idx = seq_len_keys - self.local_offset\n\n        sampling_range = end_idx - start_idx\n\n        if sampling_range <= 0:\n            raise ValueError(f\"Invalid sampling range: {sampling_range}\")\n\n        return start_idx, end_idx, sampling_range\n\n    def _get_base_sample_count(self, sampling_range: int) -> int:\n        \"\"\"Get number of base samples based on configuration.\"\"\"\n        # Ensure at least 2 samples since it is used for std estimation\n        if isinstance(self.base_rate_sampling, int):\n            return max(2, self.base_rate_sampling)\n        return max(2, int(self.base_rate_sampling * sampling_range))\n\n    def _get_std_estimate_using_base_sample(\n        self,\n        expwts: torch.Tensor,\n        batch_size: int,\n        num_heads: int,\n        seq_len_queries: int,\n        seq_len_keys: int,\n        start_idx: int,\n        end_idx: int,\n        num_base_samples: int,\n        dtype: torch.dtype,\n    ) -> tuple[Mask, torch.Tensor]:\n        \"\"\"Get standard deviation estimate using base sampling and create base mask.\"\"\"\n        # Create base sampling indices\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n\n        # Extract values and compute std\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        total_rows = batch_size * num_heads * seq_len_queries\n        row_sampled_values = sampled_values.view(total_rows, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n\n        # Create base sampling mask\n        sampling_range = end_idx - start_idx\n        base_data = torch.full_like(\n            base_row_wise_idx, num_base_samples / sampling_range, dtype=expwts.dtype\n        )\n\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=dtype,\n        )\n\n        return base_mask, std_estimate\n\n    def _compute_adaptive_budget(\n        self,\n        std_estimate: torch.Tensor,\n        estimated_denominator: torch.Tensor,\n        sampling_range: int,\n    ) -> torch.Tensor:\n        \"\"\"Compute adaptive budget based on statistical bounds.\"\"\"\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        # Ensure budget is positive and within bounds\n        budget = torch.clamp(\n            budget_squared,\n            min=1.0,  # Minimum 1 sample\n            max=float(sampling_range),  # Maximum sampling_range samples\n        ).long()\n\n        return budget\n\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"\n        Hybrid ultra-sparse: Combine high-mass deterministic selection with a minimal adaptive tail sampling for better density/quality tradeoff.\n\n        - (A) For each (b,h,q), deterministically select the top-k tokens by exp(attn mass) to cover ~98% of the total attention score mass (guaranteeing best approx quality).\n        - (B) For the remaining \"tail\", use aggressive adaptive random sampling as before, but with a smaller cap and fewer samples.\n        - (C) The new mask is the union of the high-mass top tokens, the adaptive tail mask, and the previous mask.\n        \"\"\"\n\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n\n        seq_len_keys = keys.shape[-2]\n        # Ultra-sparse, more static parameters to emphasize coverage\n        self.base_rate_sampling = 0.008\n        self.epsilon = 0.14\n        self.delta = 0.12\n        self.init_offset = 0.0\n        self.local_offset = 0.0\n\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size, dims.num_heads, dims.seq_len_queries, dims.seq_len_keys\n        )\n\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # ---- (A) Deterministic high-mass top-k tokens per row ----\n        # Cover 98% attention mass, min 1, max 14 for density/quality balance\n        sorted_expwts, sorted_idx = torch.sort(expwts, dim=-1, descending=True)\n        mass_cumsum = torch.cumsum(sorted_expwts, dim=-1)\n        total_mass = mass_cumsum[..., -1:]\n        frac = mass_cumsum / (total_mass + 1e-8)\n        k_mass = (frac < 0.98).sum(dim=-1, keepdim=True) + 1\n        k_mass = torch.clamp(k_mass, min=2, max=min(14, seq_len_keys // 7))\n        max_k = int(k_mass.max())\n        # Gather indices for all queries, mask off excess for batch/q shorter than max_k\n        arange_k = torch.arange(max_k, device=expwts.device)[None, None, None, :]\n        active_idx_mask = (arange_k < k_mass)\n        idx_selected = torch.where(\n            active_idx_mask,\n            sorted_idx[..., :max_k],\n            torch.zeros_like(sorted_idx[..., :max_k])\n        )\n        data_selected = torch.where(\n            active_idx_mask,\n            torch.ones_like(sorted_idx[..., :max_k], dtype=previous_mask.dtype),\n            torch.zeros_like(sorted_idx[..., :max_k], dtype=previous_mask.dtype)\n        )\n        topk_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=idx_selected,\n            data=data_selected,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        # ---- (B) Aggressive tail-sampling for the rest of the keys ----\n        # Only sample (ultra sparse) on tail\n        start_idx, end_idx, sampling_range = 0, seq_len_keys, seq_len_keys\n        num_base_samples = max(2, int(self.base_rate_sampling * sampling_range))\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        row_sampled_values = sampled_values.view(batch_size * num_heads * seq_len_queries, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n        base_data = torch.full_like(base_row_wise_idx, num_base_samples / sampling_range, dtype=previous_mask.dtype)\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n        sampled_denominator = apply_inv_mask_sum(expwts, base_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        min_budget = num_base_samples\n        max_budget = max(min_budget, int(0.04 * sampling_range))\n        rel_std = std_estimate / (estimated_denominator / sampling_range + 1e-8)\n        # Even more aggressive shrink for almost uniform rows\n        shrink = torch.where(rel_std < 0.10, 0.20, 0.45)\n        budget = torch.clamp(torch.ceil(budget_squared * shrink).long(), min=min_budget, max=max_budget)\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n        # For low-variance rows, skip tail\n        flat_row = (rel_std < 0.035).squeeze(-1)\n        if torch.any(flat_row):\n            budget = torch.where(flat_row, torch.full_like(budget, min_budget), budget)\n            sampling_probabilities = torch.where(\n                flat_row, torch.full_like(sampling_probabilities, min_budget/sampling_range), sampling_probabilities\n            )\n\n        tail_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n\n        # ---- (C) Merge: previous OR topk OR tail ----\n        merged = previous_mask.merge_mask(topk_mask, inplace=False).merge_mask(tail_mask, inplace=False)\n        return merged\n\n    @classmethod\n    def create_from_config(cls, config: MaskerConfig) -> \"OpenEvolveMasker\":\n        \"\"\"Create OpenEvolve masker instance from configuration.\n\n        Args:\n            config: Configuration for the OpenEvolve masker.\n\n        Returns:\n            Instance of the OpenEvolve masker.\n\n        Raises:\n            ValueError: If the config type is invalid.\n        \"\"\"\n        # not checking for config type here since we will be replacing this masker class\n        # with the new masker class in the evaluator.py file\n        return cls(config)\n\n```\n\n# Task\nSuggest improvements to the program that will lead to better performance on the specified metrics.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.\n", "responses": ["<<<<<<< SEARCH\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"\n        Hybrid ultra-sparse: Combine high-mass deterministic selection with a minimal adaptive tail sampling for better density/quality tradeoff.\n\n        - (A) For each (b,h,q), deterministically select the top-k tokens by exp(attn mass) to cover ~98% of the total attention score mass (guaranteeing best approx quality).\n        - (B) For the remaining \"tail\", use aggressive adaptive random sampling as before, but with a smaller cap and fewer samples.\n        - (C) The new mask is the union of the high-mass top tokens, the adaptive tail mask, and the previous mask.\n        \"\"\"\n\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n\n        seq_len_keys = keys.shape[-2]\n        # Ultra-sparse, more static parameters to emphasize coverage\n        self.base_rate_sampling = 0.008\n        self.epsilon = 0.14\n        self.delta = 0.12\n        self.init_offset = 0.0\n        self.local_offset = 0.0\n\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size, dims.num_heads, dims.seq_len_queries, dims.seq_len_keys\n        )\n\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # ---- (A) Deterministic high-mass top-k tokens per row ----\n        # Cover 98% attention mass, min 1, max 14 for density/quality balance\n        sorted_expwts, sorted_idx = torch.sort(expwts, dim=-1, descending=True)\n        mass_cumsum = torch.cumsum(sorted_expwts, dim=-1)\n        total_mass = mass_cumsum[..., -1:]\n        frac = mass_cumsum / (total_mass + 1e-8)\n        k_mass = (frac < 0.98).sum(dim=-1, keepdim=True) + 1\n        k_mass = torch.clamp(k_mass, min=2, max=min(14, seq_len_keys // 7))\n        max_k = int(k_mass.max())\n        # Gather indices for all queries, mask off excess for batch/q shorter than max_k\n        arange_k = torch.arange(max_k, device=expwts.device)[None, None, None, :]\n        active_idx_mask = (arange_k < k_mass)\n        idx_selected = torch.where(\n            active_idx_mask,\n            sorted_idx[..., :max_k],\n            torch.zeros_like(sorted_idx[..., :max_k])\n        )\n        data_selected = torch.where(\n            active_idx_mask,\n            torch.ones_like(sorted_idx[..., :max_k], dtype=previous_mask.dtype),\n            torch.zeros_like(sorted_idx[..., :max_k], dtype=previous_mask.dtype)\n        )\n        topk_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=idx_selected,\n            data=data_selected,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        # ---- (B) Aggressive tail-sampling for the rest of the keys ----\n        # Only sample (ultra sparse) on tail\n        start_idx, end_idx, sampling_range = 0, seq_len_keys, seq_len_keys\n        num_base_samples = max(2, int(self.base_rate_sampling * sampling_range))\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        row_sampled_values = sampled_values.view(batch_size * num_heads * seq_len_queries, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n        base_data = torch.full_like(base_row_wise_idx, num_base_samples / sampling_range, dtype=previous_mask.dtype)\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n        sampled_denominator = apply_inv_mask_sum(expwts, base_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        min_budget = num_base_samples\n        max_budget = max(min_budget, int(0.04 * sampling_range))\n        rel_std = std_estimate / (estimated_denominator / sampling_range + 1e-8)\n        # Even more aggressive shrink for almost uniform rows\n        shrink = torch.where(rel_std < 0.10, 0.20, 0.45)\n        budget = torch.clamp(torch.ceil(budget_squared * shrink).long(), min=min_budget, max=max_budget)\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n        # For low-variance rows, skip tail\n        flat_row = (rel_std < 0.035).squeeze(-1)\n        if torch.any(flat_row):\n            budget = torch.where(flat_row, torch.full_like(budget, min_budget), budget)\n            sampling_probabilities = torch.where(\n                flat_row, torch.full_like(sampling_probabilities, min_budget/sampling_range), sampling_probabilities\n            )\n\n        tail_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n\n        # ---- (C) Merge: previous OR topk OR tail ----\n        merged = previous_mask.merge_mask(topk_mask, inplace=False).merge_mask(tail_mask, inplace=False)\n        return merged\n=======\n    def add_mask(\n        self,\n        keys: torch.Tensor,\n        queries: torch.Tensor,\n        values: torch.Tensor,\n        attention_mask: torch.Tensor,\n        scaling: float,\n        dropout: float,\n        sparse_meta_data: Dict[Any, Any],\n        previous_mask: Mask,\n        **kwargs: Dict[str, Any],\n    ) -> Mask:\n        \"\"\"\n        Improved ultra-sparse: Dynamically adapt threshold and shrink tail sampling even more aggressively using statistical coverage.\n        Yields lower density while maintaining the high accuracy of full attention approximation.\n\n        - (A) For each (b,h,q), deterministically select the minimum top-k tokens by exp(attn mass) to cover at least 97% (rather than 98%) of the total row mass. This shrinks k for many rows while keeping the main mass.\n        - (B) For the tail, shrink the statistical adaptive sample budget further using a lower scaling, and skip sampling on rows that are nearly uniform, as measured by the relative std. This maximizes sparsity without significant error.\n        - (C) The mask is the union of (A), (B), and previous mask.\n        \"\"\"\n        ngroups = _get_num_key_value_groups(queries, keys)\n        keys = repeat_kv(keys, ngroups)\n        seq_len_keys = keys.shape[-2]\n        # Slightly more aggressive/optimized static for lowest density\n        self.base_rate_sampling = 0.0065\n        self.epsilon = 0.13\n        self.delta = 0.11\n        self.init_offset = 0.0\n        self.local_offset = 0.0\n        self.delta_ppf = float(norm.ppf(1 - self.delta))\n\n        if previous_mask.is_full_mask():\n            return previous_mask\n\n        dims = self._extract_tensor_dimensions(keys, queries)\n        batch_size, num_heads, seq_len_queries, seq_len_keys = (\n            dims.batch_size, dims.num_heads, dims.seq_len_queries, dims.seq_len_keys\n        )\n\n        expwts = self._compute_exp_attention_scores(\n            queries, keys, scaling, attention_mask\n        )\n        static_denominator = apply_inv_mask_sum(expwts, previous_mask)\n\n        # -- (A) Deterministic top-k per row for 97% exp(attn) mass, with tighter k cap\n        sorted_expwts, sorted_idx = torch.sort(expwts, dim=-1, descending=True)\n        cumsum = torch.cumsum(sorted_expwts, dim=-1)\n        total = cumsum[..., -1:]\n        frac = cumsum / (total + 1e-8)\n        k_mass = (frac < 0.97).sum(dim=-1, keepdim=True) + 1\n        # Cap k much lower for maximal sparsity, but at least 1 for each row\n        k_mass = torch.clamp(k_mass, min=1, max=min(10, seq_len_keys // 12 + 1))\n        max_k = int(k_mass.max())\n        arange_k = torch.arange(max_k, device=expwts.device)[None, None, None, :]\n        active_idx_mask = (arange_k < k_mass)\n        idx_selected = torch.where(\n            active_idx_mask,\n            sorted_idx[..., :max_k],\n            torch.zeros_like(sorted_idx[..., :max_k])\n        )\n        data_selected = torch.where(\n            active_idx_mask,\n            torch.ones_like(sorted_idx[..., :max_k], dtype=previous_mask.dtype),\n            torch.zeros_like(sorted_idx[..., :max_k], dtype=previous_mask.dtype)\n        )\n        topk_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=idx_selected,\n            data=data_selected,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n\n        # -- (B) Statistical tail: aggressively shrink and suppress as much as safely possible\n        start_idx, end_idx, sampling_range = 0, seq_len_keys, seq_len_keys\n        num_base_samples = max(2, int(self.base_rate_sampling * sampling_range))\n        base_row_wise_idx = torch.randint(\n            low=start_idx,\n            high=end_idx,\n            size=(batch_size, num_heads, seq_len_queries, num_base_samples),\n            device=expwts.device,\n        )\n        sampled_values = torch.gather(expwts, dim=-1, index=base_row_wise_idx)\n        row_sampled_values = sampled_values.view(batch_size * num_heads * seq_len_queries, num_base_samples)\n        std_estimate = torch.std(row_sampled_values, dim=-1, keepdim=True)\n        std_estimate = torch.clamp(std_estimate, min=1e-8)\n        std_estimate = std_estimate.view(batch_size, num_heads, seq_len_queries, 1)\n        base_data = torch.full_like(base_row_wise_idx, num_base_samples / sampling_range, dtype=previous_mask.dtype)\n        base_mask = Mask.create_from_row_wise_idx(\n            shape=(batch_size, num_heads, seq_len_queries, seq_len_keys),\n            row_wise_idx=base_row_wise_idx,\n            data=base_data,\n            type=\"index\",\n            dtype=previous_mask.dtype,\n        )\n        sampled_denominator = apply_inv_mask_sum(expwts, base_mask)\n        estimated_denominator = static_denominator + sampled_denominator\n\n        epsilon_allowable_error = self.epsilon * estimated_denominator\n        epsilon_allowable_error = torch.clamp(epsilon_allowable_error, min=1e-8)\n        budget_numerator = self.delta_ppf * std_estimate * sampling_range\n        budget_squared = (budget_numerator / epsilon_allowable_error) ** 2\n\n        min_budget = num_base_samples\n        max_budget = max(min_budget, int(0.03 * sampling_range))\n        rel_std = std_estimate / (estimated_denominator / sampling_range + 1e-8)\n        # Even more aggressive: lower the shrink (so tail is only enabled for highly non-uniform rows)\n        shrink = torch.where(rel_std < 0.06, 0.11, 0.32)\n        budget = torch.clamp(torch.ceil(budget_squared * shrink).long(), min=min_budget, max=max_budget)\n        sampling_probabilities = (budget / sampling_range).to(previous_mask.dtype)\n        # For ultra-flat rows (i.e., almost uniform), set tail budget to zero\u2014skip tail entirely\n        flat_row = (rel_std < 0.025).squeeze(-1)\n        if torch.any(flat_row):\n            budget = torch.where(flat_row, torch.full_like(budget, 0), budget)\n            sampling_probabilities = torch.where(\n                flat_row, torch.zeros_like(sampling_probabilities), sampling_probabilities\n            )\n\n        tail_mask = create_sampling_mask_with_per_head_budget(\n            budgets=budget,\n            sampling_probability=sampling_probabilities,\n            seq_len_keys=seq_len_keys,\n            start_idx=start_idx,\n            end_idx=end_idx,\n            dtype=previous_mask.dtype,\n        )\n\n        # -- (C) Merge: previous \u222a topk \u222a tail\n        merged = previous_mask.merge_mask(topk_mask, inplace=False).merge_mask(tail_mask, inplace=False)\n        return merged\n>>>>>>> REPLACE"]}}}