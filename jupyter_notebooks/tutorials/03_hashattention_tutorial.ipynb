{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# HashAttention Tutorial\n",
        "\n",
        "This notebook demonstrates **HashAttention** implementation using LocalMasker, SinkMasker, and HashAttentionTopKMasker for efficient sparse attention.\n",
        "\n",
        "**HashAttention** combines:\n",
        "- **Local attention**: Sliding window for recent context\n",
        "- **Sink tokens**: First few tokens for global context\n",
        "- **Hash-based attention**: Learned hash functions to identify important tokens\n",
        "- **Sparse pattern**: Reduces computation while maintaining quality\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/data/apdesai/code/sparse-attention-hub')\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "import numpy as np\n",
        "from typing import Dict, List\n",
        "\n",
        "from sparse_attention_hub.sparse_attention.research_attention import ResearchAttentionConfig\n",
        "from sparse_attention_hub.sparse_attention.research_attention.maskers.fixed.implementations import (\n",
        "    LocalMaskerConfig, SinkMaskerConfig, HashAttentionTopKMaskerConfig\n",
        ")\n",
        "from sparse_attention_hub.sparse_attention.integrations.hugging_face import SparseAttentionHF\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Weight Loading Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Weight conversion functions defined\n"
          ]
        }
      ],
      "source": [
        "def extract_linear_weights(sequential_module):\n",
        "    \"\"\"\n",
        "    Extract weight matrices and biases from Sequential module containing Linear layers.\n",
        "    \n",
        "    Args:\n",
        "        sequential_module: nn.Sequential containing Linear layers and activations\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (weight_matrices, bias_vectors)\n",
        "    \"\"\"\n",
        "    matrices = []\n",
        "    biases = []\n",
        "    \n",
        "    for module in sequential_module:\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            matrices.append(module.weight.data.clone())  # Shape: (out_features, in_features)\n",
        "            if module.bias is not None:\n",
        "                biases.append(module.bias.data.clone())\n",
        "            else:\n",
        "                biases.append(torch.zeros(module.out_features))\n",
        "    \n",
        "    return matrices, biases\n",
        "\n",
        "def convert_usa_weights_to_hash_attention(usa_checkpoint_path: str, num_layers: int = 32, num_heads: int = 32) -> Dict[int, Dict[str, List[torch.Tensor]]]:\n",
        "    \"\"\"\n",
        "    Convert USA module weights to HashAttentionTopKMasker format.\n",
        "    \n",
        "    Args:\n",
        "        usa_checkpoint_path: Path to USA module checkpoint\n",
        "        num_layers: Number of transformer layers\n",
        "        num_heads: Number of attention heads\n",
        "        \n",
        "    Returns:\n",
        "        Dict mapping layer_idx to weight dictionaries\n",
        "    \"\"\"\n",
        "    # Load USA checkpoint\n",
        "    print(f\"Loading USA weights from {usa_checkpoint_path}\")\n",
        "    usa_state_dict = torch.load(usa_checkpoint_path, map_location='cpu')\n",
        "    \n",
        "    hat_weights = {}\n",
        "    \n",
        "    for layer_idx in range(num_layers):\n",
        "        layer_weights = {\n",
        "            \"query_matrix\": [],\n",
        "            \"query_bias\": [],\n",
        "            \"key_matrix\": [],\n",
        "            \"key_bias\": []\n",
        "        }\n",
        "        \n",
        "        # For each head, extract weights and stack them\n",
        "        # First, collect all weights for this layer\n",
        "        query_matrices_per_layer = [[] for _ in range(3)]  # 3 linear layers\n",
        "        query_biases_per_layer = [[] for _ in range(3)]\n",
        "        key_matrices_per_layer = [[] for _ in range(3)]\n",
        "        key_biases_per_layer = [[] for _ in range(3)]\n",
        "        \n",
        "        for head_idx in range(num_heads):\n",
        "            # Extract query transformation weights\n",
        "            query_prefix = f\"{layer_idx}.learning_to_hash_transformation_q.{head_idx}\"\n",
        "            key_prefix = f\"{layer_idx}.learning_to_hash_transformation_k.{head_idx}\"\n",
        "            \n",
        "            # For 3-layer MLP: 0.weight, 0.bias, 2.weight, 2.bias, 4.weight, 4.bias\n",
        "            # (indices 1, 3 are SiLU activations)\n",
        "            for i, linear_idx in enumerate([0, 2, 4]):  # Linear layer indices in Sequential\n",
        "                weight_key = f\"{query_prefix}.{linear_idx}.weight\"\n",
        "                bias_key = f\"{query_prefix}.{linear_idx}.bias\"\n",
        "                \n",
        "                if weight_key in usa_state_dict:\n",
        "                    # HashAttentionTopKMasker expects shape (H, in_features, out_features)\n",
        "                    # USA stores as (out_features, in_features), so we need to transpose\n",
        "                    weight = usa_state_dict[weight_key].t()  # Transpose to (in_features, out_features)\n",
        "                    query_matrices_per_layer[i].append(weight)\n",
        "                    \n",
        "                    if bias_key in usa_state_dict:\n",
        "                        query_biases_per_layer[i].append(usa_state_dict[bias_key])\n",
        "                    else:\n",
        "                        query_biases_per_layer[i].append(torch.zeros(usa_state_dict[weight_key].shape[0]))\n",
        "                \n",
        "                # Same for key weights\n",
        "                weight_key = f\"{key_prefix}.{linear_idx}.weight\"\n",
        "                bias_key = f\"{key_prefix}.{linear_idx}.bias\"\n",
        "                \n",
        "                if weight_key in usa_state_dict:\n",
        "                    weight = usa_state_dict[weight_key].t()  # Transpose to (in_features, out_features)\n",
        "                    key_matrices_per_layer[i].append(weight)\n",
        "                    \n",
        "                    if bias_key in usa_state_dict:\n",
        "                        key_biases_per_layer[i].append(usa_state_dict[bias_key])\n",
        "                    else:\n",
        "                        key_biases_per_layer[i].append(torch.zeros(usa_state_dict[weight_key].shape[0]))\n",
        "        \n",
        "        # Stack all heads for each layer\n",
        "        for i in range(3):\n",
        "            if query_matrices_per_layer[i]:\n",
        "                layer_weights[\"query_matrix\"].append(torch.stack(query_matrices_per_layer[i]))\n",
        "                layer_weights[\"query_bias\"].append(torch.stack(query_biases_per_layer[i]))\n",
        "                layer_weights[\"key_matrix\"].append(torch.stack(key_matrices_per_layer[i]))\n",
        "                layer_weights[\"key_bias\"].append(torch.stack(key_biases_per_layer[i]))\n",
        "        \n",
        "        hat_weights[layer_idx] = layer_weights\n",
        "    \n",
        "    print(f\"‚úÖ Converted weights for {num_layers} layers, {num_heads} heads\")\n",
        "    return hat_weights\n",
        "\n",
        "print(\"‚úÖ Weight conversion functions defined\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Load HashAttention Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading USA weights from /data/apdesai/code/HashAttention-1.0/artifacts/llama3.1-8b-patch.32K.v1.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3327776/3454676759.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  usa_state_dict = torch.load(usa_checkpoint_path, map_location='cpu')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Converted weights for 32 layers, 32 heads\n",
            "‚úÖ Successfully loaded and converted USA weights\n"
          ]
        }
      ],
      "source": [
        "# Load and convert USA weights\n",
        "usa_checkpoint_path = \"/data/apdesai/code/HashAttention-1.0/artifacts/llama3.1-8b-patch.32K.v1.pt\"\n",
        "\n",
        "# Check if file exists\n",
        "if not os.path.exists(usa_checkpoint_path):\n",
        "    print(f\"‚ùå USA checkpoint not found at {usa_checkpoint_path}\")\n",
        "    print(\"Please make sure the checkpoint file exists at the specified path.\")\n",
        "    # Create dummy weights for demonstration\n",
        "    print(\"Creating dummy weights for demonstration...\")\n",
        "    hat_weights = {}\n",
        "    for layer_idx in range(32):\n",
        "        hat_weights[layer_idx] = {\n",
        "            \"query_matrix\": [\n",
        "                torch.randn(32, 128, 128),  # First linear layer\n",
        "                torch.randn(32, 128, 128),  # Second linear layer\n",
        "                torch.randn(32, 128, 32),   # Third linear layer\n",
        "            ],\n",
        "            \"query_bias\": [\n",
        "                torch.randn(32, 128),\n",
        "                torch.randn(32, 128),\n",
        "                torch.randn(32, 32),\n",
        "            ],\n",
        "            \"key_matrix\": [\n",
        "                torch.randn(32, 128, 128),  # First linear layer\n",
        "                torch.randn(32, 128, 128),  # Second linear layer\n",
        "                torch.randn(32, 128, 32),   # Third linear layer\n",
        "            ],\n",
        "            \"key_bias\": [\n",
        "                torch.randn(32, 128),\n",
        "                torch.randn(32, 128),\n",
        "                torch.randn(32, 32),\n",
        "            ],\n",
        "        }\n",
        "    print(\"‚úÖ Created dummy weights\")\n",
        "else:\n",
        "    try:\n",
        "        hat_weights = convert_usa_weights_to_hash_attention(\n",
        "            usa_checkpoint_path, \n",
        "            num_layers=32, \n",
        "            num_heads=32\n",
        "        )\n",
        "        print(\"‚úÖ Successfully loaded and converted USA weights\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading USA weights: {e}\")\n",
        "        print(\"Creating dummy weights for demonstration...\")\n",
        "        hat_weights = {}\n",
        "        for layer_idx in range(32):\n",
        "            hat_weights[layer_idx] = {\n",
        "                \"query_matrix\": [\n",
        "                    torch.randn(32, 128, 128),\n",
        "                    torch.randn(32, 128, 128),\n",
        "                    torch.randn(32, 128, 32),\n",
        "                ],\n",
        "                \"query_bias\": [\n",
        "                    torch.randn(32, 128),\n",
        "                    torch.randn(32, 128),\n",
        "                    torch.randn(32, 32),\n",
        "                ],\n",
        "                \"key_matrix\": [\n",
        "                    torch.randn(32, 128, 128),\n",
        "                    torch.randn(32, 128, 128),\n",
        "                    torch.randn(32, 128, 32),\n",
        "                ],\n",
        "                \"key_bias\": [\n",
        "                    torch.randn(32, 128),\n",
        "                    torch.randn(32, 128),\n",
        "                    torch.randn(32, 32),\n",
        "                ],\n",
        "            }\n",
        "        print(\"‚úÖ Created dummy weights\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c528fed161b43a9b6f623195cb591a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded meta-llama/Llama-3.1-8B-Instruct\n"
          ]
        }
      ],
      "source": [
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\"\n",
        "    )\n",
        "    print(f\"‚úÖ Loaded {model_name}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    raise e"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Configure HashAttention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ HashAttention config: Local(16) + Sink(16) + Hash(32 bits, 32 heavy)\n"
          ]
        }
      ],
      "source": [
        "# Create HashAttention configuration\n",
        "local_config = LocalMaskerConfig(window_size=16)\n",
        "sink_config = SinkMaskerConfig(sink_size=16)\n",
        "hash_config = HashAttentionTopKMaskerConfig(\n",
        "    heavy_size=32,\n",
        "    hat_bits=32,\n",
        "    hat_mlp_layers=3,\n",
        "    hat_mlp_hidden_size=128,\n",
        "    hat_mlp_activation=\"silu\",\n",
        "    hat_weights=hat_weights\n",
        ")\n",
        "\n",
        "# Combine all maskers\n",
        "research_config = ResearchAttentionConfig(\n",
        "    masker_configs=[local_config, sink_config, hash_config]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ HashAttention config: Local(16) + Sink(16) + Hash(32 bits, 32 heavy)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Create Sparse Attention Integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ SparseAttentionHF with HashAttention created\n"
          ]
        }
      ],
      "source": [
        "# Create SparseAttentionHF integration object\n",
        "sparse_attention_hf = SparseAttentionHF.create_from_config(research_config)\n",
        "print(\"‚úÖ SparseAttentionHF with HashAttention created\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Prepare Test Input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Input prepared: 190 tokens\n"
          ]
        }
      ],
      "source": [
        "# Test text about HashAttention and sparse attention mechanisms\n",
        "test_text = \"\"\" \n",
        "        HashAttention is an innovative sparse attention mechanism that combines multiple strategies:\n",
        "        \n",
        "        1. Local Attention: Maintains a sliding window of recent tokens for immediate context\n",
        "        2. Sink Tokens: Preserves the first few tokens which often contain crucial global information\n",
        "        3. Hash-based Selection: Uses learned hash functions to identify important tokens beyond the local window\n",
        "        \n",
        "        This approach allows the model to maintain long-range dependencies while significantly reducing computational costs.\n",
        "        The hash functions are trained to recognize patterns that indicate token importance, enabling dynamic attention allocation.\n",
        "        \n",
        "        Unlike traditional attention mechanisms that compute all pairwise interactions, HashAttention selectively focuses on:\n",
        "        - Recent tokens (local window)\n",
        "        - Important early tokens (sink tokens)\n",
        "        - Semantically relevant distant tokens (hash-selected)\n",
        "        \n",
        "        This combination provides an excellent balance between computational efficiency and model performance.\n",
        "        \n",
        "        Please summarize the key benefits of HashAttention in one concise sentence.\n",
        "        \"\"\"\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=32000)\n",
        "input_ids = inputs[\"input_ids\"].to(device)\n",
        "attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "print(f\"‚úÖ Input prepared: {input_ids.shape[1]} tokens\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Run Full Attention Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è±Ô∏è Full attention: 3.04s\n",
            "üìù Generated: 32 tokens\n",
            "\n",
            "Output:\n",
            "--------------------------------------------------\n",
            " \n",
            "        HashAttention is an innovative sparse attention mechanism that combines multiple strategies:\n",
            "        \n",
            "        1. Local Attention: Maintains a sliding window of recent tokens for immediate context\n",
            "        2. Sink Tokens: Preserves the first few tokens which often contain crucial global information\n",
            "        3. Hash-based Selection: Uses learned hash functions to identify important tokens beyond the local window\n",
            "        \n",
            "        This approach allows the model to maintain long-range dependencies while significantly reducing computational costs.\n",
            "        The hash functions are trained to recognize patterns that indicate token importance, enabling dynamic attention allocation.\n",
            "        \n",
            "        Unlike traditional attention mechanisms that compute all pairwise interactions, HashAttention selectively focuses on:\n",
            "        - Recent tokens (local window)\n",
            "        - Important early tokens (sink tokens)\n",
            "        - Semantically relevant distant tokens (hash-selected)\n",
            "        \n",
            "        This combination provides an excellent balance between computational efficiency and model performance.\n",
            "        \n",
            "        Please summarize the key benefits of HashAttention in one concise sentence.\n",
            "        \n",
            "\n",
            "\n",
            "The final answer is: \n",
            "HashAttention combines local attention, sink tokens, and hash-based selection to efficiently capture long-range dependencies while reducing computational costs.\n"
          ]
        }
      ],
      "source": [
        "# Run with full attention\n",
        "model.eval()\n",
        "max_new_tokens = 50\n",
        "\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    full_outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "full_time = time.time() - start_time\n",
        "\n",
        "# Get generated text\n",
        "full_generated_ids = full_outputs[0]\n",
        "full_generated_text = tokenizer.decode(full_generated_ids, skip_special_tokens=True)\n",
        "\n",
        "print(f\"‚è±Ô∏è Full attention: {full_time:.2f}s\")\n",
        "print(f\"üìù Generated: {len(full_generated_ids) - len(input_ids[0])} tokens\")\n",
        "print(\"\\nOutput:\")\n",
        "print(\"-\" * 50)\n",
        "print(full_generated_text)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Run HashAttention Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "einsum(): subscript h has size 32 for operand 1 which does not broadcast with previously seen size 8",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m sparse_attention_hf(model) \u001b[38;5;28;01mas\u001b[39;00m sparse_model:\n\u001b[0;32m----> 5\u001b[0m         sparse_outputs \u001b[38;5;241m=\u001b[39m sparse_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      6\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m      7\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m      8\u001b[0m             max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[1;32m      9\u001b[0m             temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m,\n\u001b[1;32m     10\u001b[0m             do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m             pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m     12\u001b[0m             eos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m     13\u001b[0m             sparse_meta_data\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m     14\u001b[0m         )\n\u001b[1;32m     15\u001b[0m sparse_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Get generated text\u001b[39;00m\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/transformers/generation/utils.py:2622\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2614\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2615\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2616\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2617\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2618\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2619\u001b[0m     )\n\u001b[1;32m   2621\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2622\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   2623\u001b[0m         input_ids,\n\u001b[1;32m   2624\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2625\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2626\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2627\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2628\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   2629\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2630\u001b[0m     )\n\u001b[1;32m   2632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2633\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2634\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2635\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2636\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2637\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2638\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2639\u001b[0m     )\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/transformers/generation/utils.py:3605\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3603\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3604\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3605\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3607\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3608\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3609\u001b[0m     outputs,\n\u001b[1;32m   3610\u001b[0m     model_kwargs,\n\u001b[1;32m   3611\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3612\u001b[0m )\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:552\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    548\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    549\u001b[0m )\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    553\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    554\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    555\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    556\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    557\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    558\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    559\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    560\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    561\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/transformers/utils/generic.py:943\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    940\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     output \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    945\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:440\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    438\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 440\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    441\u001b[0m     hidden_states,\n\u001b[1;32m    442\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    443\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    444\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    445\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    446\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    447\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    448\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs,\n\u001b[1;32m    450\u001b[0m )\n\u001b[1;32m    452\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:290\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    291\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    292\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    293\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    294\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    295\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    296\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    297\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    298\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    300\u001b[0m )\n\u001b[1;32m    301\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:247\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    245\u001b[0m     attention_interface \u001b[38;5;241m=\u001b[39m ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation]\n\u001b[0;32m--> 247\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    249\u001b[0m     query_states,\n\u001b[1;32m    250\u001b[0m     key_states,\n\u001b[1;32m    251\u001b[0m     value_states,\n\u001b[1;32m    252\u001b[0m     attention_mask,\n\u001b[1;32m    253\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_dropout,\n\u001b[1;32m    254\u001b[0m     scaling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling,\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    256\u001b[0m )\n\u001b[1;32m    258\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    259\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_proj(attn_output)\n",
            "File \u001b[0;32m/data/apdesai/code/sparse-attention-hub/sparse_attention_hub/sparse_attention/integrations/hugging_face.py:61\u001b[0m, in \u001b[0;36mSparseAttentionHF.get_custom_attention_function.<locals>.custom_attention_callable\u001b[0;34m(module, queries, keys, values, attention_mask, scaling, dropout, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_idx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     59\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_idx\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mlayer_idx\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_attention\u001b[38;5;241m.\u001b[39mcustom_attention(\n\u001b[1;32m     62\u001b[0m     module\u001b[38;5;241m=\u001b[39mmodule,\n\u001b[1;32m     63\u001b[0m     queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[1;32m     64\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m     65\u001b[0m     values\u001b[38;5;241m=\u001b[39mvalues,\n\u001b[1;32m     66\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     67\u001b[0m     scaling\u001b[38;5;241m=\u001b[39mscaling,\n\u001b[1;32m     68\u001b[0m     dropout\u001b[38;5;241m=\u001b[39mdropout,\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     70\u001b[0m )\n",
            "File \u001b[0;32m/data/apdesai/code/sparse-attention-hub/sparse_attention_hub/sparse_attention/research_attention/base.py:94\u001b[0m, in \u001b[0;36mResearchAttention.custom_attention\u001b[0;34m(self, module, queries, keys, values, attention_mask, scaling, dropout, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Apply all maskers sequentially, each one on the output of the previous one\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m masker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaskers:\n\u001b[0;32m---> 94\u001b[0m     sparse_attention_mask \u001b[38;5;241m=\u001b[39m masker\u001b[38;5;241m.\u001b[39madd_mask(\n\u001b[1;32m     95\u001b[0m         keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m     96\u001b[0m         queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[1;32m     97\u001b[0m         values\u001b[38;5;241m=\u001b[39mvalues,\n\u001b[1;32m     98\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     99\u001b[0m         previous_mask\u001b[38;5;241m=\u001b[39msparse_attention_mask,\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Call compute_masked_attention_output on the result of the last mask\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Always request attention weights to match the expected return signature\u001b[39;00m\n\u001b[1;32m    105\u001b[0m attention_output, attention_weights \u001b[38;5;241m=\u001b[39m get_masked_attention_output(\n\u001b[1;32m    106\u001b[0m     module\u001b[38;5;241m=\u001b[39mmodule,\n\u001b[1;32m    107\u001b[0m     queries\u001b[38;5;241m=\u001b[39mqueries,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    116\u001b[0m )\n",
            "File \u001b[0;32m/data/apdesai/code/sparse-attention-hub/sparse_attention_hub/sparse_attention/research_attention/maskers/fixed/implementations/hashattention_top_k.py:249\u001b[0m, in \u001b[0;36mHashAttentionTopKMasker.add_mask\u001b[0;34m(self, keys, queries, values, attention_mask, sparse_meta_data, previous_mask, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Mask\u001b[38;5;241m.\u001b[39mcreate_full_mask(mask_shape)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# 4. Compute score using _compute_hashattention_score\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_hashattention_score(\n\u001b[1;32m    250\u001b[0m     queries, keys, sparse_meta_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    251\u001b[0m )\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# 5. Extract row-wise top-k indices from inactive positions in previous_mask\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Get the dense mask from previous_mask to identify inactive positions\u001b[39;00m\n\u001b[1;32m    255\u001b[0m previous_dense_mask \u001b[38;5;241m=\u001b[39m previous_mask\u001b[38;5;241m.\u001b[39mget_dense_mask()\n",
            "File \u001b[0;32m/data/apdesai/code/sparse-attention-hub/sparse_attention_hub/sparse_attention/research_attention/maskers/fixed/implementations/hashattention_top_k.py:191\u001b[0m, in \u001b[0;36mHashAttentionTopKMasker._compute_hashattention_score\u001b[0;34m(self, queries, keys, sparse_meta_data, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayer_idx must be provided in kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# 1. Get key signatures\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m key_signatures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_key_signatures(keys, sparse_meta_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# 2. Compute query signatures directly for input queries\u001b[39;00m\n\u001b[1;32m    194\u001b[0m query_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhat_weights[layer_idx]\n",
            "File \u001b[0;32m/data/apdesai/code/sparse-attention-hub/sparse_attention_hub/sparse_attention/research_attention/maskers/fixed/implementations/hashattention_top_k.py:150\u001b[0m, in \u001b[0;36mHashAttentionTopKMasker._update_key_signatures\u001b[0;34m(self, keys, sparse_meta_data, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m key_matrix_list \u001b[38;5;241m=\u001b[39m key_weights[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    148\u001b[0m key_bias_list \u001b[38;5;241m=\u001b[39m key_weights[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 150\u001b[0m new_key_signatures: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_signatures(\n\u001b[1;32m    151\u001b[0m     new_keys, key_matrix_list, key_bias_list\n\u001b[1;32m    152\u001b[0m )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Update sparse_meta_data with new signatures\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached_key_signatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# First run - store new signatures\u001b[39;00m\n",
            "File \u001b[0;32m/data/apdesai/code/sparse-attention-hub/sparse_attention_hub/sparse_attention/research_attention/maskers/fixed/implementations/hashattention_top_k.py:80\u001b[0m, in \u001b[0;36mHashAttentionTopKMasker._get_signatures\u001b[0;34m(self, input_tensor, matrix_list, bias_list)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Apply MLP layers except the last one with activation\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(matrix_list) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Use einsum for proper broadcasting: (B,H,s,d) x (H,d,d_out) -> (B,H,s,d_out)\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     signatures \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhsd,hde->bhse\u001b[39m\u001b[38;5;124m\"\u001b[39m, signatures, matrix_list[i])\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# Add bias with proper broadcasting: (B,H,s,d_out) + (H,d_out) -> (B,H,s,d_out)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     signatures \u001b[38;5;241m=\u001b[39m signatures \u001b[38;5;241m+\u001b[39m bias_list[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n",
            "File \u001b[0;32m/data/apdesai/anaconda3/envs/infllm/lib/python3.12/site-packages/torch/functional.py:402\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39meinsum(equation, operands)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    404\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
            "\u001b[0;31mRuntimeError\u001b[0m: einsum(): subscript h has size 32 for operand 1 which does not broadcast with previously seen size 8"
          ]
        }
      ],
      "source": [
        "# Run with HashAttention\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    with sparse_attention_hf(model) as sparse_model:\n",
        "        sparse_outputs = sparse_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            sparse_meta_data={}\n",
        "        )\n",
        "sparse_time = time.time() - start_time\n",
        "\n",
        "# Get generated text\n",
        "sparse_generated_ids = sparse_outputs[0]\n",
        "sparse_generated_text = tokenizer.decode(sparse_generated_ids, skip_special_tokens=True)\n",
        "\n",
        "print(f\"‚ö° HashAttention: {sparse_time:.2f}s\")\n",
        "print(f\"üìù Generated: {len(sparse_generated_ids) - len(input_ids[0])} tokens\")\n",
        "print(\"\\nOutput:\")\n",
        "print(\"-\" * 50)\n",
        "print(sparse_generated_text)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Performance Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare performance\n",
        "speedup = full_time / sparse_time if sparse_time > 0 else 0\n",
        "print(f\"\\nüìä Performance Comparison:\")\n",
        "print(f\"{'Method':<20} {'Time (s)':<10} {'Speedup':<10}\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"{'Full Attention':<20} {full_time:<10.2f} {'1.00x':<10}\")\n",
        "print(f\"{'HashAttention':<20} {sparse_time:<10.2f} {speedup:<10.2f}x\")\n",
        "\n",
        "# Analyze attention pattern efficiency\n",
        "total_possible_attention = input_ids.shape[1] * input_ids.shape[1]\n",
        "estimated_sparse_attention = (\n",
        "    16 * input_ids.shape[1] +  # Local window\n",
        "    16 * input_ids.shape[1] +  # Sink tokens\n",
        "    32 * input_ids.shape[1]    # Hash-selected tokens\n",
        ")\n",
        "sparsity_ratio = estimated_sparse_attention / total_possible_attention\n",
        "\n",
        "print(f\"\\nüéØ Attention Efficiency:\")\n",
        "print(f\"Full attention pairs: {total_possible_attention:,}\")\n",
        "print(f\"Sparse attention pairs: ~{estimated_sparse_attention:,}\")\n",
        "print(f\"Sparsity ratio: {sparsity_ratio:.3f} ({sparsity_ratio*100:.1f}% of full attention)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Analysis of HashAttention Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîç HashAttention Components Analysis:\")\n",
        "print(\"\\n1. Local Attention (Window Size: 16)\")\n",
        "print(\"   - Maintains attention to the most recent 16 tokens\")\n",
        "print(\"   - Ensures local coherence and immediate context\")\n",
        "\n",
        "print(\"\\n2. Sink Tokens (Sink Size: 16)\")\n",
        "print(\"   - Preserves attention to the first 16 tokens\")\n",
        "print(\"   - Captures global context and important initial information\")\n",
        "\n",
        "print(\"\\n3. Hash-based Selection (Heavy Size: 32)\")\n",
        "print(\"   - Uses learned hash functions to identify 32 important tokens\")\n",
        "print(\"   - Hash dimension: 32 bits\")\n",
        "print(\"   - MLP layers: 3 (with SiLU activation)\")\n",
        "print(\"   - Hidden size: 128\")\n",
        "\n",
        "print(\"\\n4. Combined Strategy:\")\n",
        "print(f\"   - Total attended tokens per query: up to {16 + 16 + 32} tokens\")\n",
        "print(f\"   - Reduction from full attention: {(1 - sparsity_ratio)*100:.1f}%\")\n",
        "print(\"   - Maintains both local and global context\")\n",
        "print(\"   - Adaptive selection of important distant tokens\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Text Quality Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract only the generated portions for comparison\n",
        "full_generated_only = tokenizer.decode(\n",
        "    full_generated_ids[len(input_ids[0]):], \n",
        "    skip_special_tokens=True\n",
        ")\n",
        "sparse_generated_only = tokenizer.decode(\n",
        "    sparse_generated_ids[len(input_ids[0]):], \n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(\"üìù Generated Text Comparison:\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FULL ATTENTION OUTPUT:\")\n",
        "print(\"=\"*60)\n",
        "print(full_generated_only)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HASHATTENTION OUTPUT:\")\n",
        "print(\"=\"*60)\n",
        "print(sparse_generated_only)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Simple similarity metrics\n",
        "full_tokens = set(full_generated_only.split())\n",
        "sparse_tokens = set(sparse_generated_only.split())\n",
        "\n",
        "if full_tokens and sparse_tokens:\n",
        "    token_overlap = len(full_tokens.intersection(sparse_tokens))\n",
        "    token_union = len(full_tokens.union(sparse_tokens))\n",
        "    jaccard_similarity = token_overlap / token_union if token_union > 0 else 0\n",
        "    \n",
        "    print(f\"\\nüîç Text Similarity Analysis:\")\n",
        "    print(f\"Full attention tokens: {len(full_tokens)}\")\n",
        "    print(f\"HashAttention tokens: {len(sparse_tokens)}\")\n",
        "    print(f\"Token overlap: {token_overlap}\")\n",
        "    print(f\"Jaccard similarity: {jaccard_similarity:.3f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This tutorial demonstrates how to implement HashAttention using the sparse-attention-hub framework. HashAttention combines three complementary sparse attention strategies:\n",
        "\n",
        "1. **Local Attention**: Maintains attention to recent tokens for immediate context\n",
        "2. **Sink Tokens**: Preserves attention to initial tokens for global context\n",
        "3. **Hash-based Selection**: Uses learned hash functions to identify important distant tokens\n",
        "\n",
        "The hash functions are trained using the USA module weights, which learn to identify semantically important tokens beyond the local window. This creates an efficient sparse attention pattern that maintains both local coherence and global context while significantly reducing computational costs.\n",
        "\n",
        "Key benefits of HashAttention:\n",
        "- Significantly reduced computational complexity\n",
        "- Maintained model quality through strategic token selection\n",
        "- Adaptive attention allocation based on content importance\n",
        "- Scalable to very long sequences\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "infllm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
